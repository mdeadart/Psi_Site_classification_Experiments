{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Define all parameters for model tuning\n",
    "##################################################################################\n",
    "\n",
    "n_fold = 10\n",
    "count_emsemble = 10\n",
    "expName = \"PSI_Site_DLNN_CORENup_ensemble_withBN\"\n",
    "outPath = \"Results\"\n",
    "foldName = \"folds.pickle\"\n",
    "\n",
    "# modelNames = [\"DLNN_3\", \"DLNN_5\"]\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "shuffle = True\n",
    "seed = None\n",
    "\n",
    "input_data_folder = \"Data\\\\Psi_Site_Chen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from Bio import SeqIO\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# print(tf.test.is_gpu_available(cuda_only=True))\n",
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### define all CUSTOM functions\n",
    "##################################################################################\n",
    "\n",
    "def one_hot_encode_dna(sequence):\n",
    "    \n",
    "    seq_encoded = np.zeros((len(sequence),4))\n",
    "    dict_nuc = {\n",
    "        \"A\": 0,\n",
    "        \"C\": 1,\n",
    "        \"G\": 2,\n",
    "        \"T\":3\n",
    "    }\n",
    "    i = 0\n",
    "    \n",
    "    for single_character in sequence:\n",
    "        if(single_character.upper() in dict_nuc.keys()):\n",
    "            seq_encoded[i][dict_nuc[single_character.upper()]] = 1\n",
    "            i = i+1\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    return seq_encoded\n",
    "\n",
    "def one_hot_encode_rna(sequence):\n",
    "    \n",
    "    seq_encoded = np.zeros((len(sequence),4))\n",
    "    dict_nuc = {\n",
    "        \"A\": 0,\n",
    "        \"C\": 1,\n",
    "        \"G\": 2,\n",
    "        \"U\":3\n",
    "    }\n",
    "    i = 0\n",
    "    \n",
    "    for single_character in sequence:\n",
    "        if(single_character.upper() in dict_nuc.keys()):\n",
    "            seq_encoded[i][dict_nuc[single_character.upper()]] = 1\n",
    "            i = i+1\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    return seq_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### define evaluator functions\n",
    "##################################################################################\n",
    "\n",
    "## Build the K-fold from dataset\n",
    "def build_kfold(features, labels, k=10, shuffle=False, seed=None):\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=shuffle, random_state=seed)\n",
    "    kfoldList = []\n",
    "    for train_index, test_index in skf.split(features, labels):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        kfoldList.append({\n",
    "            \"X_train\": X_train,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_train\":y_train,\n",
    "            \"y_test\":y_test\n",
    "        })\n",
    "    return kfoldList\n",
    "\n",
    "def pred2label(y_pred):\n",
    "    y_pred = np.round(y_pred).astype(int)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Function to customize the DLNN architecture with parameters\n",
    "##################################################################################\n",
    "\n",
    "def DLNN_CORENup(input_shape = (21,4),\n",
    "                   conv_filters_per_layer_1 = 50, kernel_length_1 = 5, conv_strides_1 = 1, ## 1st Convolutional layer parameters\n",
    "                   max_pool_width_1 = 2, max_pool_stride_1 = 2, ## 1st Maxpool layer parameters\n",
    "                   lstm_decode_units = 50, ## LSTM layer parameters\n",
    "                   conv_filters_per_layer_2 = 50,  kernel_length_2 = 10, conv_strides_2 = 1, ## 2nd Convolutional layer parameters\n",
    "                   max_pool_width_2 = 2, max_pool_stride_2 = 2, ## 2nd Maxpool layer parameters\n",
    "                   dense_decode_units = 370, ## Dense layer parameters\n",
    "                   prob = 0.5, learn_rate = 0.0003, loss = 'binary_crossentropy', metrics = None):\n",
    "    \n",
    "    beta = 0.001\n",
    "    \n",
    "    input1 = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    x1 = tf.keras.layers.Conv1D(conv_filters_per_layer_1, kernel_length_1, input_shape = input_shape, \n",
    "                                strides = conv_strides_1, kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "                                padding = \"same\"\n",
    "                               )(input1)\n",
    "    x1 = tf.keras.layers.Activation('relu')(x1)\n",
    "    x1 = tf.keras.layers.MaxPool1D(pool_size = max_pool_width_1, strides = max_pool_stride_1)(x1)\n",
    "    x1 = tf.keras.layers.Dropout(prob)(x1)\n",
    "\n",
    "    ## LSTM Path\n",
    "\n",
    "    x2 = tf.keras.layers.LSTM(lstm_decode_units, return_sequences = True, \n",
    "                              kernel_regularizer = tf.keras.regularizers.l2(beta))(x1)\n",
    "    x2 = tf.keras.layers.Dropout(prob)(x2)\n",
    "    \n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "\n",
    "    ## Conv Path\n",
    "\n",
    "    x3 = tf.keras.layers.Conv1D(conv_filters_per_layer_2, kernel_length_2, strides = conv_strides_2, \n",
    "                                kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "                                padding = 'same'\n",
    "                               )(x1)\n",
    "    x3 = tf.keras.layers.Activation('relu')(x3)\n",
    "    x3 = tf.keras.layers.MaxPooling1D(pool_size = max_pool_width_2, strides = max_pool_stride_2)(x3)\n",
    "    x3 = tf.keras.layers.Dropout(prob)(x3)\n",
    "    \n",
    "    x3 = tf.keras.layers.Flatten()(x3)\n",
    "\n",
    "    ## Fully connected Layers\n",
    "\n",
    "    y = tf.keras.layers.Concatenate(1)([x2,x3])\n",
    "    \n",
    "    y = tf.keras.layers.Dense(dense_decode_units, \n",
    "                              kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "                              activation = 'relu')(y)\n",
    "    # adding batch normalization\n",
    "    y = tf.keras.layers.BatchNormalization()(y)\n",
    "    y = tf.keras.layers.Dropout(prob)(y)\n",
    "    y = tf.keras.layers.Dense(1, \n",
    "                              kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "                              activation = 'sigmoid')(y)\n",
    "\n",
    "    ## Generate Model from input and output\n",
    "    model = tf.keras.models.Model(inputs=[input1], outputs=y)\n",
    "    \n",
    "    ## Compile model\n",
    "    if(metrics != None):\n",
    "        model.compile(optimizer = tf.keras.optimizers.Adam(lr=learn_rate), loss = loss, metrics = metrics)\n",
    "    else:\n",
    "        model.compile(optimizer = tf.keras.optimizers.Adam(lr=learn_rate), loss = loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### For each input file, train model and generate different outputs in a structured folder\n",
    "##################################################################################\n",
    "\n",
    "## create the evaluation data structure for all iterations\n",
    "sum_evaluations = {\n",
    "    \"Model\" : [],\n",
    "    \"Dataset\" : [],\n",
    "    \"Fold\" : [],\n",
    "    \"Train_Test\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "vote_evaluations = {\n",
    "    \"Model\" : [],\n",
    "    \"Dataset\" : [],\n",
    "    \"Fold\" : [],\n",
    "    \"Train_Test\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "File: Data\\Psi_Site_Chen\\HS_990.txt\n",
      "Positive: 495\n",
      "Negative: 495\n",
      "\n",
      "Train/Test model HS_990 on Fold #0.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x000001965936AE08>) with an unsupported type (<class 'tensorflow.python.keras.layers.normalization_v2.BatchNormalization'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-5a6ddad51c22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[1;31m## Generate model using function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m                 \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDLNN_CORENup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                 \u001b[0mmodel_file_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelPath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"{}_bestModel_fold{}_ens{}.hdf5\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_dataset_variety\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-1e62c2f89436>\u001b[0m in \u001b[0;36mDLNN_CORENup\u001b[1;34m(input_shape, conv_filters_per_layer_1, kernel_length_1, conv_strides_1, max_pool_width_1, max_pool_stride_1, lstm_decode_units, conv_filters_per_layer_2, kernel_length_2, conv_strides_2, max_pool_width_2, max_pool_stride_2, dense_decode_units, prob, learn_rate, loss, metrics)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;31m# adding batch normalization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     y = tf.keras.layers.Dense(1, \n\u001b[0;32m     57\u001b[0m                               \u001b[0mkernel_regularizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\base_tf_24\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    967\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 968\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\base_tf_24\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m    209\u001b[0m     output = tf_utils.smart_cond(training,\n\u001b[0;32m    210\u001b[0m                                  \u001b[0mdropped_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m                                  lambda: array_ops.identity(inputs))\n\u001b[0m\u001b[0;32m    212\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\base_tf_24\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36msmart_cond\u001b[1;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[0;32m     63\u001b[0m         pred, true_fn=true_fn, false_fn=false_fn, name=name)\n\u001b[0;32m     64\u001b[0m   return smart_module.smart_cond(\n\u001b[1;32m---> 65\u001b[1;33m       pred, true_fn=true_fn, false_fn=false_fn, name=name)\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\base_tf_24\\lib\\site-packages\\tensorflow\\python\\framework\\smart_cond.py\u001b[0m in \u001b[0;36msmart_cond\u001b[1;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[0;32m     54\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     return control_flow_ops.cond(pred, true_fn=true_fn, false_fn=false_fn,\n",
      "\u001b[1;32m~\\.conda\\envs\\base_tf_24\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    209\u001b[0m     output = tf_utils.smart_cond(training,\n\u001b[0;32m    210\u001b[0m                                  \u001b[0mdropped_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m                                  lambda: array_ops.identity(inputs))\n\u001b[0m\u001b[0;32m    212\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\base_tf_24\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\base_tf_24\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[1;31m# Make sure we get an input with handle data attached from resource\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m     \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m   \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m   \u001b[1;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\base_tf_24\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1341\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\base_tf_24\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    319\u001b[0m                                          as_ref=False):\n\u001b[0;32m    320\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\base_tf_24\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    260\u001b[0m   \"\"\"\n\u001b[0;32m    261\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[1;32m--> 262\u001b[1;33m                         allow_broadcast=True)\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\base_tf_24\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    268\u001b[0m   \u001b[0mctx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m     \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\base_tf_24\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Attempt to convert a value (<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x000001965936AE08>) with an unsupported type (<class 'tensorflow.python.keras.layers.normalization_v2.BatchNormalization'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(input_data_folder):\n",
    "    for file in files:\n",
    "        \n",
    "        input_data_file = os.path.join(root, file)\n",
    "        \n",
    "        current_dataset_variety = input_data_file.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "        \n",
    "        openFile = open(input_data_file)\n",
    "        fastaSequences = SeqIO.parse(openFile, \"fasta\")\n",
    "        \n",
    "        ##################################################################################\n",
    "        ##### extract data from the current fasta file\n",
    "        ##################################################################################\n",
    "\n",
    "        positive_List = []\n",
    "        negative_List = []\n",
    "        positive_onehotencoded_List = []\n",
    "        negative_onehotencoded_List = []\n",
    "\n",
    "        for fasta in fastaSequences: \n",
    "            name, sequence = fasta.id, str(fasta.seq)\n",
    "            if \"P\" in name:\n",
    "                positive_List.append(sequence)\n",
    "                aus_seq = one_hot_encode_rna(sequence)\n",
    "                if(len(aus_seq) != 0):\n",
    "                    positive_onehotencoded_List.append(aus_seq)\n",
    "            elif \"N\" in name:\n",
    "                negative_List.append(sequence)\n",
    "                aus_seq = one_hot_encode_rna(sequence)\n",
    "                if(len(aus_seq) != 0):\n",
    "                    negative_onehotencoded_List.append(aus_seq)\n",
    "\n",
    "        openFile.close()\n",
    "\n",
    "        print(\"\\n======================================================================\")\n",
    "        print(\"\\nFile: \"+os.path.join(root, file))\n",
    "        print(\"Positive: \"+str(len(positive_onehotencoded_List)))\n",
    "        print(\"Negative: \"+str(len(negative_onehotencoded_List)))\n",
    "        \n",
    "        ##################################################################################\n",
    "        ##### Generate Folds from dataset, and store to file\n",
    "        ##################################################################################\n",
    "\n",
    "        ## create the features and labels datasets for the training\n",
    "        input_size = (len(positive_onehotencoded_List[1]), 4)\n",
    "        labels = np.concatenate((np.ones((len(positive_onehotencoded_List), 1), dtype=np.float32), np.zeros((len(negative_onehotencoded_List), 1), dtype=np.float32)), axis=0)\n",
    "        features = np.concatenate((positive_onehotencoded_List,negative_onehotencoded_List), 0)\n",
    "\n",
    "        ## Generate the k-fold dataset\n",
    "        folds = build_kfold(features, labels, k=n_fold, shuffle=shuffle, seed=seed)\n",
    "\n",
    "        ## Write the k-fold dataset to file\n",
    "        foldPath = os.path.join(outPath, expName, current_dataset_variety, \"{}fold\".format(n_fold))\n",
    "        if(not os.path.isdir(foldPath)):\n",
    "            os.makedirs(foldPath)\n",
    "        pickle.dump(folds, open(os.path.join(foldPath, foldName), \"wb\"))\n",
    "\n",
    "        ## Create and set directory to save model\n",
    "        modelPath = os.path.join(outPath, expName, current_dataset_variety, \"{}fold\".format(n_fold), \"models\")\n",
    "        if(not os.path.isdir(modelPath)):\n",
    "            os.makedirs(modelPath)\n",
    "            \n",
    "        ##################################################################################\n",
    "        ##### TRAIN and PREDICT for every Fold, using n-ensemble of models\n",
    "        ##################################################################################\n",
    "\n",
    "        # fold counter\n",
    "        i = -1\n",
    "        for fold in folds:\n",
    "            i += 1\n",
    "\n",
    "            print(\"\\nTrain/Test model \"+current_dataset_variety+\" on Fold #\"+str(i)+\".\")\n",
    "            \n",
    "            ensemble_folds = build_kfold(fold[\"X_train\"], fold[\"y_train\"], k=count_emsemble, shuffle=shuffle, seed=seed)\n",
    "            \n",
    "            # ens fold counter\n",
    "            j = -1\n",
    "            for ens_fold in ensemble_folds:\n",
    "                \n",
    "                # adding random shuffling of the dataset for training purpose\n",
    "                randomized_index_arr = np.arange(ens_fold[\"X_train\"].shape[0])\n",
    "                randomized_index_arr = np.random.permutation(randomized_index_arr)\n",
    "                \n",
    "                j += 1\n",
    "                \n",
    "                ## Generate model using function\n",
    "                model = DLNN_CORENup(input_shape = input_size)\n",
    "\n",
    "                model_file_path = os.path.join(modelPath, \"{}_bestModel_fold{}_ens{}.hdf5\".format(current_dataset_variety, i, j))\n",
    "                ## Define the model callbacks for early stopping and saving the model. Then train model\n",
    "                modelCallbacks = [\n",
    "                    tf.keras.callbacks.ModelCheckpoint(model_file_path,\n",
    "                                                       monitor = 'val_loss', verbose = 1, save_best_only = True, \n",
    "                                                       save_weights_only = False, mode = 'auto', save_freq = 'epoch'),\n",
    "                ]\n",
    "                \n",
    "                model.fit(x = ens_fold[\"X_train\"][randomized_index_arr], y = ens_fold[\"y_train\"][randomized_index_arr], \n",
    "                          validation_data = (ens_fold[\"X_test\"], ens_fold[\"y_test\"]),\n",
    "                          batch_size = batch_size, epochs = epochs, \n",
    "                          verbose = 1, \n",
    "                          callbacks = modelCallbacks\n",
    "                         )\n",
    "            \n",
    "            ##################################################################################\n",
    "            ##################################################################################\n",
    "            #####\n",
    "            ##### Prediction and metrics for TRAIN dataset\n",
    "            #####\n",
    "            ##################################################################################\n",
    "            ##################################################################################\n",
    "            \n",
    "            fold_label = fold[\"y_train\"]\n",
    "            \n",
    "            y_pred_list = []\n",
    "            \n",
    "            for val in range(count_emsemble):\n",
    "                model_file_path = os.path.join(modelPath, \"{}_bestModel_fold{}_ens{}.hdf5\".format(current_dataset_variety, i, val))\n",
    "                model = tf.keras.models.load_model(model_file_path)\n",
    "                y_pred_list.append(model.predict(fold[\"X_train\"]))\n",
    "            \n",
    "            ##################################################################################\n",
    "            ##### Prediction and metrics using sum of all folds\n",
    "            ##################################################################################\n",
    "\n",
    "            y_pred_list_arr = np.swapaxes(np.array(y_pred_list), 0,1)\n",
    "            y_pred_vote = np.mean(y_pred_list_arr, axis = 1)\n",
    "            label_sum = pred2label(y_pred_vote)\n",
    "\n",
    "            # Compute precision, recall, sensitivity, specifity, mcc\n",
    "            acc = accuracy_score(fold_label, label_sum)\n",
    "            prec = precision_score(fold_label, label_sum)\n",
    "\n",
    "            conf = confusion_matrix(fold_label, label_sum)\n",
    "            if(conf[0][0]+conf[1][0]):\n",
    "                sens = float(conf[0][0])/float(conf[0][0]+conf[1][0])\n",
    "            else:\n",
    "                sens = 0.0\n",
    "            if(conf[1][1]+conf[0][1]):\n",
    "                spec = float(conf[1][1])/float(conf[1][1]+conf[0][1])\n",
    "            else:\n",
    "                spec = 0.0\n",
    "            if((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0])):\n",
    "                mcc = (float(conf[0][0])*float(conf[1][1]) - float(conf[1][0])*float(conf[0][1]))/math.sqrt((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0]))\n",
    "            else:\n",
    "                mcc= 0.0\n",
    "            fpr, tpr, thresholds = roc_curve(fold_label, y_pred_vote)\n",
    "            auc = roc_auc_score(fold_label, y_pred_vote)\n",
    "\n",
    "            sum_evaluations[\"Model\"].append(current_dataset_variety)\n",
    "            sum_evaluations[\"Dataset\"].append(current_dataset_variety)\n",
    "            sum_evaluations[\"Fold\"].append(i)\n",
    "            sum_evaluations[\"Train_Test\"].append(\"Train\")\n",
    "            sum_evaluations[\"Accuracy\"].append(acc)\n",
    "            sum_evaluations[\"Precision\"].append(prec)\n",
    "            sum_evaluations[\"TPR\"].append(tpr)\n",
    "            sum_evaluations[\"FPR\"].append(fpr)\n",
    "            sum_evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "            sum_evaluations[\"AUC\"].append(auc)\n",
    "            sum_evaluations[\"Sensitivity\"].append(sens)\n",
    "            sum_evaluations[\"Specificity\"].append(spec)\n",
    "            sum_evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "            ##################################################################################\n",
    "            ##### Prediction and metrics using vote of all folds\n",
    "            ##################################################################################\n",
    "\n",
    "            y_pred_list_arr = np.swapaxes(np.array(y_pred_list), 0,1)\n",
    "            y_pred_vote = np.sum(np.round(y_pred_list_arr), axis = 1)\n",
    "            label_vote = (y_pred_vote > (count_emsemble/2)).astype(int)\n",
    "\n",
    "            # Compute precision, recall, sensitivity, specifity, mcc\n",
    "            acc = accuracy_score(fold_label, label_vote)\n",
    "            prec = precision_score(fold_label, label_vote)\n",
    "\n",
    "            conf = confusion_matrix(fold_label, label_vote)\n",
    "            if(conf[0][0]+conf[1][0]):\n",
    "                sens = float(conf[0][0])/float(conf[0][0]+conf[1][0])\n",
    "            else:\n",
    "                sens = 0.0\n",
    "            if(conf[1][1]+conf[0][1]):\n",
    "                spec = float(conf[1][1])/float(conf[1][1]+conf[0][1])\n",
    "            else:\n",
    "                spec = 0.0\n",
    "            if((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0])):\n",
    "                mcc = (float(conf[0][0])*float(conf[1][1]) - float(conf[1][0])*float(conf[0][1]))/math.sqrt((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0]))\n",
    "            else:\n",
    "                mcc= 0.0\n",
    "            fpr, tpr, thresholds = roc_curve(fold_label, y_pred_vote)\n",
    "            auc = roc_auc_score(fold_label, y_pred_vote)\n",
    "\n",
    "            vote_evaluations[\"Model\"].append(current_dataset_variety)\n",
    "            vote_evaluations[\"Dataset\"].append(current_dataset_variety)\n",
    "            vote_evaluations[\"Fold\"].append(i)\n",
    "            vote_evaluations[\"Train_Test\"].append(\"Train\")\n",
    "            vote_evaluations[\"Accuracy\"].append(acc)\n",
    "            vote_evaluations[\"Precision\"].append(prec)\n",
    "            vote_evaluations[\"TPR\"].append(tpr)\n",
    "            vote_evaluations[\"FPR\"].append(fpr)\n",
    "            vote_evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "            vote_evaluations[\"AUC\"].append(auc)\n",
    "            vote_evaluations[\"Sensitivity\"].append(sens)\n",
    "            vote_evaluations[\"Specificity\"].append(spec)\n",
    "            vote_evaluations[\"MCC\"].append(mcc)\n",
    "            \n",
    "            ##################################################################################\n",
    "            ##################################################################################\n",
    "            #####\n",
    "            ##### Prediction and metrics for TEST dataset\n",
    "            #####\n",
    "            ##################################################################################\n",
    "            ##################################################################################\n",
    "            \n",
    "            fold_label = fold[\"y_test\"]\n",
    "            y_pred_list = []\n",
    "            \n",
    "            for val in range(count_emsemble):\n",
    "                model_file_path = os.path.join(modelPath, \"{}_bestModel_fold{}_ens{}.hdf5\".format(current_dataset_variety, i, val))\n",
    "                model = tf.keras.models.load_model(model_file_path)\n",
    "                y_pred_list.append(model.predict(fold[\"X_test\"]))\n",
    "            \n",
    "            ##################################################################################\n",
    "            ##### Prediction and metrics using sum of all folds\n",
    "            ##################################################################################\n",
    "\n",
    "            y_pred_list_arr = np.swapaxes(np.array(y_pred_list), 0,1)\n",
    "            y_pred_vote = np.mean(y_pred_list_arr, axis = 1)\n",
    "            label_sum = pred2label(y_pred_vote)\n",
    "\n",
    "            # Compute precision, recall, sensitivity, specifity, mcc\n",
    "            acc = accuracy_score(fold_label, label_sum)\n",
    "            prec = precision_score(fold_label, label_sum)\n",
    "\n",
    "            conf = confusion_matrix(fold_label, label_sum)\n",
    "            if(conf[0][0]+conf[1][0]):\n",
    "                sens = float(conf[0][0])/float(conf[0][0]+conf[1][0])\n",
    "            else:\n",
    "                sens = 0.0\n",
    "            if(conf[1][1]+conf[0][1]):\n",
    "                spec = float(conf[1][1])/float(conf[1][1]+conf[0][1])\n",
    "            else:\n",
    "                spec = 0.0\n",
    "            if((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0])):\n",
    "                mcc = (float(conf[0][0])*float(conf[1][1]) - float(conf[1][0])*float(conf[0][1]))/math.sqrt((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0]))\n",
    "            else:\n",
    "                mcc= 0.0\n",
    "            fpr, tpr, thresholds = roc_curve(fold_label, y_pred_vote)\n",
    "            auc = roc_auc_score(fold_label, y_pred_vote)\n",
    "\n",
    "            sum_evaluations[\"Model\"].append(current_dataset_variety)\n",
    "            sum_evaluations[\"Dataset\"].append(current_dataset_variety)\n",
    "            sum_evaluations[\"Fold\"].append(i)\n",
    "            sum_evaluations[\"Train_Test\"].append(\"Test\")\n",
    "            sum_evaluations[\"Accuracy\"].append(acc)\n",
    "            sum_evaluations[\"Precision\"].append(prec)\n",
    "            sum_evaluations[\"TPR\"].append(tpr)\n",
    "            sum_evaluations[\"FPR\"].append(fpr)\n",
    "            sum_evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "            sum_evaluations[\"AUC\"].append(auc)\n",
    "            sum_evaluations[\"Sensitivity\"].append(sens)\n",
    "            sum_evaluations[\"Specificity\"].append(spec)\n",
    "            sum_evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "            ##################################################################################\n",
    "            ##### Prediction and metrics using vote of all folds\n",
    "            ##################################################################################\n",
    "\n",
    "            y_pred_list_arr = np.swapaxes(np.array(y_pred_list), 0,1)\n",
    "            y_pred_vote = np.sum(np.round(y_pred_list_arr), axis = 1)\n",
    "            label_vote = (y_pred_vote > (count_emsemble/2)).astype(int)\n",
    "\n",
    "            # Compute precision, recall, sensitivity, specifity, mcc\n",
    "            acc = accuracy_score(fold_label, label_vote)\n",
    "            prec = precision_score(fold_label, label_vote)\n",
    "\n",
    "            conf = confusion_matrix(fold_label, label_vote)\n",
    "            if(conf[0][0]+conf[1][0]):\n",
    "                sens = float(conf[0][0])/float(conf[0][0]+conf[1][0])\n",
    "            else:\n",
    "                sens = 0.0\n",
    "            if(conf[1][1]+conf[0][1]):\n",
    "                spec = float(conf[1][1])/float(conf[1][1]+conf[0][1])\n",
    "            else:\n",
    "                spec = 0.0\n",
    "            if((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0])):\n",
    "                mcc = (float(conf[0][0])*float(conf[1][1]) - float(conf[1][0])*float(conf[0][1]))/math.sqrt((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0]))\n",
    "            else:\n",
    "                mcc= 0.0\n",
    "            fpr, tpr, thresholds = roc_curve(fold_label, y_pred_vote)\n",
    "            auc = roc_auc_score(fold_label, y_pred_vote)\n",
    "\n",
    "            vote_evaluations[\"Model\"].append(current_dataset_variety)\n",
    "            vote_evaluations[\"Dataset\"].append(current_dataset_variety)\n",
    "            vote_evaluations[\"Fold\"].append(i)\n",
    "            vote_evaluations[\"Train_Test\"].append(\"Test\")\n",
    "            vote_evaluations[\"Accuracy\"].append(acc)\n",
    "            vote_evaluations[\"Precision\"].append(prec)\n",
    "            vote_evaluations[\"TPR\"].append(tpr)\n",
    "            vote_evaluations[\"FPR\"].append(fpr)\n",
    "            vote_evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "            vote_evaluations[\"AUC\"].append(auc)\n",
    "            vote_evaluations[\"Sensitivity\"].append(sens)\n",
    "            vote_evaluations[\"Specificity\"].append(spec)\n",
    "            vote_evaluations[\"MCC\"].append(mcc)\n",
    "            \n",
    "            del model\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "        ##################################################################################\n",
    "        ##### Dump evaluations to a file\n",
    "        ##################################################################################\n",
    "\n",
    "        evalPath = os.path.join(outPath, expName, \"_Evaluation_All_Datasets\")\n",
    "        if(not os.path.isdir(evalPath)):\n",
    "            os.makedirs(evalPath)\n",
    "        \n",
    "        pickle.dump(sum_evaluations,\n",
    "                    open(os.path.join(evalPath, \"{}fold_sum_evaluations.pickle\".format(n_fold)), \"wb\"))\n",
    "        \n",
    "        pickle.dump(vote_evaluations,\n",
    "                    open(os.path.join(evalPath, \"{}fold_vote_evaluations.pickle\".format(n_fold)), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Add import statement here, to make this next part of code standalone executable\n",
    "##################################################################################\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter, FormatStrFormatter\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Load file and convert to dataframe for easy manipulation\n",
    "##################################################################################\n",
    "\n",
    "# evalPath = os.path.join(outPath, expName, \"_Evaluation_All_Datasets\")\n",
    "# if(not os.path.isdir(evalPath)):\n",
    "#     os.makedirs(evalPath)\n",
    "\n",
    "# evaluations = pickle.load(open(os.path.join(evalPath, \"{}fold_evaluations.pickle\".format(n_fold)), \"rb\"))\n",
    "\n",
    "sum_evaluations_df = pd.DataFrame.from_dict(sum_evaluations)\n",
    "\n",
    "sum_evaluations_df_grouped = sum_evaluations_df.groupby([\"Dataset\", \n",
    "                                                         \"Model\", \n",
    "                                                         \"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                                       'Precision', \n",
    "                                                                                       'AUC', \n",
    "                                                                                       'Sensitivity', \n",
    "                                                                                       'Specificity', \n",
    "                                                                                       'MCC'])\n",
    "\n",
    "sum_evaluations_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without batch normalization\n",
    "\n",
    "#                           Accuracy\tPrecision\tAUC     \tSensitivity\tSpecificity\tMCC\n",
    "# Dataset\tModel\tTrain_Test\t\t\t\t\t\t\n",
    "# HS_990\tHS_990\tTest\t0.607071\t0.605671\t0.656694\t0.609640\t0.605671\t0.214672\n",
    "#                   Train\t0.849046\t0.859624\t0.927507\t0.839433\t0.859624\t0.698579\n",
    "# MM_944\tMM_944\tTest\t0.682184\t0.667139\t0.750953\t0.707580\t0.667139\t0.369486\n",
    "#                   Train\t0.888893\t0.854628\t0.956332\t0.930847\t0.854628\t0.781621\n",
    "# SS_628\tSS_628\tTest\t0.668920\t0.669939\t0.732947\t0.669452\t0.669939\t0.338492\n",
    "#                   Train\t0.975938\t0.982093\t0.997878\t0.970006\t0.982093\t0.951988"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_evaluations_df = pd.DataFrame.from_dict(vote_evaluations)\n",
    "\n",
    "vote_evaluations_df_grouped = vote_evaluations_df.groupby([\"Dataset\", \n",
    "                                                           \"Model\", \n",
    "                                                           \"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                                           'Precision', \n",
    "                                                                                           'AUC', \n",
    "                                                                                           'Sensitivity', \n",
    "                                                                                           'Specificity', \n",
    "                                                                                           'MCC'])\n",
    "\n",
    "vote_evaluations_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without batch normalization\n",
    "\n",
    "#                         Accuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# Dataset\tModel\tTrain_Test\t\t\t\t\t\t\n",
    "# HS_990\tHS_990\tTest\t0.602020\t0.608799\t0.646041\t0.597516\t0.608799\t0.205151\n",
    "#                   Train\t0.849270\t0.871025\t0.915484\t0.830324\t0.871025\t0.699948\n",
    "# MM_944\tMM_944\tTest\t0.681120\t0.673985\t0.727843\t0.695436\t0.673985\t0.365812\n",
    "#                   Train\t0.894070\t0.866656\t0.945783\t0.926168\t0.866656\t0.790480\n",
    "# SS_628\tSS_628\tTest\t0.662545\t0.665991\t0.722646\t0.660243\t0.665991\t0.325361\n",
    "#                   Train\t0.978592\t0.987408\t0.997487\t0.970247\t0.987408\t0.957422"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max values in evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluations_df_max = evaluations_df[[\"Dataset\",\n",
    "#                                      \"Model\",\n",
    "#                                      \"Train_Test\",\n",
    "#                                      \"Accuracy\",\n",
    "#                                      \"Precision\",\n",
    "#                                      \"Sensitivity\",\n",
    "#                                      \"Specificity\",\n",
    "#                                      \"AUC\",\n",
    "#                                      \"MCC\"]].groupby([\"Dataset\", \n",
    "#                                                       \"Model\", \n",
    "#                                                       \"Train_Test\"]).max().filter(['Accuracy', \n",
    "#                                                                                'Precision', \n",
    "#                                                                                'AUC', \n",
    "#                                                                                'Sensitivity', \n",
    "#                                                                                'Specificity', \n",
    "#                                                                                'MCC']).reset_index()\n",
    "\n",
    "# evaluations_df_test_max = evaluations_df_max[evaluations_df_max[\"Train_Test\"] == 'Test']\n",
    "# evaluations_df_test_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate only top 5 model folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluations_df5 = evaluations_df.sort_values(['Accuracy'], ascending=False).groupby([\"Dataset\", \n",
    "#                                                                                     \"Model\", \n",
    "#                                                                                     \"Train_Test\"]).head(5).reset_index()\n",
    "\n",
    "# evaluations_df5_grouped = evaluations_df5.groupby([\"Dataset\", \n",
    "#                                                  \"Model\", \n",
    "#                                                  \"Train_Test\"]).mean().filter(['Accuracy', \n",
    "#                                                                                'Precision', \n",
    "#                                                                                'AUC', \n",
    "#                                                                                'Sensitivity', \n",
    "#                                                                                'Specificity', \n",
    "#                                                                                'MCC']).reset_index()\n",
    "\n",
    "# evaluations_df5_grouped[evaluations_df5_grouped[\"Train_Test\"] == 'Test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# independent_data_folder = \"Data\\\\Psi_Site_Chen_Independent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################################\n",
    "# ##### For each input file, train model and generate different outputs in a structured folder\n",
    "# ##################################################################################\n",
    "\n",
    "# ## create the evaluation data structure for all iterations\n",
    "# evaluations = {\n",
    "#     \"Model\" : [],\n",
    "#     \"Dataset\" : [],\n",
    "#     \"Fold\" : [],\n",
    "#     \"Accuracy\" : [],\n",
    "#     \"Precision\": [],\n",
    "#     \"TPR\": [],\n",
    "#     \"FPR\": [],\n",
    "#     \"TPR_FPR_Thresholds\": [],\n",
    "#     \"AUC\": [],\n",
    "#     \"Sensitivity\": [],\n",
    "#     \"Specificity\": [],\n",
    "#     \"MCC\":[]\n",
    "# }\n",
    "\n",
    "# sum_evaluations = {\n",
    "#     \"Model\" : [],\n",
    "#     \"Dataset\" : [],\n",
    "#     \"Accuracy\" : [],\n",
    "#     \"Precision\": [],\n",
    "#     \"TPR\": [],\n",
    "#     \"FPR\": [],\n",
    "#     \"TPR_FPR_Thresholds\": [],\n",
    "#     \"AUC\": [],\n",
    "#     \"Sensitivity\": [],\n",
    "#     \"Specificity\": [],\n",
    "#     \"MCC\":[]\n",
    "# }\n",
    "\n",
    "# vote_evaluations = {\n",
    "#     \"Model\" : [],\n",
    "#     \"Dataset\" : [],\n",
    "#     \"Accuracy\" : [],\n",
    "#     \"Precision\": [],\n",
    "#     \"TPR\": [],\n",
    "#     \"FPR\": [],\n",
    "#     \"TPR_FPR_Thresholds\": [],\n",
    "#     \"AUC\": [],\n",
    "#     \"Sensitivity\": [],\n",
    "#     \"Specificity\": [],\n",
    "#     \"MCC\":[]\n",
    "# }\n",
    "\n",
    "# for root, dirs, files in os.walk(independent_data_folder):\n",
    "#     for file in files:\n",
    "        \n",
    "#         input_data_file = os.path.join(root, file)\n",
    "        \n",
    "#         if 'HS' in file:\n",
    "#             bench_data = 'HS_990'\n",
    "#         elif 'SS' in file:\n",
    "#             bench_data = 'SS_628'\n",
    "            \n",
    "#         current_dataset_variety = input_data_file.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "        \n",
    "#         openFile = open(input_data_file)\n",
    "#         fastaSequences = SeqIO.parse(openFile, \"fasta\")\n",
    "        \n",
    "#         ##################################################################################\n",
    "#         ##### extract data from the current fasta file\n",
    "#         ##################################################################################\n",
    "\n",
    "#         positive_List = []\n",
    "#         negative_List = []\n",
    "#         positive_onehotencoded_List = []\n",
    "#         negative_onehotencoded_List = []\n",
    "\n",
    "#         for fasta in fastaSequences: \n",
    "#             name, sequence = fasta.id, str(fasta.seq)\n",
    "#             if \"P\" in name:\n",
    "#                 positive_List.append(sequence)\n",
    "#                 aus_seq = one_hot_encode_rna(sequence)\n",
    "#                 if(len(aus_seq) != 0):\n",
    "#                     positive_onehotencoded_List.append(aus_seq)\n",
    "#             elif \"N\" in name:\n",
    "#                 negative_List.append(sequence)\n",
    "#                 aus_seq = one_hot_encode_rna(sequence)\n",
    "#                 if(len(aus_seq) != 0):\n",
    "#                     negative_onehotencoded_List.append(aus_seq)\n",
    "\n",
    "#         openFile.close()\n",
    "\n",
    "#         print(\"\\n======================================================================\")\n",
    "#         print(\"\\nFile: \"+os.path.join(root, file))\n",
    "#         print(\"Positive: \"+str(len(positive_onehotencoded_List)))\n",
    "#         print(\"Negative: \"+str(len(negative_onehotencoded_List)))\n",
    "        \n",
    "#         ##################################################################################\n",
    "#         ##### Generate Folds from dataset, and store to file\n",
    "#         ##################################################################################\n",
    "\n",
    "#         ## create the features and labels datasets for the training\n",
    "#         labels = np.concatenate((np.ones((len(positive_onehotencoded_List), 1), dtype=np.float32), np.zeros((len(negative_onehotencoded_List), 1), dtype=np.float32)), axis=0)\n",
    "#         features = np.concatenate((positive_onehotencoded_List,negative_onehotencoded_List), 0)\n",
    "        \n",
    "#         benchModelPath = os.path.join(outPath, expName, bench_data, \"{}fold\".format(n_fold), \"models\")\n",
    "            \n",
    "#         ##################################################################################\n",
    "#         ##### TRAIN and PREDICT for every Fold, using models\n",
    "#         ##################################################################################\n",
    "        \n",
    "#         y_pred_list = []\n",
    "\n",
    "#         for fold in range(n_fold):\n",
    "\n",
    "#             print(\"\\nIndependent test on \"+current_dataset_variety+\" using Fold #\"+str(fold)+\" model from \"+bench_data+\".\")\n",
    "            \n",
    "#             current_model_path = os.path.join(\n",
    "#                 benchModelPath, \n",
    "#                 \"{}_bestModel-fold{}.hdf5\".format(bench_data, fold)\n",
    "#             )\n",
    "            \n",
    "#             model = tf.keras.models.load_model(current_model_path)\n",
    "\n",
    "#             ##################################################################################\n",
    "#             ##### Prediction and metrics for TEST dataset\n",
    "#             ##################################################################################\n",
    "            \n",
    "#             y_pred = model.predict(features)\n",
    "#             y_pred_list.append(y_pred)\n",
    "#             label_pred = pred2label(y_pred)\n",
    "#             # Compute precision, recall, sensitivity, specifity, mcc\n",
    "#             acc = accuracy_score(labels, label_pred)\n",
    "#             prec = precision_score(labels,label_pred)\n",
    "\n",
    "#             conf = confusion_matrix(labels, label_pred)\n",
    "#             if(conf[0][0]+conf[1][0]):\n",
    "#                 sens = float(conf[0][0])/float(conf[0][0]+conf[1][0])\n",
    "#             else:\n",
    "#                 sens = 0.0\n",
    "#             if(conf[1][1]+conf[0][1]):\n",
    "#                 spec = float(conf[1][1])/float(conf[1][1]+conf[0][1])\n",
    "#             else:\n",
    "#                 spec = 0.0\n",
    "#             if((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0])):\n",
    "#                 mcc = (float(conf[0][0])*float(conf[1][1]) - float(conf[1][0])*float(conf[0][1]))/math.sqrt((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0]))\n",
    "#             else:\n",
    "#                 mcc= 0.0\n",
    "#             fpr, tpr, thresholds = roc_curve(labels, y_pred)\n",
    "#             auc = roc_auc_score(labels, y_pred)\n",
    "\n",
    "#             evaluations[\"Model\"].append(current_dataset_variety)\n",
    "#             evaluations[\"Dataset\"].append(current_dataset_variety)\n",
    "#             evaluations[\"Fold\"].append(i)\n",
    "#             evaluations[\"Accuracy\"].append(acc)\n",
    "#             evaluations[\"Precision\"].append(prec)\n",
    "#             evaluations[\"TPR\"].append(tpr)\n",
    "#             evaluations[\"FPR\"].append(fpr)\n",
    "#             evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "#             evaluations[\"AUC\"].append(auc)\n",
    "#             evaluations[\"Sensitivity\"].append(sens)\n",
    "#             evaluations[\"Specificity\"].append(spec)\n",
    "#             evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "#             del model\n",
    "#             tf.keras.backend.clear_session()\n",
    "            \n",
    "#         ##################################################################################\n",
    "#         ##### Prediction and metrics using sum of all folds\n",
    "#         ##################################################################################\n",
    "        \n",
    "#         y_pred_list_arr = np.swapaxes(np.array(y_pred_list), 0,1)\n",
    "#         y_pred_vote = np.mean(y_pred_list_arr, axis = 1)\n",
    "#         label_vote = pred2label(y_pred_vote)\n",
    "        \n",
    "#         # Compute precision, recall, sensitivity, specifity, mcc\n",
    "#         acc = accuracy_score(labels, label_vote)\n",
    "#         prec = precision_score(labels, label_vote)\n",
    "\n",
    "#         conf = confusion_matrix(labels, label_vote)\n",
    "#         if(conf[0][0]+conf[1][0]):\n",
    "#             sens = float(conf[0][0])/float(conf[0][0]+conf[1][0])\n",
    "#         else:\n",
    "#             sens = 0.0\n",
    "#         if(conf[1][1]+conf[0][1]):\n",
    "#             spec = float(conf[1][1])/float(conf[1][1]+conf[0][1])\n",
    "#         else:\n",
    "#             spec = 0.0\n",
    "#         if((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0])):\n",
    "#             mcc = (float(conf[0][0])*float(conf[1][1]) - float(conf[1][0])*float(conf[0][1]))/math.sqrt((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0]))\n",
    "#         else:\n",
    "#             mcc= 0.0\n",
    "#         fpr, tpr, thresholds = roc_curve(labels, y_pred_vote)\n",
    "#         auc = roc_auc_score(labels, y_pred_vote)\n",
    "        \n",
    "#         sum_evaluations[\"Model\"].append(current_dataset_variety)\n",
    "#         sum_evaluations[\"Dataset\"].append(current_dataset_variety)\n",
    "#         sum_evaluations[\"Accuracy\"].append(acc)\n",
    "#         sum_evaluations[\"Precision\"].append(prec)\n",
    "#         sum_evaluations[\"TPR\"].append(tpr)\n",
    "#         sum_evaluations[\"FPR\"].append(fpr)\n",
    "#         sum_evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "#         sum_evaluations[\"AUC\"].append(auc)\n",
    "#         sum_evaluations[\"Sensitivity\"].append(sens)\n",
    "#         sum_evaluations[\"Specificity\"].append(spec)\n",
    "#         sum_evaluations[\"MCC\"].append(mcc)\n",
    "        \n",
    "#         ##################################################################################\n",
    "#         ##### Prediction and metrics using vote of all folds\n",
    "#         ##################################################################################\n",
    "        \n",
    "#         y_pred_list_arr = np.swapaxes(np.array(y_pred_list), 0,1)\n",
    "#         y_pred_vote = np.sum(np.round(y_pred_list_arr), axis = 1)\n",
    "#         label_vote = (y_pred_vote > 5).astype(int)\n",
    "        \n",
    "#         # Compute precision, recall, sensitivity, specifity, mcc\n",
    "#         acc = accuracy_score(labels, label_vote)\n",
    "#         prec = precision_score(labels, label_vote)\n",
    "\n",
    "#         conf = confusion_matrix(labels, label_vote)\n",
    "#         if(conf[0][0]+conf[1][0]):\n",
    "#             sens = float(conf[0][0])/float(conf[0][0]+conf[1][0])\n",
    "#         else:\n",
    "#             sens = 0.0\n",
    "#         if(conf[1][1]+conf[0][1]):\n",
    "#             spec = float(conf[1][1])/float(conf[1][1]+conf[0][1])\n",
    "#         else:\n",
    "#             spec = 0.0\n",
    "#         if((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0])):\n",
    "#             mcc = (float(conf[0][0])*float(conf[1][1]) - float(conf[1][0])*float(conf[0][1]))/math.sqrt((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0]))\n",
    "#         else:\n",
    "#             mcc= 0.0\n",
    "#         fpr, tpr, thresholds = roc_curve(labels, y_pred_vote)\n",
    "#         auc = roc_auc_score(labels, y_pred_vote)\n",
    "\n",
    "#         vote_evaluations[\"Model\"].append(current_dataset_variety)\n",
    "#         vote_evaluations[\"Dataset\"].append(current_dataset_variety)\n",
    "#         vote_evaluations[\"Accuracy\"].append(acc)\n",
    "#         vote_evaluations[\"Precision\"].append(prec)\n",
    "#         vote_evaluations[\"TPR\"].append(tpr)\n",
    "#         vote_evaluations[\"FPR\"].append(fpr)\n",
    "#         vote_evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "#         vote_evaluations[\"AUC\"].append(auc)\n",
    "#         vote_evaluations[\"Sensitivity\"].append(sens)\n",
    "#         vote_evaluations[\"Specificity\"].append(spec)\n",
    "#         vote_evaluations[\"MCC\"].append(mcc)\n",
    "        \n",
    "#         ##################################################################################\n",
    "#         ##### Dump evaluations to a file\n",
    "#         ##################################################################################\n",
    "\n",
    "#         evalPath = os.path.join(outPath, expName, \"_Evaluation_Independent_Datasets\")\n",
    "#         if(not os.path.isdir(evalPath)):\n",
    "#             os.makedirs(evalPath)\n",
    "\n",
    "#         pickle.dump(evaluations,\n",
    "#                     open(os.path.join(evalPath, \"{}fold_evaluations.pickle\".format(n_fold)), \"wb\"))\n",
    "        \n",
    "#         pickle.dump(sum_evaluations,\n",
    "#                     open(os.path.join(evalPath, \"{}fold_sum_evaluations.pickle\".format(n_fold)), \"wb\"))\n",
    "        \n",
    "#         pickle.dump(vote_evaluations,\n",
    "#                     open(os.path.join(evalPath, \"{}fold_vote_evaluations.pickle\".format(n_fold)), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using each fold, average result of all 10 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluations_df = pd.DataFrame.from_dict(evaluations)\n",
    "\n",
    "# evaluations_df_grouped = evaluations_df.groupby([\"Dataset\", \n",
    "#                                                  \"Model\"]).mean().filter(['Accuracy', \n",
    "#                                                                            'Precision', \n",
    "#                                                                            'AUC', \n",
    "#                                                                            'Sensitivity', \n",
    "#                                                                            'Specificity', \n",
    "#                                                                            'MCC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluations_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\tModel\tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# HS_200\tHS_200\t0.6780\t0.676877\t0.72104\t0.684022\t0.676877\t0.358413\n",
    "# SS_200\tSS_200\t0.6765\t0.646861\t0.75923\t0.733891\t0.646861\t0.366287"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                 Accuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# Dataset\tModel\t\t\t\t\t\t\n",
    "# HS_200\tHS_200\t0.6750\t0.664200\t0.72708\t0.693586\t0.664200\t0.353822\n",
    "# SS_200\tSS_200\t0.6975\t0.674367\t0.76201\t0.731014\t0.674367\t0.400129"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using 1 fold, average result of top 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluations_df5 = evaluations_df.sort_values(['Accuracy'],ascending=False).groupby([\"Dataset\", \n",
    "#                                                                                     \"Model\"]).head(5).reset_index()\n",
    "\n",
    "# evaluations_df5_grouped = evaluations_df5.groupby([\"Dataset\", \n",
    "#                                                    \"Model\"]).mean().filter(['Accuracy', \n",
    "#                                                                             'Precision', \n",
    "#                                                                             'AUC', \n",
    "#                                                                             'Sensitivity', \n",
    "#                                                                             'Specificity', \n",
    "#                                                                             'MCC']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluations_df5_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\tModel\tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# 0\tHS_200\tHS_200\t0.704\t0.697657\t0.73026\t0.712350\t0.697657\t0.409001\n",
    "# 1\tSS_200\tSS_200\t0.701\t0.679132\t0.76770\t0.731744\t0.679132\t0.406404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \tDataset\tModel\tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# 0\tHS_200\tHS_200\t0.690\t0.690418\t0.72364\t0.690651\t0.690418\t0.380534\n",
    "# 1\tSS_200\tSS_200\t0.714\t0.695143\t0.76684\t0.738915\t0.695143\t0.431015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using all 10 folds, vote using sum of scores of all 10 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_evaluations_df = pd.DataFrame.from_dict(sum_evaluations)\n",
    "\n",
    "# sum_evaluations_df.filter([\"Dataset\", \"Model\", 'Accuracy', 'Precision', 'AUC', 'Sensitivity', 'Specificity', 'MCC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \tDataset\tModel\tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# 0\tHS_200\tHS_200\t0.705\t0.702970\t0.7352\t0.707071\t0.702970\t0.410021\n",
    "# 1\tSS_200\tSS_200\t0.710\t0.666667\t0.7834\t0.783784\t0.666667\t0.434959"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \tDataset\tModel\tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# 0\tHS_200\tHS_200\t0.695\t0.685714\t0.7445\t0.705263\t0.685714\t0.390488\n",
    "# 1\tSS_200\tSS_200\t0.705\t0.672269\t0.7873\t0.753086\t0.672269\t0.417607"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using all 10 folds, vote using absolute vote of all 10 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vote_evaluations_df = pd.DataFrame.from_dict(vote_evaluations)\n",
    "\n",
    "# vote_evaluations_df.filter([\"Dataset\", \"Model\", 'Accuracy', 'Precision', 'AUC', 'Sensitivity', 'Specificity', 'MCC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Dataset\tModel\tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# 0\tHS_200\tHS_200\t0.715\t0.712871\t0.72785\t0.717172\t0.712871\t0.430022\n",
    "# 1\tSS_200\tSS_200\t0.695\t0.666667\t0.76455\t0.734940\t0.666667\t0.395761"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
