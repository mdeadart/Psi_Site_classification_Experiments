{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DReqRQWukwXk"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "try:\n",
        "  shutil.rmtree('Results_Colab')\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97025HQwhzYZ",
        "outputId": "e990e2e8-f918-4b11-fa7d-1dc56652b7b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "USPcSZs0H4o2"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U keras-tuner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyofMbfnIWJL"
      },
      "source": [
        "# Import Statements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qjzvQAV9hh0-"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import keras_tuner as kt\n",
        "\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkOCobMPISQz"
      },
      "source": [
        "# Define Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VNfiLnM7hh09"
      },
      "outputs": [],
      "source": [
        "##################################################################################\n",
        "##### Define all parameters for model tuning\n",
        "##################################################################################\n",
        "\n",
        "expName = \"PSI_Site_DLNN_MergedSeq_2DOP_autoencoder_kerasTuner\"\n",
        "\n",
        "input_data_folder = \"/content/drive/MyDrive/_Uni/5. Thesis/Thesis_Work/Psi_Site_Data/Aziz\"\n",
        "drive_out_path = \"/content/drive/MyDrive/_Uni/5. Thesis/Thesis_Work/Results_Colab\"\n",
        "# outPath = 'Results_Colab'\n",
        "outPath = \"/content/drive/MyDrive/_Uni/5. Thesis/Thesis_Work/Results_Colab\"\n",
        "\n",
        "output_data_folder = os.path.join(outPath, expName)\n",
        "\n",
        "p_epoch = 30\n",
        "p_factor = 2\n",
        "# p_objective = 'val_sequential_loss'\n",
        "objective = \"val_sequential_loss\"\n",
        "p_objective = kt.Objective(objective, direction=\"min\")\n",
        "p_hyperband_iterations = 3\n",
        "\n",
        "p_max_trials = 100\n",
        "p_num_initial_points = 25\n",
        "\n",
        "p_batch_size = 32\n",
        "p_final_epochs = 30\n",
        "\n",
        "n_fold = 10\n",
        "foldName = \"folds.pickle\"\n",
        "\n",
        "epochs = 100\n",
        "batch_size = 32\n",
        "shuffle = True\n",
        "seed = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Il334C4INB8"
      },
      "source": [
        "# Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uSC8K0dXhh1A"
      },
      "outputs": [],
      "source": [
        "##################################################################################\n",
        "##### define all CUSTOM functions\n",
        "##################################################################################\n",
        "\n",
        "def one_hot_encode_dna(sequence):\n",
        "    seq_encoded = np.zeros((len(sequence),4))\n",
        "    dict_nuc = {\n",
        "        \"A\": 0,\n",
        "        \"C\": 1,\n",
        "        \"G\": 2,\n",
        "        \"T\": 3\n",
        "    }\n",
        "    i = 0\n",
        "    for single_character in sequence:\n",
        "        if(single_character.upper() in dict_nuc.keys()):\n",
        "            seq_encoded[i][dict_nuc[single_character.upper()]] = 1\n",
        "            i = i+1\n",
        "        else:\n",
        "            raise ValueError('Incorrect character in DNA sequence: '+sequence)\n",
        "    return seq_encoded\n",
        "\n",
        "def one_hot_encode_rna(sequence):\n",
        "    seq_encoded = np.zeros((len(sequence),4))\n",
        "    dict_nuc = {\n",
        "        \"A\": 0,\n",
        "        \"C\": 1,\n",
        "        \"G\": 2,\n",
        "        \"U\": 3\n",
        "    }\n",
        "    i = 0\n",
        "    for single_character in sequence:\n",
        "        if(single_character.upper() in dict_nuc.keys()):\n",
        "            seq_encoded[i][dict_nuc[single_character.upper()]] = 1\n",
        "            i = i+1\n",
        "        else:\n",
        "            raise ValueError('Incorrect character in RNA sequence: '+sequence)\n",
        "    return seq_encoded\n",
        "\n",
        "def one_hot_encode_rnafold(sequence):\n",
        "    seq_encoded = np.zeros((len(sequence),3))\n",
        "    dict_fold = {\n",
        "        \"(\": 0,\n",
        "        \")\": 1,\n",
        "        \".\": 2\n",
        "    }\n",
        "    i = 0\n",
        "    for single_character in sequence:\n",
        "        if(single_character in dict_fold.keys()):\n",
        "            seq_encoded[i][dict_fold[single_character]] = 1\n",
        "            i = i+1\n",
        "        else:\n",
        "            raise ValueError('Incorrect character in RNAfold: '+sequence)\n",
        "    return seq_encoded\n",
        "\n",
        "def one_hot_encode_rna_mergedseq(sequence):\n",
        "    dict_nuc = {\n",
        "        \"A\": 0,\n",
        "        \"C\": 1,\n",
        "        \"G\": 2,\n",
        "        \"U\": 3\n",
        "    }\n",
        "    dict_fold = {\n",
        "        \"(\": 0,\n",
        "        \")\": 1,\n",
        "        \".\": 2\n",
        "    }\n",
        "    list_seq = sequence.strip().split(' ')\n",
        "    seq_encoded = np.zeros((len(list_seq),12))\n",
        "    i = 0\n",
        "    for single_character in list_seq:\n",
        "        if(single_character[0].upper() in dict_nuc.keys()):\n",
        "            idx1 = dict_nuc[single_character[0].upper()]+1\n",
        "        else:\n",
        "            raise ValueError('Incorrect RNA character in MergedSeq sequence: '+sequence)\n",
        "        if(single_character[1] in dict_fold.keys()):\n",
        "            idx2 = dict_fold[single_character[1]]+1\n",
        "        else:\n",
        "            raise ValueError('Incorrect RNAfold character in MergedSeq sequence: '+sequence)\n",
        "        idx = (idx1 * idx2) - 1\n",
        "        seq_encoded[i][idx] = 1\n",
        "        i = i+1        \n",
        "    return seq_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bB48pbkthh1C"
      },
      "outputs": [],
      "source": [
        "##################################################################################\n",
        "##### define evaluator functions\n",
        "##################################################################################\n",
        "\n",
        "## Build the K-fold from dataset\n",
        "def build_kfold(features, labels, k=10, shuffle=False, seed=None):\n",
        "    \n",
        "    skf = StratifiedKFold(n_splits=k, shuffle=shuffle, random_state=seed)\n",
        "    kfoldList = []\n",
        "    for train_index, test_index in skf.split(features, labels):\n",
        "        X_train, X_test = features[train_index], features[test_index]\n",
        "        y_train, y_test = labels[train_index], labels[test_index]\n",
        "        kfoldList.append({\n",
        "            \"X_train\": X_train,\n",
        "            \"X_test\": X_test,\n",
        "            \"y_train\":y_train,\n",
        "            \"y_test\":y_test\n",
        "        })\n",
        "    return kfoldList\n",
        "\n",
        "def build_kfold_multifeature(features_1, features_2, labels, k=10, shuffle=False, seed=None):\n",
        "    \n",
        "    skf = StratifiedKFold(n_splits=k, shuffle=shuffle, random_state=seed)\n",
        "    kfoldList = []\n",
        "    for train_index, test_index in skf.split(features_1, labels):\n",
        "        X1_train, X1_test = features_1[train_index], features_1[test_index]\n",
        "        X2_train, X2_test = features_2[train_index], features_2[test_index]\n",
        "        y_train, y_test = labels[train_index], labels[test_index]\n",
        "        kfoldList.append({\n",
        "            \"X1_train\": X1_train,\n",
        "            \"X1_test\": X1_test,\n",
        "            \"X2_train\": X2_train,\n",
        "            \"X2_test\": X2_test,\n",
        "            \"y_train\":y_train,\n",
        "            \"y_test\":y_test\n",
        "        })\n",
        "    return kfoldList\n",
        "\n",
        "def pred2label(y_pred):\n",
        "    y_pred = np.round(y_pred).astype(int)\n",
        "    return y_pred\n",
        "\n",
        "def label_scalar_to_vector(label):\n",
        "    label_2d = np.zeros((label.shape[0], 2))\n",
        "    for i in range(label.shape[0]):\n",
        "        label_2d[i][int(label[i][0])] = 1\n",
        "    return label_2d\n",
        "\n",
        "def label_vector_to_scalar(label_2d_arr):\n",
        "    labels = (label_2d_arr[:,1] > label_2d_arr[:,0]).astype(int)\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wGFIOcaJieBP"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def save_plot2(H, path, loss_name):\n",
        "    # plot the training loss and accuracy\n",
        "    plt.style.use(\"ggplot\")\n",
        "\n",
        "    if(not os.path.isdir(path)):\n",
        "        os.makedirs(path)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(H.history[loss_name+\"_loss\"], label=\"train_loss\")\n",
        "    plt.plot(H.history[\"val_\"+loss_name+\"_loss\"], label=\"val_loss\")\n",
        "    plt.title(\"Training Loss\")\n",
        "    plt.xlabel(\"Epoch #\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(path, 'loss.png'))\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(H.history[loss_name+\"_accuracy\"], label=\"train_acc\")\n",
        "    plt.plot(H.history[\"val_\"+loss_name+\"_accuracy\"], label=\"val_acc\")\n",
        "    plt.title(\"Training Accuracy\")\n",
        "    plt.xlabel(\"Epoch #\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(path, 'accuracy.png'))\n",
        "\n",
        "def save_plot(H, path):\n",
        "    # plot the training loss and accuracy\n",
        "    plt.style.use(\"ggplot\")\n",
        "\n",
        "    if(not os.path.isdir(path)):\n",
        "        os.makedirs(path)\n",
        "\n",
        "    seq_model_no = [s for s in list(H.history.keys()) if \"sequential\" in s][0].split('_')[1]\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(H.history['sequential_{}_loss'.format(seq_model_no)], label=\"train_loss\")\n",
        "    plt.plot(H.history['val_sequential_{}_loss'.format(seq_model_no)], label=\"val_loss\")\n",
        "    plt.title(\"Training Loss\")\n",
        "    plt.xlabel(\"Epoch #\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(path, 'loss.png'))\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(H.history['sequential_{}_accuracy'.format(seq_model_no)], label=\"train_acc\")\n",
        "    plt.plot(H.history['val_sequential_{}_accuracy'.format(seq_model_no)], label=\"val_acc\")\n",
        "    plt.title(\"Training Accuracy\")\n",
        "    plt.xlabel(\"Epoch #\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(path, 'accuracy.png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bamwvpxsIKbc"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bskcG3MAIQUU"
      },
      "outputs": [],
      "source": [
        "##################################################################################\n",
        "##### Function to customize the DLNN architecture with parameters\n",
        "##################################################################################\n",
        "\n",
        "def model_builder_21(hp):\n",
        "    \n",
        "    hp_beta = hp.Choice('beta', values=[0.01, 0.001, 0.0001])\n",
        "    \n",
        "    hp_conv_activation = hp.Choice('conv_activation', values=['relu', 'selu', 'tanh'])\n",
        "    hp_conv_filters_per_layer = hp.Int('conv_filters_per_layer', min_value=5, max_value=25, step=5)\n",
        "    hp_conv_kernel_length_1 = hp.Choice('max_kernel_length_1', values=[3, 5, 7, 9])\n",
        "    hp_conv_kernel_length_2 = hp.Choice('max_kernel_length_2', values=[3, 5, 7, 9])\n",
        "    # hp_conv_stride = hp.Choice('conv_stride', values=[1, 2, 3])\n",
        "    hp_conv_stride = 1\n",
        "\n",
        "    hp_latent_dim = hp.Choice('count_conv_layers', values=[10, 20, 30])\n",
        "    \n",
        "    hp_count_dense_layers = hp.Choice('count_dense_layers', values=[1, 2, 3])\n",
        "    hp_dense_units = hp.Int('dense_units_1', min_value=100, max_value=500, step=100)\n",
        "    hp_dropout_prob = hp.Choice('dropout_prob', values=[0.1, 0.3, 0.5])\n",
        "    \n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[0.01, 0.001, 0.0001, 0.00001])\n",
        "    hp_opt_func = hp.Choice('opt_func', values=['adam', 'adagrad', 'rmsprop'])\n",
        "    \n",
        "    input_shape = (21, 12)\n",
        "    \n",
        "    ae_input = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "    ###########################################################################\n",
        "    ##### Encoder\n",
        "    ###########################################################################\n",
        "\n",
        "    xe = tf.keras.layers.Conv1D(hp_conv_filters_per_layer, hp_conv_kernel_length_1,\n",
        "                                strides = hp_conv_stride,\n",
        "                                # kernel_regularizer=tf.keras.regularizers.l2(hp_beta),\n",
        "                                # activation=hp_conv_activation\n",
        "                               )(ae_input)\n",
        "\n",
        "    xe = tf.keras.layers.Conv1D(hp_conv_filters_per_layer, hp_conv_kernel_length_2,\n",
        "                                strides = hp_conv_stride,\n",
        "                                # kernel_regularizer=tf.keras.regularizers.l2(hp_beta),\n",
        "                                # activation=hp_conv_activation\n",
        "                               )(xe)\n",
        "    \n",
        "    xe = tf.keras.layers.Flatten()(xe)\n",
        "    xe = tf.keras.layers.Dense(hp_latent_dim)(xe)\n",
        "\n",
        "    encoder = tf.keras.models.Model(inputs=ae_input, outputs=xe)\n",
        "    \n",
        "    ###########################################################################\n",
        "    ##### Decoder\n",
        "    ###########################################################################\n",
        "    \n",
        "    dec_input = tf.keras.layers.Input(shape=(hp_latent_dim,))\n",
        "    \n",
        "    xd = tf.keras.layers.RepeatVector(input_shape[0]-hp_conv_kernel_length_1-hp_conv_kernel_length_2+2)(dec_input)\n",
        "\n",
        "    xd = tf.keras.layers.Conv1DTranspose(hp_conv_filters_per_layer, hp_conv_kernel_length_2,\n",
        "                                         strides = hp_conv_stride,\n",
        "                                        #  kernel_regularizer=tf.keras.regularizers.l2(hp_beta),\n",
        "                                         # activation='relu'\n",
        "                                        )(xd)\n",
        "\n",
        "    xd = tf.keras.layers.Conv1DTranspose(4, hp_conv_kernel_length_1,\n",
        "                                         strides = hp_conv_stride,\n",
        "                                        #  kernel_regularizer=tf.keras.regularizers.l2(hp_beta)\n",
        "                                        )(xd)\n",
        "\n",
        "    xd = tf.keras.layers.Activation('softmax')(xd)\n",
        "\n",
        "    decoder = tf.keras.models.Model(inputs=dec_input, outputs=xd)\n",
        "    \n",
        "    ###########################################################################\n",
        "    ##### Classifier\n",
        "    ###########################################################################\n",
        "    \n",
        "    classifier = tf.keras.models.Sequential()\n",
        "\n",
        "    classifier.add(tf.keras.layers.Input(shape=(hp_latent_dim,)))\n",
        "\n",
        "    for i in range(hp_count_dense_layers):\n",
        "        classifier.add(tf.keras.layers.Dense(hp_dense_units/(i+1),\n",
        "                                             kernel_regularizer=tf.keras.regularizers.l2(hp_beta)))\n",
        "        classifier.add(tf.keras.layers.BatchNormalization())\n",
        "        # classifier.add(tf.keras.layers.Activation(hp_dense_activation))\n",
        "        classifier.add(tf.keras.layers.Dropout(hp_dropout_prob))\n",
        "\n",
        "    classifier.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "    #########################\n",
        "    ##### Generate Model from input and output\n",
        "    #########################\n",
        "    \n",
        "    autoencoder = tf.keras.models.Model(ae_input, [decoder(encoder(ae_input)), classifier(encoder(ae_input))])\n",
        "\n",
        "    if hp_opt_func == 'adam':\n",
        "        optimizer_function = tf.keras.optimizers.Adam(learning_rate=hp_learning_rate, epsilon = 0.01)\n",
        "    elif hp_opt_func == 'adagrad':\n",
        "        optimizer_function = tf.keras.optimizers.Adagrad(learning_rate=hp_learning_rate)\n",
        "    elif hp_opt_func == 'rmsprop':\n",
        "        optimizer_function = tf.keras.optimizers.RMSprop(learning_rate=hp_learning_rate)\n",
        "\n",
        "    autoencoder.compile(optimizer=optimizer_function, \n",
        "                        loss={autoencoder.output[0].name.split('/')[0]: 'categorical_crossentropy', \n",
        "                              autoencoder.output[1].name.split('/')[0]: 'categorical_crossentropy'}, \n",
        "                        metrics='accuracy')\n",
        "    \n",
        "    return autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "G2zWkM-o9qHD"
      },
      "outputs": [],
      "source": [
        "# ##################################################################################\n",
        "# ##### Function to customize the DLNN architecture with parameters\n",
        "# ##################################################################################\n",
        "\n",
        "# def model_builder_test():\n",
        "    \n",
        "#     hp_beta = 0.01\n",
        "    \n",
        "#     hp_conv_activation = 'relu'\n",
        "#     hp_conv_filters_per_layer = 5\n",
        "#     hp_conv_kernel_length_1 = 5\n",
        "#     hp_conv_kernel_length_2 = 5\n",
        "#     hp_conv_stride = 1\n",
        "\n",
        "#     hp_latent_dim = 10\n",
        "    \n",
        "#     hp_count_dense_layers = 3\n",
        "#     hp_dense_units = 100\n",
        "#     hp_dropout_prob = 0.1, 0.3, 0.5\n",
        "    \n",
        "#     hp_learning_rate = 0.01\n",
        "#     hp_opt_func = 'adam'\n",
        "    \n",
        "#     input_shape = (21, 12)\n",
        "    \n",
        "#     ae_input = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "#     ###########################################################################\n",
        "#     ##### Encoder\n",
        "#     ###########################################################################\n",
        "\n",
        "#     xe = tf.keras.layers.Conv1D(hp_conv_filters_per_layer, hp_conv_kernel_length_1,\n",
        "#                                 strides = hp_conv_stride,\n",
        "#                                 kernel_regularizer=tf.keras.regularizers.l2(hp_beta),\n",
        "#                                 activation=hp_conv_activation\n",
        "#                                )(ae_input)\n",
        "\n",
        "#     xe = tf.keras.layers.Conv1D(hp_conv_filters_per_layer, hp_conv_kernel_length_2,\n",
        "#                                 strides = hp_conv_stride,\n",
        "#                                 kernel_regularizer=tf.keras.regularizers.l2(hp_beta),\n",
        "#                                 activation=hp_conv_activation\n",
        "#                                )(xe)\n",
        "    \n",
        "#     xe = tf.keras.layers.Flatten()(xe)\n",
        "#     xe = tf.keras.layers.Dense(hp_latent_dim)(xe)\n",
        "\n",
        "#     encoder = tf.keras.models.Model(inputs=ae_input, outputs=xe)\n",
        "    \n",
        "#     ###########################################################################\n",
        "#     ##### Decoder\n",
        "#     ###########################################################################\n",
        "    \n",
        "#     dec_input = tf.keras.layers.Input(shape=(hp_latent_dim,))\n",
        "    \n",
        "#     xd = tf.keras.layers.RepeatVector(input_shape[0]-hp_conv_kernel_length_1-hp_conv_kernel_length_2+2)(dec_input)\n",
        "\n",
        "#     xd = tf.keras.layers.Conv1DTranspose(hp_conv_filters_per_layer, hp_conv_kernel_length_2,\n",
        "#                                          strides = hp_conv_stride,\n",
        "#                                          kernel_regularizer=tf.keras.regularizers.l2(hp_beta),\n",
        "#                                          # activation='relu'\n",
        "#                                         )(xd)\n",
        "\n",
        "#     xd = tf.keras.layers.Conv1DTranspose(4, hp_conv_kernel_length_1,\n",
        "#                                          strides = hp_conv_stride,\n",
        "#                                          kernel_regularizer=tf.keras.regularizers.l2(hp_beta)\n",
        "#                                         )(xd)\n",
        "\n",
        "#     xd = tf.keras.layers.Activation('softmax')(xd)\n",
        "\n",
        "#     decoder = tf.keras.models.Model(inputs=dec_input, outputs=xd)\n",
        "    \n",
        "#     ###########################################################################\n",
        "#     ##### Classifier\n",
        "#     ###########################################################################\n",
        "    \n",
        "#     classifier = tf.keras.models.Sequential()\n",
        "\n",
        "#     classifier.add(tf.keras.layers.Input(shape=(hp_latent_dim,)))\n",
        "\n",
        "#     for i in range(hp_count_dense_layers):\n",
        "#         classifier.add(tf.keras.layers.Dense(hp_dense_units/(i+1),\n",
        "#                                              kernel_regularizer=tf.keras.regularizers.l2(hp_beta)))\n",
        "#         classifier.add(tf.keras.layers.BatchNormalization())\n",
        "#         # classifier.add(tf.keras.layers.Activation(hp_dense_activation))\n",
        "#         classifier.add(tf.keras.layers.Dropout(hp_dropout_prob))\n",
        "\n",
        "#     classifier.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "#     #########################\n",
        "#     ##### Generate Model from input and output\n",
        "#     #########################\n",
        "    \n",
        "#     autoencoder = tf.keras.models.Model(ae_input, [decoder(encoder(ae_input)), classifier(encoder(ae_input))])\n",
        "\n",
        "#     if hp_opt_func == 'adam':\n",
        "#         optimizer_function = tf.keras.optimizers.Adam(learning_rate=hp_learning_rate, epsilon = 0.01)\n",
        "#     elif hp_opt_func == 'adagrad':\n",
        "#         optimizer_function = tf.keras.optimizers.Adagrad(learning_rate=hp_learning_rate)\n",
        "#     elif hp_opt_func == 'rmsprop':\n",
        "#         optimizer_function = tf.keras.optimizers.RMSprop(learning_rate=hp_learning_rate)\n",
        "\n",
        "#     autoencoder.compile(optimizer=optimizer_function, \n",
        "#                         loss={autoencoder.output[0].name.split('/')[0]: 'categorical_crossentropy', \n",
        "#                               autoencoder.output[1].name.split('/')[0]: 'categorical_crossentropy'}, \n",
        "#                         metrics='accuracy')\n",
        "    \n",
        "#     return autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "s3MXEFp295W7"
      },
      "outputs": [],
      "source": [
        "# tf.keras.utils.plot_model(model_builder_test())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDTY6gAaHRU3"
      },
      "source": [
        "# HS_990"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter optimization"
      ],
      "metadata": {
        "id": "k2uhuhJo1zJo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unWB8PIUzVNb",
        "outputId": "bc823676-9d3a-498a-823c-acdabddabd0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "File: /content/drive/MyDrive/_Uni/5. Thesis/Thesis_Work/Psi_Site_Data/Aziz/HS_990.csv\n",
            "Positive: 495\n",
            "Negative: 495\n"
          ]
        }
      ],
      "source": [
        "###########################################################################\n",
        "##### Prepare dataset\n",
        "###########################################################################\n",
        "\n",
        "file = 'HS_990.csv'\n",
        "input_data_file = os.path.join(input_data_folder, file)\n",
        "\n",
        "csv_data = pd.read_csv(input_data_file)\n",
        "\n",
        "csv_data[\"OHE\"] = pd.Series([one_hot_encode_rna(val) for val in csv_data[\"Sequence\"]])\n",
        "csv_data[\"OHE_MergedSeq\"] = pd.Series([one_hot_encode_rna_mergedseq(val) for val in csv_data[\"MergedSeq\"]])\n",
        "\n",
        "df_positive = csv_data[csv_data['Number'].str.contains(\"P\")]\n",
        "df_negative = csv_data[csv_data['Number'].str.contains(\"N\")]\n",
        "\n",
        "positive_ohe_mergedseq = np.array(list(df_positive['OHE_MergedSeq']))\n",
        "negative_ohe_mergedseq = np.array(list(df_negative['OHE_MergedSeq']))\n",
        "\n",
        "positive_ohe_seq = np.array(list(df_positive['OHE']))\n",
        "negative_ohe_seq = np.array(list(df_negative['OHE']))\n",
        "\n",
        "print(\"\\n======================================================================\")\n",
        "print(\"\\nFile:\", input_data_file)\n",
        "print(\"Positive:\", positive_ohe_mergedseq.shape[0])\n",
        "print(\"Negative:\", negative_ohe_mergedseq.shape[0])\n",
        "\n",
        "## create the features and labels datasets for the training\n",
        "input_size = positive_ohe_mergedseq[0].shape\n",
        "output_size = positive_ohe_seq[0].shape\n",
        "\n",
        "labels = np.concatenate((np.ones((df_positive.shape[0], 1), \n",
        "                                  dtype=np.float32), \n",
        "                          np.zeros((df_negative.shape[0], 1), \n",
        "                                  dtype=np.float32)), \n",
        "                        axis=0)\n",
        "\n",
        "features_mergedseq = np.concatenate((positive_ohe_mergedseq, \n",
        "                                      negative_ohe_mergedseq), \n",
        "                                    axis=0)\n",
        "\n",
        "features_seq = np.concatenate((positive_ohe_seq,\n",
        "                               negative_ohe_seq),\n",
        "                              axis=0)\n",
        "\n",
        "labels_2d = label_scalar_to_vector(labels)\n",
        "\n",
        "# shuffling data\n",
        "index_arr = np.arange(labels.shape[0])\n",
        "index_arr = np.random.permutation(index_arr)\n",
        "\n",
        "labels = labels[index_arr]\n",
        "labels_2d = labels_2d[index_arr]\n",
        "features_mergedseq = features_mergedseq[index_arr]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0kmFKS55gg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8d3f7c0-ec2a-4c16-f398-5d39cfaf1135"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 271 Complete [00h 00m 09s]\n",
            "val_sequential_loss: 0.7492715120315552\n",
            "\n",
            "Best val_sequential_loss So Far: 0.6496452689170837\n",
            "Total elapsed time: 00h 36m 33s\n",
            "\n",
            "Search: Running Trial #272\n",
            "\n",
            "Hyperparameter    |Value             |Best Value So Far \n",
            "beta              |0.001             |0.01              \n",
            "conv_activation   |tanh              |relu              \n",
            "conv_filters_pe...|10                |5                 \n",
            "max_kernel_leng...|5                 |9                 \n",
            "max_kernel_leng...|3                 |7                 \n",
            "count_conv_layers |30                |10                \n",
            "count_dense_layers|2                 |1                 \n",
            "dense_units_1     |100               |500               \n",
            "dropout_prob      |0.1               |0.1               \n",
            "learning_rate     |0.01              |0.0001            \n",
            "opt_func          |adagrad           |rmsprop           \n",
            "tuner/epochs      |30                |30                \n",
            "tuner/initial_e...|0                 |0                 \n",
            "tuner/bracket     |0                 |0                 \n",
            "tuner/round       |0                 |0                 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "hs_tuner = kt.Hyperband(hypermodel =            model_builder_21,\n",
        "                        objective =             p_objective,\n",
        "                        max_epochs =            p_epoch,\n",
        "                        factor =                p_factor,\n",
        "                        hyperband_iterations =  p_hyperband_iterations,\n",
        "                        directory =             output_data_folder, \n",
        "                        project_name =          'HS_990_hpo_1')\n",
        "\n",
        "hs_stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "hs_tuner.search(features_mergedseq, [features_seq, labels_2d], epochs=p_epoch, validation_split=0.2, callbacks=[hs_stop_early])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v028vVyXv2oX"
      },
      "outputs": [],
      "source": [
        "# hs_tuner = kt.BayesianOptimization(\n",
        "#     hypermodel =          model_builder_21,\n",
        "#     objective =           p_objective,\n",
        "#     max_trials =          p_max_trials,\n",
        "#     num_initial_points =  p_num_initial_points,\n",
        "#     directory =           output_data_folder,\n",
        "#     project_name =        'HS_990_hpo_1'\n",
        "# )\n",
        "\n",
        "# hs_stop_early = tf.keras.callbacks.EarlyStopping(monitor=objective, patience=10)\n",
        "\n",
        "# hs_tuner.search(features_mergedseq, [features_seq, labels_2d], epochs=p_epoch, validation_split=0.2, callbacks=[hs_stop_early])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s__6QUi7FjWH"
      },
      "outputs": [],
      "source": [
        "hs_best_hps = hs_tuner.get_best_hyperparameters()[0]\n",
        "\n",
        "print('beta:', hs_best_hps.get('beta'))\n",
        "print('max_kernel_length_1:', hs_best_hps.get('max_kernel_length_1'))\n",
        "print('max_kernel_length_2:', hs_best_hps.get('max_kernel_length_2'))\n",
        "print('learning_rate:', hs_best_hps.get('learning_rate'))\n",
        "print('conv_activation:', hs_best_hps.get('conv_activation'))\n",
        "print('count_conv_layers:', hs_best_hps.get('count_conv_layers'))\n",
        "print('conv_filters_per_layer:', hs_best_hps.get('conv_filters_per_layer'))\n",
        "print('count_dense_layers:', hs_best_hps.get('count_dense_layers'))\n",
        "print('dense_units_1:', hs_best_hps.get('dense_units_1'))\n",
        "print('dropout_prob:', hs_best_hps.get('dropout_prob'))\n",
        "print('opt_func:', hs_best_hps.get('opt_func'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lohv-mqSfQVB"
      },
      "outputs": [],
      "source": [
        "# model = hs_tuner.hypermodel.build(hs_best_hps)\n",
        "\n",
        "# model_path = os.path.join(output_data_folder, 'HS_990', 'best_model_full_train.h5')\n",
        "\n",
        "# modelCallbacks = [\n",
        "#     tf.keras.callbacks.ModelCheckpoint(model_path,\n",
        "#                                        monitor = 'val_sequential_loss', verbose = 1, save_best_only = True,\n",
        "#                                        save_weights_only = False, mode = 'auto', save_freq = 'epoch'\n",
        "#                                       )\n",
        "# ]\n",
        "\n",
        "# H = model.fit(x=features_mergedseq, y=[features_seq, labels],\n",
        "#               validation_split=0.2, batch_size=p_batch_size,\n",
        "#               epochs=p_final_epochs, callbacks=modelCallbacks, verbose=1)\n",
        "\n",
        "# save_plot(H, os.path.join(drive_out_path, expName, 'HS_990', 'Full_Retrain'))\n",
        "\n",
        "# model.evaluate(features_mergedseq, [features_seq, labels])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kXO8NsLc9C-"
      },
      "outputs": [],
      "source": [
        "# tuner_best_models = hs_tuner.get_best_models(num_models=1)\n",
        "\n",
        "# tuner_best_model = tuner_best_models[0]\n",
        "\n",
        "# model_path = os.path.join(output_data_folder, 'HS_990', 'best_model_transfer_learning.h5')\n",
        "\n",
        "# modelCallbacks = [\n",
        "#     tf.keras.callbacks.ModelCheckpoint(model_path,\n",
        "#                                        monitor = 'val_sequential_loss', verbose = 1, save_best_only = True,\n",
        "#                                        save_weights_only = False, mode = 'auto', save_freq = 'epoch'\n",
        "#                                       )\n",
        "# ]\n",
        "\n",
        "# H = tuner_best_model.fit(x=features_mergedseq, y=[features_seq, labels],\n",
        "#               validation_split=0.2, batch_size=p_batch_size,\n",
        "#               epochs=p_epoch, callbacks=modelCallbacks, verbose=1)\n",
        "\n",
        "# save_plot2(H, os.path.join(drive_out_path, expName, 'HS_990', 'Transfer_Learning'))\n",
        "\n",
        "# tuner_best_model.evaluate(features_mergedseq, [features_seq, labels])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwL7i4jDsVLK"
      },
      "source": [
        "## Execute on folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUN8Rmm1IijJ"
      },
      "outputs": [],
      "source": [
        "##################################################################################\n",
        "##### For each input file, train model and generate different outputs in a structured folder\n",
        "##################################################################################\n",
        "\n",
        "## create the evaluation data structure for all iterations\n",
        "evaluations = {\n",
        "    \"Model\" : [],\n",
        "    # \"Kernel_Length\" : [],\n",
        "    \"Dataset\" : [],\n",
        "    \"Fold\" : [],\n",
        "    \"Train_Test\" : [],\n",
        "    \"Accuracy\" : [],\n",
        "    \"Precision\": [],\n",
        "    \"TPR\": [],\n",
        "    \"FPR\": [],\n",
        "    \"TPR_FPR_Thresholds\": [],\n",
        "    \"AUC\": [],\n",
        "    \"Sensitivity\": [],\n",
        "    \"Specificity\": [],\n",
        "    \"MCC\":[]\n",
        "}\n",
        "        \n",
        "input_data_file = os.path.join(input_data_folder, file)\n",
        "\n",
        "current_dataset_variety = input_data_file.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "csv_data = pd.read_csv(input_data_file)\n",
        "\n",
        "##################################################################################\n",
        "##### extract data from the current CSV file\n",
        "##################################################################################\n",
        "\n",
        "csv_data[\"OHE\"] = pd.Series([one_hot_encode_rna(val) for val in csv_data[\"Sequence\"]])\n",
        "csv_data[\"OHE_MergedSeq\"] = pd.Series([one_hot_encode_rna_mergedseq(val) for val in csv_data[\"MergedSeq\"]])\n",
        "\n",
        "df_positive = csv_data[csv_data['Number'].str.contains(\"P\")]\n",
        "df_negative = csv_data[csv_data['Number'].str.contains(\"N\")]\n",
        "\n",
        "positive_ohe_mergedseq = np.array(list(df_positive['OHE_MergedSeq']))\n",
        "negative_ohe_mergedseq = np.array(list(df_negative['OHE_MergedSeq']))\n",
        "\n",
        "positive_ohe_seq = np.array(list(df_positive['OHE']))\n",
        "negative_ohe_seq = np.array(list(df_negative['OHE']))\n",
        "\n",
        "print(\"\\n======================================================================\")\n",
        "print(\"\\nFile:\", input_data_file)\n",
        "print(\"Positive:\", positive_ohe_mergedseq.shape[0])\n",
        "print(\"Negative:\", negative_ohe_mergedseq.shape[0])\n",
        "\n",
        "##################################################################################\n",
        "##### Generate Folds from dataset, and store to file\n",
        "##################################################################################\n",
        "\n",
        "## create the features and labels datasets for the training\n",
        "input_size = positive_ohe_mergedseq[0].shape\n",
        "output_size = positive_ohe_seq[0].shape\n",
        "\n",
        "labels = np.concatenate((np.ones((df_positive.shape[0], 1), \n",
        "                                  dtype=np.float32), \n",
        "                          np.zeros((df_negative.shape[0], 1), \n",
        "                                  dtype=np.float32)), \n",
        "                        axis=0)\n",
        "features_mergedseq = np.concatenate((positive_ohe_mergedseq, \n",
        "                                      negative_ohe_mergedseq), \n",
        "                                    axis=0)\n",
        "\n",
        "features_seq = np.concatenate((positive_ohe_seq, \n",
        "                                negative_ohe_seq), \n",
        "                              axis=0)\n",
        "\n",
        "folds = build_kfold_multifeature(features_mergedseq, features_seq, labels, \n",
        "                                  k=n_fold, shuffle=shuffle, seed=seed)\n",
        "\n",
        "##### 2D label generating for each fold\n",
        "for j in range(0, len(folds)):\n",
        "    folds[j][\"y_train_2d\"] = label_scalar_to_vector(folds[j][\"y_train\"])\n",
        "    folds[j][\"y_test_2d\"] = label_scalar_to_vector(folds[j][\"y_test\"])\n",
        "\n",
        "## Write the k-fold dataset to file\n",
        "foldPath = os.path.join(outPath, expName, current_dataset_variety, \"{}fold\".format(n_fold))\n",
        "if(not os.path.isdir(foldPath)):\n",
        "    os.makedirs(foldPath)\n",
        "pickle.dump(folds, open(os.path.join(foldPath, foldName), \"wb\"))\n",
        "\n",
        "## Create and set directory to save model\n",
        "modelPath = os.path.join(outPath, expName, current_dataset_variety, \"{}fold\".format(n_fold), \"models\")\n",
        "if(not os.path.isdir(modelPath)):\n",
        "    os.makedirs(modelPath)\n",
        "\n",
        "##################################################################################\n",
        "##### TRAIN and PREDICT for every Fold, using random generated models using best hyperparameters\n",
        "##################################################################################\n",
        "\n",
        "# fold counter\n",
        "i = 0\n",
        "\n",
        "for fold in folds:\n",
        "\n",
        "    print(\"\\nTrain/Test model \"+current_dataset_variety+\" on Fold #\"+str(i)+\".\")\n",
        "\n",
        "    # model, encoder, decoder, classifier, loss_names = DLNN_AutoEncoder_Classifier(input_shape=input_size,\n",
        "                                                                                  # learn_rate = 0.01)\n",
        "\n",
        "    # adding random shuffling of the dataset for training purpose\n",
        "    index_arr = np.arange(fold[\"X1_train\"].shape[0])\n",
        "    index_arr = np.random.permutation(index_arr)\n",
        "\n",
        "    model = hs_tuner.hypermodel.build(hs_best_hps)\n",
        "\n",
        "    loss_name = [s for s in list(model.loss.keys()) if 'sequential' in s][0]\n",
        "\n",
        "    current_model_path = os.path.join(modelPath, \"{}_bestModel-fold{}.hdf5\".format(current_dataset_variety, i))\n",
        "    modelCallbacks = [\n",
        "        tf.keras.callbacks.ModelCheckpoint(current_model_path,\n",
        "                                           monitor = \"val_\"+loss_name+\"_loss\", verbose = 1, save_best_only = True,\n",
        "                                           save_weights_only = False, mode = 'auto', save_freq = 'epoch'\n",
        "                                          )\n",
        "    ]\n",
        "    \n",
        "    H = model.fit(\n",
        "        x = fold[\"X1_train\"][index_arr], y = [fold[\"X2_train\"][index_arr], fold[\"y_train_2d\"][index_arr]],\n",
        "        batch_size = batch_size, epochs = epochs,\n",
        "        validation_split=0.2,\n",
        "        # validation_data = (fold[\"X1_test\"], [fold[\"X2_test\"], fold[\"y_test_2d\"]]),\n",
        "        verbose = 1,\n",
        "        callbacks = modelCallbacks\n",
        "    )\n",
        "    \n",
        "    plot_path = os.path.join(outPath, expName, current_dataset_variety, \"{}fold\".format(n_fold), \"models\", \"{}_bestModel-fold{}_charts\".format(current_dataset_variety, i))\n",
        "    save_plot2(H, plot_path, loss_name)\n",
        "\n",
        "    model = tf.keras.models.load_model(current_model_path)\n",
        "\n",
        "    ##################################################################################\n",
        "    ##### Prediction and metrics for TRAIN dataset\n",
        "    ##################################################################################\n",
        "\n",
        "    y_pred_seq, y_pred_2d = model.predict(fold[\"X1_train\"])\n",
        "    label_pred_2d = pred2label(y_pred_2d)\n",
        "    \n",
        "    y_pred = y_pred_2d[:,1].reshape((y_pred_2d.shape[0], 1))\n",
        "\n",
        "    label_pred = label_vector_to_scalar(label_pred_2d)\n",
        "    label_actual = fold[\"y_train\"]\n",
        "\n",
        "    # Compute precision, recall, sensitivity, specifity, mcc\n",
        "    acc = accuracy_score(label_actual, label_pred)\n",
        "    prec = precision_score(label_actual,label_pred)\n",
        "\n",
        "    conf = confusion_matrix(label_actual, label_pred)\n",
        "    if(conf[0][0]+conf[1][0]):\n",
        "        sens = float(conf[0][0])/float(conf[0][0]+conf[1][0])\n",
        "    else:\n",
        "        sens = 0.0\n",
        "    if(conf[1][1]+conf[0][1]):\n",
        "        spec = float(conf[1][1])/float(conf[1][1]+conf[0][1])\n",
        "    else:\n",
        "        spec = 0.0\n",
        "    if((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0])):\n",
        "        mcc = (float(conf[0][0])*float(conf[1][1]) - float(conf[1][0])*float(conf[0][1]))/math.sqrt((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0]))\n",
        "    else:\n",
        "        mcc= 0.0\n",
        "    fpr, tpr, thresholds = roc_curve(label_actual, y_pred)\n",
        "    auc = roc_auc_score(label_actual, y_pred)\n",
        "\n",
        "    evaluations[\"Model\"].append(current_dataset_variety)\n",
        "    # evaluations[\"Kernel_Length\"].append(kernel_length)\n",
        "    evaluations[\"Dataset\"].append(current_dataset_variety)\n",
        "    evaluations[\"Fold\"].append(i)\n",
        "    evaluations[\"Train_Test\"].append(\"Train\")\n",
        "    evaluations[\"Accuracy\"].append(acc)\n",
        "    evaluations[\"Precision\"].append(prec)\n",
        "    evaluations[\"TPR\"].append(tpr)\n",
        "    evaluations[\"FPR\"].append(fpr)\n",
        "    evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
        "    evaluations[\"AUC\"].append(auc)\n",
        "    evaluations[\"Sensitivity\"].append(sens)\n",
        "    evaluations[\"Specificity\"].append(spec)\n",
        "    evaluations[\"MCC\"].append(mcc)\n",
        "\n",
        "    ##################################################################################\n",
        "    ##### Prediction and metrics for TEST dataset\n",
        "    ##################################################################################\n",
        "\n",
        "    y_pred_seq, y_pred_2d = model.predict(fold[\"X1_test\"])\n",
        "    label_pred_2d = pred2label(y_pred_2d)\n",
        "\n",
        "    y_pred = y_pred_2d[:,1].reshape((y_pred_2d.shape[0], 1))\n",
        "\n",
        "    label_pred = label_vector_to_scalar(label_pred_2d)\n",
        "    label_actual = fold[\"y_test\"]\n",
        "\n",
        "    # Compute precision, recall, sensitivity, specifity, mcc\n",
        "    acc = accuracy_score(label_actual, label_pred)\n",
        "    prec = precision_score(label_actual,label_pred)\n",
        "\n",
        "    conf = confusion_matrix(label_actual, label_pred)\n",
        "    if(conf[0][0]+conf[1][0]):\n",
        "        sens = float(conf[0][0])/float(conf[0][0]+conf[1][0])\n",
        "    else:\n",
        "        sens = 0.0\n",
        "    if(conf[1][1]+conf[0][1]):\n",
        "        spec = float(conf[1][1])/float(conf[1][1]+conf[0][1])\n",
        "    else:\n",
        "        spec = 0.0\n",
        "    if((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0])):\n",
        "        mcc = (float(conf[0][0])*float(conf[1][1]) - float(conf[1][0])*float(conf[0][1]))/math.sqrt((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0]))\n",
        "    else:\n",
        "        mcc= 0.0\n",
        "    fpr, tpr, thresholds = roc_curve(label_actual, y_pred)\n",
        "    auc = roc_auc_score(label_actual, y_pred)\n",
        "\n",
        "    evaluations[\"Model\"].append(current_dataset_variety)\n",
        "    # evaluations[\"Kernel_Length\"].append(kernel_length)\n",
        "    evaluations[\"Dataset\"].append(current_dataset_variety)\n",
        "    evaluations[\"Fold\"].append(i)\n",
        "    evaluations[\"Train_Test\"].append(\"Test\")\n",
        "    evaluations[\"Accuracy\"].append(acc)\n",
        "    evaluations[\"Precision\"].append(prec)\n",
        "    evaluations[\"TPR\"].append(tpr)\n",
        "    evaluations[\"FPR\"].append(fpr)\n",
        "    evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
        "    evaluations[\"AUC\"].append(auc)\n",
        "    evaluations[\"Sensitivity\"].append(sens)\n",
        "    evaluations[\"Specificity\"].append(spec)\n",
        "    evaluations[\"MCC\"].append(mcc)\n",
        "\n",
        "    i = i+1\n",
        "    del model\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "##################################################################################\n",
        "##### Dump evaluations to a file\n",
        "##################################################################################\n",
        "\n",
        "evalPath = os.path.join(outPath, expName, \"_Evaluation_All_Datasets\")\n",
        "if(not os.path.isdir(evalPath)):\n",
        "    os.makedirs(evalPath)\n",
        "\n",
        "pickle.dump(evaluations,\n",
        "            open(os.path.join(evalPath, \"{}fold_evaluations.pickle\".format(n_fold)), \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dq26y9-PWn2g"
      },
      "outputs": [],
      "source": [
        "evaluations_df = pd.DataFrame.from_dict(evaluations)\n",
        "\n",
        "##################################################################################\n",
        "##### Group dataset (mean of metrics) by [Dataset, Model, Train_Test] combinations\n",
        "##################################################################################\n",
        "\n",
        "evaluations_df_grouped = evaluations_df.groupby([\"Dataset\", \n",
        "                                                 \"Model\", \n",
        "                                                 \"Train_Test\"]).mean().filter(['Accuracy', \n",
        "                                                                               'Precision', \n",
        "                                                                               'AUC', \n",
        "                                                                               'Sensitivity', \n",
        "                                                                               'Specificity', \n",
        "                                                                               'MCC'])\n",
        "\n",
        "# DLNN_3 = evaluations_df_grouped[np.in1d(evaluations_df_grouped.index.get_level_values(1), ['DLNN_3'])]\n",
        "# DLNN_5 = evaluations_df_grouped[np.in1d(evaluations_df_grouped.index.get_level_values(1), ['DLNN_5'])]\n",
        "\n",
        "# DLNN_3_Train = DLNN_3[np.in1d(DLNN_3.index.get_level_values(2), ['Train'])]\n",
        "# DLNN_3_Test = DLNN_3[np.in1d(DLNN_3.index.get_level_values(2), ['Test'])]\n",
        "\n",
        "# DLNN_5_Train = DLNN_5[np.in1d(DLNN_5.index.get_level_values(2), ['Train'])]\n",
        "# DLNN_5_Test = DLNN_5[np.in1d(DLNN_5.index.get_level_values(2), ['Test'])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqkGC1YbsZ6X"
      },
      "source": [
        "## Fold Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhbWivK0h5um"
      },
      "outputs": [],
      "source": [
        "evaluations_df[evaluations_df['Train_Test'] == 'Test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIs-tfVcWuxs"
      },
      "outputs": [],
      "source": [
        "evaluations_df_grouped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DACVMfSrqS3q"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWswo_UpqS1g"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9Pv3CRmqSzP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCqF0rjUqSw3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul8mq0mCHU2m"
      },
      "source": [
        "# MM_944"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R77LcNDBHU2n"
      },
      "outputs": [],
      "source": [
        "# ###########################################################################\n",
        "# ##### Prepare dataset\n",
        "# ###########################################################################\n",
        "\n",
        "# file = 'MM_944.csv'\n",
        "# input_data_file = os.path.join(input_data_folder, file)\n",
        "\n",
        "# csv_data = pd.read_csv(input_data_file)\n",
        "\n",
        "# csv_data[\"OHE\"] = pd.Series([one_hot_encode_rna(val) for val in csv_data[\"Sequence\"]])\n",
        "# csv_data[\"OHE_MergedSeq\"] = pd.Series([one_hot_encode_rna_mergedseq(val) for val in csv_data[\"MergedSeq\"]])\n",
        "\n",
        "# df_positive = csv_data[csv_data['Number'].str.contains(\"P\")]\n",
        "# df_negative = csv_data[csv_data['Number'].str.contains(\"N\")]\n",
        "\n",
        "# positive_ohe_mergedseq = np.array(list(df_positive['OHE_MergedSeq']))\n",
        "# negative_ohe_mergedseq = np.array(list(df_negative['OHE_MergedSeq']))\n",
        "\n",
        "# positive_ohe_seq = np.array(list(df_positive['OHE']))\n",
        "# negative_ohe_seq = np.array(list(df_negative['OHE']))\n",
        "\n",
        "# print(\"\\n======================================================================\")\n",
        "# print(\"\\nFile:\", input_data_file)\n",
        "# print(\"Positive:\", positive_ohe_mergedseq.shape[0])\n",
        "# print(\"Negative:\", negative_ohe_mergedseq.shape[0])\n",
        "\n",
        "# ## create the features and labels datasets for the training\n",
        "# input_size = positive_ohe_mergedseq[0].shape\n",
        "# output_size = positive_ohe_seq[0].shape\n",
        "\n",
        "# labels = np.concatenate((np.ones((df_positive.shape[0], 1), \n",
        "#                                   dtype=np.float32), \n",
        "#                           np.zeros((df_negative.shape[0], 1), \n",
        "#                                   dtype=np.float32)), \n",
        "#                         axis=0)\n",
        "\n",
        "# features_mergedseq = np.concatenate((positive_ohe_mergedseq, \n",
        "#                                       negative_ohe_mergedseq), \n",
        "#                                     axis=0)\n",
        "\n",
        "# # shuffling data\n",
        "# index_arr = np.arange(labels.shape[0])\n",
        "# index_arr = np.random.permutation(index_arr)\n",
        "\n",
        "# labels = labels[index_arr]\n",
        "# features_mergedseq = features_mergedseq[index_arr]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKOZUTNnHU2n"
      },
      "outputs": [],
      "source": [
        "# mm_tuner = kt.Hyperband(hypermodel =            model_builder_21,\n",
        "#                         objective =             p_objective,\n",
        "#                         max_epochs =            p_epoch,\n",
        "#                         factor =                p_factor,\n",
        "#                         hyperband_iterations =  p_hyperband_iterations,\n",
        "#                         directory =             output_data_folder,\n",
        "#                         project_name =          'mm_944_hpo_1')\n",
        "\n",
        "# mm_stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "# mm_tuner.search(features_mergedseq, labels, epochs=p_epoch, validation_split=0.2, callbacks=[mm_stop_early])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKGP41B2w7QV"
      },
      "outputs": [],
      "source": [
        "# mm_tuner = kt.BayesianOptimization(\n",
        "#     hypermodel =          model_builder_21,\n",
        "#     objective =           p_objective,\n",
        "#     max_trials =          p_max_trials,\n",
        "#     num_initial_points =  p_num_initial_points,\n",
        "#     directory =           output_data_folder,\n",
        "#     project_name =        'MM_944_hpo_1'\n",
        "# )\n",
        "\n",
        "# mm_stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "# mm_tuner.search(features_mergedseq, labels, epochs=p_epoch, validation_split=0.2, callbacks=[mm_stop_early])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFJI5Zv9HU2o"
      },
      "outputs": [],
      "source": [
        "# mm_best_hps = mm_tuner.get_best_hyperparameters()[0]\n",
        "\n",
        "# print('beta:', mm_best_hps.get('beta'))\n",
        "# print('max_kernel_length:', mm_best_hps.get('max_kernel_length'))\n",
        "# print('kernel_length_step:', mm_best_hps.get('kernel_length_step'))\n",
        "# print('learning_rate:', mm_best_hps.get('learning_rate'))\n",
        "# print('conv_activation:', mm_best_hps.get('conv_activation'))\n",
        "# print('count_conv_layers:', mm_best_hps.get('count_conv_layers'))\n",
        "# print('conv_filters_per_layer:', mm_best_hps.get('conv_filters_per_layer'))\n",
        "# print('count_dense_layers:', mm_best_hps.get('count_dense_layers'))\n",
        "# print('dense_units:', mm_best_hps.get('dense_units_1'))\n",
        "# # print('dense_activation:', mm_best_hps.get('dense_activation'))\n",
        "# print('dropout_prob:', mm_best_hps.get('dropout_prob'))\n",
        "# print('opt_func:', mm_best_hps.get('opt_func'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SR_mAbfdjKTL"
      },
      "outputs": [],
      "source": [
        "# model = mm_tuner.hypermodel.build(mm_best_hps)\n",
        "\n",
        "# model_path = os.path.join(output_data_folder, 'MM_944', 'best_model.h5')\n",
        "\n",
        "# modelCallbacks = [\n",
        "#     tf.keras.callbacks.ModelCheckpoint(model_path,\n",
        "#                                        monitor = 'val_loss', verbose = 1, save_best_only = True,\n",
        "#                                        save_weights_only = False, mode = 'auto', save_freq = 'epoch'\n",
        "#                                       )\n",
        "# ]\n",
        "\n",
        "# H = model.fit(x=features_mergedseq, y=labels,\n",
        "#               validation_split=0.2, batch_size=p_batch_size,\n",
        "#               epochs=p_final_epochs, callbacks=modelCallbacks, verbose=1)\n",
        "\n",
        "# save_plot(H, os.path.join(output_data_folder, 'MM_944'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbVkPWoLHtbo"
      },
      "source": [
        "# SN_628"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxF7T97AHtbo"
      },
      "outputs": [],
      "source": [
        "# ###########################################################################\n",
        "# ##### Prepare dataset\n",
        "# ###########################################################################\n",
        "\n",
        "# file = 'SN_628.csv'\n",
        "# input_data_file = os.path.join(input_data_folder, file)\n",
        "\n",
        "# csv_data = pd.read_csv(input_data_file)\n",
        "\n",
        "# csv_data[\"OHE\"] = pd.Series([one_hot_encode_rna(val) for val in csv_data[\"Sequence\"]])\n",
        "# csv_data[\"OHE_MergedSeq\"] = pd.Series([one_hot_encode_rna_mergedseq(val) for val in csv_data[\"MergedSeq\"]])\n",
        "\n",
        "# df_positive = csv_data[csv_data['Number'].str.contains(\"P\")]\n",
        "# df_negative = csv_data[csv_data['Number'].str.contains(\"N\")]\n",
        "\n",
        "# positive_ohe_mergedseq = np.array(list(df_positive['OHE_MergedSeq']))\n",
        "# negative_ohe_mergedseq = np.array(list(df_negative['OHE_MergedSeq']))\n",
        "\n",
        "# positive_ohe_seq = np.array(list(df_positive['OHE']))\n",
        "# negative_ohe_seq = np.array(list(df_negative['OHE']))\n",
        "\n",
        "# print(\"\\n======================================================================\")\n",
        "# print(\"\\nFile:\", input_data_file)\n",
        "# print(\"Positive:\", positive_ohe_mergedseq.shape[0])\n",
        "# print(\"Negative:\", negative_ohe_mergedseq.shape[0])\n",
        "\n",
        "# ## create the features and labels datasets for the training\n",
        "# input_size = positive_ohe_mergedseq[0].shape\n",
        "# output_size = positive_ohe_seq[0].shape\n",
        "\n",
        "# labels = np.concatenate((np.ones((df_positive.shape[0], 1), \n",
        "#                                   dtype=np.float32), \n",
        "#                           np.zeros((df_negative.shape[0], 1), \n",
        "#                                   dtype=np.float32)), \n",
        "#                         axis=0)\n",
        "\n",
        "# features_mergedseq = np.concatenate((positive_ohe_mergedseq, \n",
        "#                                       negative_ohe_mergedseq), \n",
        "#                                     axis=0)\n",
        "\n",
        "# # shuffling data\n",
        "# index_arr = np.arange(labels.shape[0])\n",
        "# index_arr = np.random.permutation(index_arr)\n",
        "\n",
        "# labels = labels[index_arr]\n",
        "# features_mergedseq = features_mergedseq[index_arr]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r168YEhqHtbp"
      },
      "outputs": [],
      "source": [
        "# sn_tuner = kt.Hyperband(hypermodel =            model_builder_31,\n",
        "#                         objective =             p_objective,\n",
        "#                         max_epochs =            p_epoch,\n",
        "#                         factor =                p_factor,\n",
        "#                         hyperband_iterations =  p_hyperband_iterations,\n",
        "#                         directory =             output_data_folder,\n",
        "#                         project_name =          'SN_628_hpo_1')\n",
        "\n",
        "# sn_stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "# sn_tuner.search(features_mergedseq, labels, epochs=p_epoch, validation_split=0.2, callbacks=[sn_stop_early])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dF1FAHLdxgGe"
      },
      "outputs": [],
      "source": [
        "# sn_tuner = kt.BayesianOptimization(\n",
        "#     hypermodel =          model_builder_31,\n",
        "#     objective =           p_objective,\n",
        "#     max_trials =          p_max_trials,\n",
        "#     num_initial_points =  p_num_initial_points,\n",
        "#     directory =           output_data_folder,\n",
        "#     project_name =        'SN_944_hpo_1'\n",
        "# )\n",
        "\n",
        "# sn_stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "# sn_tuner.search(features_mergedseq, labels, epochs=p_epoch, validation_split=0.2, callbacks=[sn_stop_early])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wenAbVfkHtbp"
      },
      "outputs": [],
      "source": [
        "# sn_best_hps = sn_tuner.get_best_hyperparameters()[0]\n",
        "\n",
        "# print('beta:', sn_best_hps.get('beta'))\n",
        "# print('max_kernel_length:', sn_best_hps.get('max_kernel_length'))\n",
        "# print('kernel_length_step:', sn_best_hps.get('kernel_length_step'))\n",
        "# print('learning_rate:', sn_best_hps.get('learning_rate'))\n",
        "# print('conv_activation:', sn_best_hps.get('conv_activation'))\n",
        "# print('count_conv_layers:', sn_best_hps.get('count_conv_layers'))\n",
        "# print('conv_filters_per_layer:', sn_best_hps.get('conv_filters_per_layer'))\n",
        "# print('count_dense_layers:', sn_best_hps.get('count_dense_layers'))\n",
        "# print('dense_units:', sn_best_hps.get('dense_units_1'))\n",
        "# # print('dense_activation:', sn_best_hps.get('dense_activation'))\n",
        "# print('dropout_prob:', sn_best_hps.get('dropout_prob'))\n",
        "# print('opt_func:', sn_best_hps.get('opt_func'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjXfaXvBjSqq"
      },
      "outputs": [],
      "source": [
        "# model = sn_tuner.hypermodel.build(sn_best_hps)\n",
        "\n",
        "# model_path = os.path.join(output_data_folder, 'SN_628', 'best_model.h5')\n",
        "\n",
        "# modelCallbacks = [\n",
        "#     tf.keras.callbacks.ModelCheckpoint(model_path,\n",
        "#                                        monitor = 'val_loss', verbose = 1, save_best_only = True,\n",
        "#                                        save_weights_only = False, mode = 'auto', save_freq = 'epoch'\n",
        "#                                       )\n",
        "# ]\n",
        "\n",
        "# H = model.fit(x=features_mergedseq, y=labels,\n",
        "#               validation_split=0.2, batch_size=p_batch_size,\n",
        "#               epochs=p_final_epochs, callbacks=modelCallbacks, verbose=1)\n",
        "\n",
        "# save_plot(H, os.path.join(output_data_folder, 'SN_628'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "XyofMbfnIWJL",
        "hkOCobMPISQz",
        "2Il334C4INB8",
        "bamwvpxsIKbc",
        "FwL7i4jDsVLK",
        "GqkGC1YbsZ6X",
        "ul8mq0mCHU2m",
        "UbVkPWoLHtbo"
      ],
      "name": "Psi_Site_Classification_DLNN_MergedSeq_2DOP_autoencoder_kerasTuner.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}