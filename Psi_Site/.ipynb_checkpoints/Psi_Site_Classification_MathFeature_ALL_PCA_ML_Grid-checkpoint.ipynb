{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# from Bio import SeqIO\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import tensorflow.keras.backend as K\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# import xgboost as xgb\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Define all experiment parameters\n",
    "##################################################################################\n",
    "\n",
    "expName = \"Psi_Site_Chen_MathFeature_Latest_5_0_10_5_1_ALL_GRID_PCA_GradientBoostingClassifier\"\n",
    "\n",
    "dataset_path = \"Data\"\n",
    "setting = \"Psi_Site_Chen_MathFeature_Latest_5_0_10_5_1_ALL\"\n",
    "\n",
    "output_path = \"Results\"\n",
    "\n",
    "datafile_extensions = \".csv\"\n",
    "\n",
    "modelNames = [\"GradientBoosting\"]\n",
    "\n",
    "shuffle = False\n",
    "seed = None\n",
    "\n",
    "##################################################################################\n",
    "##### Define the modelling hyperparameters\n",
    "##################################################################################\n",
    "\n",
    "n_fold = 10\n",
    "\n",
    "validation_fraction = 0.1\n",
    "\n",
    "##################################################################################\n",
    "##### Define the GRID hyperparameters\n",
    "##################################################################################\n",
    "\n",
    "pca_n_components = [10, 20, 30, 50, 100]\n",
    "\n",
    "loss = ['deviance', 'exponential']\n",
    "learning_rate = [0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001]\n",
    "n_estimators = [10, 30, 50, 100, 200, 300, 500]\n",
    "max_depth = [5, 10, 15, 20, 25, 30]\n",
    "ccp_alpha = [0.0, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001]\n",
    "n_iter_no_change = [10, 25, 50, 100]\n",
    "tol = [0.1, 0.01]\n",
    "criterion = ['friedman_mse', 'squared_error', 'mse', 'mae']\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "\n",
    "param_grid = {\n",
    "    \"reduction__n_components\": pca_n_components,\n",
    "    \"model__loss\": loss,\n",
    "    \"model__learning_rate\": learning_rate,\n",
    "    \"model__n_estimators\": n_estimators,\n",
    "    \"model__max_depth\": max_depth,\n",
    "    \"model__ccp_alpha\": ccp_alpha,\n",
    "    \"model__n_iter_no_change\": n_iter_no_change,\n",
    "    \"model__tol\": tol,\n",
    "    \"model__criterion\": criterion,\n",
    "    \"model__max_features\": max_features\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Checking the directory\n",
    "##################################################################################\n",
    "\n",
    "dataset_setting_path = os.path.join(dataset_path, setting)\n",
    "dataset_varieties = next(os.walk(dataset_setting_path))\n",
    "result_output_path = os.path.join(output_path, expName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### define evaluator functions\n",
    "##################################################################################\n",
    "\n",
    "## Build the K-fold from dataset\n",
    "def build_kfold(features, labels, k=10, shuffle=False, seed=None):\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=shuffle, random_state=seed)\n",
    "    kfoldList = []\n",
    "    for train_index, test_index in skf.split(features, labels):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        kfoldList.append({\n",
    "            \"X_train\": X_train,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_train\":y_train,\n",
    "            \"y_test\":y_test\n",
    "        })\n",
    "    return kfoldList\n",
    "\n",
    "def pred2label(y_pred):\n",
    "    y_pred = np.round(np.clip(y_pred, 0, 1))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "File: Data\\Psi_Site_Chen_MathFeature_Latest_5_0_10_5_1_ALL\\HS_990_ALL.csv\n",
      "Positive: 495\n",
      "Negative: 495\n",
      "\n",
      "======================================================================\n",
      "\n",
      "File: Data\\Psi_Site_Chen_MathFeature_Latest_5_0_10_5_1_ALL\\MM_944_ALL.csv\n",
      "Positive: 472\n",
      "Negative: 472\n",
      "\n",
      "======================================================================\n",
      "\n",
      "File: Data\\Psi_Site_Chen_MathFeature_Latest_5_0_10_5_1_ALL\\SS_628_ALL.csv\n",
      "Positive: 314\n",
      "Negative: 314\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "##### For each input file, train model and generate different outputs in a structured folder\n",
    "##################################################################################\n",
    "\n",
    "error_list = []\n",
    "\n",
    "evaluations = {}\n",
    "\n",
    "for root, dirs, files in os.walk(dataset_setting_path):\n",
    "    for file in files:\n",
    "        if os.path.splitext(file)[-1] == datafile_extensions:\n",
    "            \n",
    "            try:\n",
    "            \n",
    "                current_dataset_variety = \"_\".join(file.split(\".\")[0].split(\"_\")[0:(len(file.split(\".\")[0].split(\"_\")) - 1)])\n",
    "                encoding_type = file.split(\".\")[0].split(\"_\")[-1]\n",
    "\n",
    "                ##################################################################################\n",
    "                ##### read the current file\n",
    "                ##################################################################################\n",
    "\n",
    "                input_file_full_path = os.path.join(root, file)\n",
    "\n",
    "                ## check if input file has header\n",
    "                file_obj = open(input_file_full_path, \"r\")\n",
    "                first_line = file_obj.readline()\n",
    "                file_obj.close()\n",
    "                file_has_header = None\n",
    "                if first_line.split(\",\")[0] == \"nameseq\" or first_line.replace(\"\\n\", \"\").split(\",\")[-1] == \"label\":\n",
    "                    file_has_header = 0\n",
    "\n",
    "                sequences_df = pd.read_csv(input_file_full_path, header = file_has_header)\n",
    "\n",
    "                ##################################################################################\n",
    "                ##### extract data from the current dataframe file\n",
    "                ##################################################################################\n",
    "\n",
    "                sequences_df[\"class\"] = np.where(sequences_df[sequences_df.columns[0]].str.contains(\"P\"), 1, 0)\n",
    "\n",
    "                print(\"\\n======================================================================\")\n",
    "                print(\"\\nFile: \"+os.path.join(root, file))\n",
    "                print(\"Positive: \"+str(sum(sequences_df[\"class\"])))\n",
    "                print(\"Negative: \"+str(len(sequences_df) - sum(sequences_df[\"class\"])))\n",
    "\n",
    "                ##################################################################################\n",
    "                ##### Perform PCA on entire data\n",
    "                ##################################################################################\n",
    "\n",
    "                ## create the features and labels datasets for the training\n",
    "                labels = sequences_df[\"class\"].values\n",
    "                features = sequences_df.drop('nameseq', axis = 1).drop('class', axis = 1).values\n",
    "                # features = features.astype(np.float)\n",
    "                \n",
    "                ##################################################################################\n",
    "                ##### Grid Search\n",
    "                ##################################################################################\n",
    "                \n",
    "                pipe = Pipeline(steps=[(\"reduction\", PCA()), (\"model\", GradientBoostingClassifier())])\n",
    "                \n",
    "                search = GridSearchCV(pipe, \n",
    "                                      param_grid, \n",
    "                                      cv=n_fold, \n",
    "                                      scoring=[\"accuracy\", \"roc_auc\", \"precision\", \"recall\", \"f1\"], \n",
    "                                      refit=\"accuracy\")\n",
    "                search.fit(features, labels)\n",
    "                \n",
    "                evaluations[current_dataset_variety] = search\n",
    "                        \n",
    "            except Exception as error:\n",
    "                error_list.append((input_file_full_path, error))\n",
    "                \n",
    "##################################################################################\n",
    "##### Dump evaluations to a file\n",
    "##################################################################################\n",
    "\n",
    "evalPath = os.path.join(result_output_path, \"_Evaluation_All_Datasets\", \"{}fold\".format(n_fold))\n",
    "if(not os.path.isdir(evalPath)):\n",
    "    os.makedirs(evalPath)\n",
    "\n",
    "pickle.dump(evaluations,\n",
    "            open(os.path.join(evalPath, \"{}fold_evaluations_{}.pickle\".format(n_fold, modelNames[0])), \"wb\"))\n",
    "\n",
    "##################################################################################\n",
    "##### Dump exceptions to a file\n",
    "##################################################################################\n",
    "\n",
    "pickle.dump(error_list,\n",
    "            open(os.path.join(result_output_path, \"exceptions.pickle\"), \"wb\"))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Data\\\\Psi_Site_Chen_MathFeature_Latest_5_0_10_5_1_ALL\\\\HS_990_ALL.csv',\n",
       "  ValueError('For multi-metric scoring, the parameter refit must be set to a scorer key or a callable to refit an estimator with the best parameter setting on the whole data and make the best_* attributes available for that metric. If this is not needed, refit should be set to False explicitly. True was passed.')),\n",
       " ('Data\\\\Psi_Site_Chen_MathFeature_Latest_5_0_10_5_1_ALL\\\\MM_944_ALL.csv',\n",
       "  ValueError('For multi-metric scoring, the parameter refit must be set to a scorer key or a callable to refit an estimator with the best parameter setting on the whole data and make the best_* attributes available for that metric. If this is not needed, refit should be set to False explicitly. True was passed.')),\n",
       " ('Data\\\\Psi_Site_Chen_MathFeature_Latest_5_0_10_5_1_ALL\\\\SS_628_ALL.csv',\n",
       "  ValueError('For multi-metric scoring, the parameter refit must be set to a scorer key or a callable to refit an estimator with the best parameter setting on the whole data and make the best_* attributes available for that metric. If this is not needed, refit should be set to False explicitly. True was passed.'))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Add import statement here, to make this next part of code standalone executable\n",
    "##################################################################################\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "# import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import ScalarFormatter, FormatStrFormatter\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################################\n",
    "# ##### Parameters used only in this section\n",
    "# ##################################################################################\n",
    "\n",
    "# n_fold = 10\n",
    "\n",
    "# expName = \"MathFeature_setting1_kgap_fickett\"\n",
    "# outPath = \"Generated\"\n",
    "# setting = \"Setting1\"\n",
    "# output_path = \"Results\"\n",
    "\n",
    "# ExtraTreeForest, RandomForest, XGBoost\n",
    "# modelNames = [\"XGBoost\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Load file and convert to dataframe for easy manipulation\n",
    "##################################################################################\n",
    "\n",
    "evalPath = os.path.join(result_output_path, \"_Evaluation_All_Datasets\", \"{}fold\".format(n_fold))\n",
    "\n",
    "evaluations = pickle.load(open(os.path.join(evalPath, \"{}fold_evaluations_{}.pickle\".format(n_fold, modelNames[0])), \"rb\"))\n",
    "\n",
    "evaluations_df = pd.DataFrame.from_dict(evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Group dataset (mean of metrics) by [Dataset, Model, Train_Test] combinations\n",
    "##################################################################################\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Dataset\",\n",
    "                                                 \"Encoding_Type\",\n",
    "                                                 \"Model\", \n",
    "                                                 \"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC'])\n",
    "\n",
    "Eval_Train = evaluations_df_grouped[np.in1d(evaluations_df_grouped.index.get_level_values(3), ['Train'])]\n",
    "Eval_Test = evaluations_df_grouped[np.in1d(evaluations_df_grouped.index.get_level_values(3), ['Test'])]\n",
    "\n",
    "datasets = np.unique(evaluations_df_grouped.index.get_level_values(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Decide on metric to visualize\n",
    "##################################################################################\n",
    "\n",
    "print(\"Metrics Available : \", list(evaluations_df_grouped.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select a metric to plot below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_to_plot = \"Accuracy\"\n",
    "# dataset_to_print = \"HS_990\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Visualize with a multiple Bar chart\n",
    "##################################################################################\n",
    "\n",
    "# df = evaluations_df_grouped[np.in1d(evaluations_df_grouped.index.get_level_values(0), [dataset_to_print])]\n",
    "df = evaluations_df_grouped.reset_index(level=['Dataset', 'Train_Test'])\n",
    "\n",
    "# Some boilerplate to initialise things\n",
    "sns.set()\n",
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "# This is where the actual plot gets made\n",
    "ax = sns.barplot(data=df, x=\"Dataset\", y=metric_to_plot, hue=\"Train_Test\")\n",
    "\n",
    "# Customise some display properties\n",
    "ax.set_title(modelNames[0])\n",
    "ax.grid(color='#cccccc')\n",
    "ax.set_ylabel(metric_to_plot)\n",
    "ax.set_xlabel(\"Dataset\")\n",
    "ax.set_xticklabels(df[\"Dataset\"].unique().astype(str), rotation='vertical')\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height()*100, '.4f'),\n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha = 'center', va = 'center', \n",
    "                size=15,\n",
    "                xytext = (0, -12), \n",
    "                textcoords = 'offset points')\n",
    "\n",
    "##############################\n",
    "\n",
    "# Ask Matplotlib to show it\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################################\n",
    "# ##### Visualize with a multiple Bar chart\n",
    "# ##################################################################################\n",
    "\n",
    "# df = evaluations_df_grouped[np.in1d(evaluations_df_grouped.index.get_level_values(0), [dataset_to_print])]\n",
    "# df = df.reset_index(level=['Encoding_Type', 'Train_Test'])\n",
    "\n",
    "# # Some boilerplate to initialise things\n",
    "# sns.set()\n",
    "# plt.figure(figsize=(20,8))\n",
    "\n",
    "# # This is where the actual plot gets made\n",
    "# ax = sns.barplot(data=df, x=\"Encoding_Type\", y=metric_to_plot, hue=\"Train_Test\")\n",
    "\n",
    "# # Customise some display properties\n",
    "# ax.set_title(dataset_to_print+\" - \"+modelNames[0])\n",
    "# ax.grid(color='#cccccc')\n",
    "# ax.set_ylabel(metric_to_plot)\n",
    "# ax.set_xlabel(\"Encoding_Type\")\n",
    "# ax.set_xticklabels(df[\"Encoding_Type\"].unique().astype(str), rotation='vertical')\n",
    "\n",
    "# # for p in ax.patches:\n",
    "# #     ax.annotate(format(p.get_height()*100, '.4f'),\n",
    "# #                 (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "# #                 ha = 'center', va = 'center', \n",
    "# #                 size=15,\n",
    "# #                 xytext = (0, -12), \n",
    "# #                 textcoords = 'offset points')\n",
    "\n",
    "# ##############################\n",
    "\n",
    "# # Ask Matplotlib to show it\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store all metrics' plots to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################################\n",
    "# ##### Iteratively generate comparison plot using every metric\n",
    "# ##################################################################################\n",
    "\n",
    "# for metric_to_plot in list(evaluations_df_grouped.columns):\n",
    "#     for dataset_to_print in datasets:\n",
    "    \n",
    "#         df = evaluations_df_grouped[np.in1d(evaluations_df_grouped.index.get_level_values(0), [dataset_to_print])]\n",
    "#         df = df.reset_index(level=['Encoding_Type', 'Train_Test'])\n",
    "\n",
    "#         # Some boilerplate to initialise things\n",
    "#         sns.set()\n",
    "#         plt.figure(figsize=(20,8))\n",
    "\n",
    "#         # This is where the actual plot gets made\n",
    "#         ax = sns.barplot(data=df, x=\"Encoding_Type\", y=metric_to_plot, hue=\"Train_Test\")\n",
    "\n",
    "#         # Customise some display properties\n",
    "#         ax.set_title(dataset_to_print+\" - \"+modelNames[0])\n",
    "#         ax.grid(color='#cccccc')\n",
    "#         ax.set_ylabel(metric_to_plot)\n",
    "#         ax.set_xlabel(\"Encoding_Type\")\n",
    "#         ax.set_xticklabels(df[\"Encoding_Type\"].unique().astype(str), rotation='vertical')\n",
    "        \n",
    "# #         for p in ax.patches:\n",
    "# #             ax.annotate(format(p.get_height()*100, '.4f'),\n",
    "# #                         (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "# #                         ha = 'center', va = 'center', \n",
    "# #                         size=15,\n",
    "# #                         xytext = (0, -12), \n",
    "# #                         textcoords = 'offset points')\n",
    "        \n",
    "#         plt.savefig(os.path.join(evalPath, \"{}_{}_{}_Comparison\".format(metric_to_plot, dataset_to_print, modelNames[0])))\n",
    "#         plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
