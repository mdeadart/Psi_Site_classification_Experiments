{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Define all parameters for model tuning\n",
    "##################################################################################\n",
    "\n",
    "n_fold = 10\n",
    "expName = \"PSI_Site_DLNN_CORENup\"\n",
    "outPath = \"Results\"\n",
    "foldName = \"folds.pickle\"\n",
    "\n",
    "# modelNames = [\"DLNN_3\", \"DLNN_5\"]\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "shuffle = True\n",
    "seed = None\n",
    "\n",
    "input_data_folder = \"Data\\\\Psi_Site_Chen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from Bio import SeqIO\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# print(tf.test.is_gpu_available(cuda_only=True))\n",
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### define all CUSTOM functions\n",
    "##################################################################################\n",
    "\n",
    "def one_hot_encode_dna(sequence):\n",
    "    \n",
    "    seq_encoded = np.zeros((len(sequence),4))\n",
    "    dict_nuc = {\n",
    "        \"A\": 0,\n",
    "        \"C\": 1,\n",
    "        \"G\": 2,\n",
    "        \"T\":3\n",
    "    }\n",
    "    i = 0\n",
    "    \n",
    "    for single_character in sequence:\n",
    "        if(single_character.upper() in dict_nuc.keys()):\n",
    "            seq_encoded[i][dict_nuc[single_character.upper()]] = 1\n",
    "            i = i+1\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    return seq_encoded\n",
    "\n",
    "def one_hot_encode_rna(sequence):\n",
    "    \n",
    "    seq_encoded = np.zeros((len(sequence),4))\n",
    "    dict_nuc = {\n",
    "        \"A\": 0,\n",
    "        \"C\": 1,\n",
    "        \"G\": 2,\n",
    "        \"U\":3\n",
    "    }\n",
    "    i = 0\n",
    "    \n",
    "    for single_character in sequence:\n",
    "        if(single_character.upper() in dict_nuc.keys()):\n",
    "            seq_encoded[i][dict_nuc[single_character.upper()]] = 1\n",
    "            i = i+1\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    return seq_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### define evaluator functions\n",
    "##################################################################################\n",
    "\n",
    "## Build the K-fold from dataset\n",
    "def build_kfold(features, labels, k=10, shuffle=False, seed=None):\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=shuffle, random_state=seed)\n",
    "    kfoldList = []\n",
    "    for train_index, test_index in skf.split(features, labels):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        kfoldList.append({\n",
    "            \"X_train\": X_train,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_train\":y_train,\n",
    "            \"y_test\":y_test\n",
    "        })\n",
    "    return kfoldList\n",
    "\n",
    "def pred2label(y_pred):\n",
    "    y_pred = np.round(y_pred).astype(int)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Function to customize the DLNN architecture with parameters\n",
    "##################################################################################\n",
    "\n",
    "def DLNN_CORENup(input_shape = (21,4),\n",
    "                   conv_filters_per_layer_1 = 50, kernel_length_1 = 5, conv_strides_1 = 1, ## 1st Convolutional layer parameters\n",
    "                   max_pool_width_1 = 2, max_pool_stride_1 = 2, ## 1st Maxpool layer parameters\n",
    "                   lstm_decode_units = 50, ## LSTM layer parameters\n",
    "                   conv_filters_per_layer_2 = 50,  kernel_length_2 = 10, conv_strides_2 = 1, ## 2nd Convolutional layer parameters\n",
    "                   max_pool_width_2 = 2, max_pool_stride_2 = 2, ## 2nd Maxpool layer parameters\n",
    "                   dense_decode_units = 370, ## Dense layer parameters\n",
    "                   prob = 0.5, learn_rate = 0.0003, loss = 'binary_crossentropy', metrics = None):\n",
    "    \n",
    "    beta = 0.001\n",
    "    \n",
    "    input1 = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    x1 = tf.keras.layers.Conv1D(conv_filters_per_layer_1, kernel_length_1, input_shape = input_shape, \n",
    "                                strides = conv_strides_1, kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "                                padding = \"same\"\n",
    "                               )(input1)\n",
    "    x1 = tf.keras.layers.Activation('relu')(x1)\n",
    "    x1 = tf.keras.layers.MaxPool1D(pool_size = max_pool_width_1, strides = max_pool_stride_1)(x1)\n",
    "    x1 = tf.keras.layers.Dropout(prob)(x1)\n",
    "\n",
    "    ## LSTM Path\n",
    "\n",
    "    x2 = tf.keras.layers.LSTM(lstm_decode_units, return_sequences = True, \n",
    "                              kernel_regularizer = tf.keras.regularizers.l2(beta))(x1)\n",
    "    x2 = tf.keras.layers.Dropout(prob)(x2)\n",
    "    \n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "\n",
    "    ## Conv Path\n",
    "\n",
    "    x3 = tf.keras.layers.Conv1D(conv_filters_per_layer_2, kernel_length_2, strides = conv_strides_2, \n",
    "                                kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "                                padding = 'same'\n",
    "                               )(x1)\n",
    "    x3 = tf.keras.layers.Activation('relu')(x3)\n",
    "    x3 = tf.keras.layers.MaxPooling1D(pool_size = max_pool_width_2, strides = max_pool_stride_2)(x3)\n",
    "    x3 = tf.keras.layers.Dropout(prob)(x3)\n",
    "    \n",
    "    x3 = tf.keras.layers.Flatten()(x3)\n",
    "\n",
    "    ## Fully connected Layers\n",
    "\n",
    "    y = tf.keras.layers.Concatenate(1)([x2,x3])\n",
    "    \n",
    "    y = tf.keras.layers.Dense(dense_decode_units, kernel_regularizer = tf.keras.regularizers.l2(beta), activation = 'relu')(y)\n",
    "    \n",
    "    y = tf.keras.layers.Dropout(prob)(y)\n",
    "    \n",
    "    y = tf.keras.layers.Dense(1, kernel_regularizer = tf.keras.regularizers.l2(beta), activation = 'sigmoid')(y)\n",
    "\n",
    "    ## Generate Model from input and output\n",
    "    model = tf.keras.models.Model(inputs=[input1], outputs=y)\n",
    "    \n",
    "    ## Compile model\n",
    "    if(metrics != None):\n",
    "        model.compile(optimizer = tf.keras.optimizers.Adam(lr=learn_rate), loss = loss, metrics = metrics)\n",
    "    else:\n",
    "        model.compile(optimizer = tf.keras.optimizers.Adam(lr=learn_rate), loss = loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 21, 4)]      0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 21, 50)       1050        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 21, 50)       0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 10, 50)       0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 10, 50)       0           ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 10, 50)       25050       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 10, 50)       0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 10, 50)       20200       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 5, 50)       0           ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 10, 50)       0           ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 5, 50)        0           ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 500)          0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 250)          0           ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 750)          0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 370)          277870      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 370)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            371         ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 324,541\n",
      "Trainable params: 324,541\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tf_2_8_py_3_10\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "DLNN_CORENup().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### For each input file, train model and generate different outputs in a structured folder\n",
    "##################################################################################\n",
    "\n",
    "## create the evaluation data structure for all iterations\n",
    "evaluations = {\n",
    "    \"Model\" : [],\n",
    "    \"Dataset\" : [],\n",
    "    \"Fold\" : [],\n",
    "    \"Train_Test\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "File: Data\\Psi_Site_Chen\\HS_990.txt\n",
      "Positive: 495\n",
      "Negative: 495\n",
      "\n",
      "Train/Test model HS_990 on Fold #0.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tf_2_8_py_3_10\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/28 [==========================>...] - ETA: 0s - loss: 1.3100\n",
      "Epoch 1: val_loss improved from inf to 1.26578, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 2s 34ms/step - loss: 1.3063 - val_loss: 1.2658\n",
      "Epoch 2/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.2512\n",
      "Epoch 2: val_loss improved from 1.26578 to 1.20996, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.2487 - val_loss: 1.2100\n",
      "Epoch 3/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.2001\n",
      "Epoch 3: val_loss improved from 1.20996 to 1.16363, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.1992 - val_loss: 1.1636\n",
      "Epoch 4/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 1.1508\n",
      "Epoch 4: val_loss improved from 1.16363 to 1.12448, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.1504 - val_loss: 1.1245\n",
      "Epoch 5/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.1193\n",
      "Epoch 5: val_loss improved from 1.12448 to 1.09094, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.1169 - val_loss: 1.0909\n",
      "Epoch 6/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 1.0776\n",
      "Epoch 6: val_loss improved from 1.09094 to 1.05944, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.0778 - val_loss: 1.0594\n",
      "Epoch 7/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 1.0531\n",
      "Epoch 7: val_loss improved from 1.05944 to 1.03090, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.0532 - val_loss: 1.0309\n",
      "Epoch 8/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.0288\n",
      "Epoch 8: val_loss improved from 1.03090 to 1.00461, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 1.0298 - val_loss: 1.0046\n",
      "Epoch 9/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 1.0156\n",
      "Epoch 9: val_loss improved from 1.00461 to 0.98325, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.0111 - val_loss: 0.9833\n",
      "Epoch 10/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9845\n",
      "Epoch 10: val_loss improved from 0.98325 to 0.96309, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9802 - val_loss: 0.9631\n",
      "Epoch 11/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.9636\n",
      "Epoch 11: val_loss improved from 0.96309 to 0.94404, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9648 - val_loss: 0.9440\n",
      "Epoch 12/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.9602\n",
      "Epoch 12: val_loss improved from 0.94404 to 0.93170, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9555 - val_loss: 0.9317\n",
      "Epoch 13/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9387\n",
      "Epoch 13: val_loss improved from 0.93170 to 0.91927, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9364 - val_loss: 0.9193\n",
      "Epoch 14/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.9200\n",
      "Epoch 14: val_loss improved from 0.91927 to 0.90135, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9135 - val_loss: 0.9014\n",
      "Epoch 15/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.9085\n",
      "Epoch 15: val_loss improved from 0.90135 to 0.88782, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.9095 - val_loss: 0.8878\n",
      "Epoch 16/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8926\n",
      "Epoch 16: val_loss improved from 0.88782 to 0.87726, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8934 - val_loss: 0.8773\n",
      "Epoch 17/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8755\n",
      "Epoch 17: val_loss improved from 0.87726 to 0.86272, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8750 - val_loss: 0.8627\n",
      "Epoch 18/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.8599\n",
      "Epoch 18: val_loss improved from 0.86272 to 0.85299, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8661 - val_loss: 0.8530\n",
      "Epoch 19/100\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.8496\n",
      "Epoch 19: val_loss improved from 0.85299 to 0.84276, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.8511 - val_loss: 0.8428\n",
      "Epoch 20/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.8514\n",
      "Epoch 20: val_loss improved from 0.84276 to 0.83933, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8489 - val_loss: 0.8393\n",
      "Epoch 21/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8337\n",
      "Epoch 21: val_loss improved from 0.83933 to 0.83117, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8352 - val_loss: 0.8312\n",
      "Epoch 22/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8432\n",
      "Epoch 22: val_loss improved from 0.83117 to 0.82442, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8417 - val_loss: 0.8244\n",
      "Epoch 23/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.8226\n",
      "Epoch 23: val_loss improved from 0.82442 to 0.81599, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.8211 - val_loss: 0.8160\n",
      "Epoch 24/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.8089\n",
      "Epoch 24: val_loss improved from 0.81599 to 0.80952, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8043 - val_loss: 0.8095\n",
      "Epoch 25/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7946\n",
      "Epoch 25: val_loss improved from 0.80952 to 0.79891, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7972 - val_loss: 0.7989\n",
      "Epoch 26/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8019\n",
      "Epoch 26: val_loss did not improve from 0.79891\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.8010 - val_loss: 0.8039\n",
      "Epoch 27/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.7779\n",
      "Epoch 27: val_loss improved from 0.79891 to 0.78809, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7862 - val_loss: 0.7881\n",
      "Epoch 28/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7745\n",
      "Epoch 28: val_loss improved from 0.78809 to 0.78530, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7747 - val_loss: 0.7853\n",
      "Epoch 29/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.7603\n",
      "Epoch 29: val_loss improved from 0.78530 to 0.77926, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7714 - val_loss: 0.7793\n",
      "Epoch 30/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.7653\n",
      "Epoch 30: val_loss did not improve from 0.77926\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.7653 - val_loss: 0.7803\n",
      "Epoch 31/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7556\n",
      "Epoch 31: val_loss improved from 0.77926 to 0.77119, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.7538 - val_loss: 0.7712\n",
      "Epoch 32/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7360\n",
      "Epoch 32: val_loss improved from 0.77119 to 0.76728, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7410 - val_loss: 0.7673\n",
      "Epoch 33/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7568\n",
      "Epoch 33: val_loss improved from 0.76728 to 0.76494, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.7532 - val_loss: 0.7649\n",
      "Epoch 34/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.7534\n",
      "Epoch 34: val_loss improved from 0.76494 to 0.76071, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7465 - val_loss: 0.7607\n",
      "Epoch 35/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.7221\n",
      "Epoch 35: val_loss improved from 0.76071 to 0.75424, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7355 - val_loss: 0.7542\n",
      "Epoch 36/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7270\n",
      "Epoch 36: val_loss did not improve from 0.75424\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7223 - val_loss: 0.7601\n",
      "Epoch 37/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.7188\n",
      "Epoch 37: val_loss improved from 0.75424 to 0.74861, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7194 - val_loss: 0.7486\n",
      "Epoch 38/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.7261\n",
      "Epoch 38: val_loss improved from 0.74861 to 0.74690, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.7182 - val_loss: 0.7469\n",
      "Epoch 39/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.7224\n",
      "Epoch 39: val_loss did not improve from 0.74690\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7263 - val_loss: 0.7494\n",
      "Epoch 40/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7278\n",
      "Epoch 40: val_loss improved from 0.74690 to 0.74546, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7284 - val_loss: 0.7455\n",
      "Epoch 41/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.7008\n",
      "Epoch 41: val_loss improved from 0.74546 to 0.74125, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7004 - val_loss: 0.7413\n",
      "Epoch 42/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.7086\n",
      "Epoch 42: val_loss improved from 0.74125 to 0.73712, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.7047 - val_loss: 0.7371\n",
      "Epoch 43/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.6749\n",
      "Epoch 43: val_loss did not improve from 0.73712\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6852 - val_loss: 0.7377\n",
      "Epoch 44/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6974\n",
      "Epoch 44: val_loss improved from 0.73712 to 0.73578, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7028 - val_loss: 0.7358\n",
      "Epoch 45/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.6802\n",
      "Epoch 45: val_loss improved from 0.73578 to 0.73451, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6806 - val_loss: 0.7345\n",
      "Epoch 46/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6930\n",
      "Epoch 46: val_loss improved from 0.73451 to 0.73091, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6874 - val_loss: 0.7309\n",
      "Epoch 47/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6650\n",
      "Epoch 47: val_loss improved from 0.73091 to 0.73086, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6658 - val_loss: 0.7309\n",
      "Epoch 48/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.6624\n",
      "Epoch 48: val_loss improved from 0.73086 to 0.73059, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6746 - val_loss: 0.7306\n",
      "Epoch 49/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.6588\n",
      "Epoch 49: val_loss did not improve from 0.73059\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.6647 - val_loss: 0.7490\n",
      "Epoch 50/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6703\n",
      "Epoch 50: val_loss did not improve from 0.73059\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6647 - val_loss: 0.7308\n",
      "Epoch 51/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.6712\n",
      "Epoch 51: val_loss did not improve from 0.73059\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6648 - val_loss: 0.7341\n",
      "Epoch 52/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.6686\n",
      "Epoch 52: val_loss did not improve from 0.73059\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.6681 - val_loss: 0.7358\n",
      "Epoch 53/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6512\n",
      "Epoch 53: val_loss did not improve from 0.73059\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6510 - val_loss: 0.7338\n",
      "Epoch 54/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6408\n",
      "Epoch 54: val_loss did not improve from 0.73059\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6428 - val_loss: 0.7369\n",
      "Epoch 55/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6561\n",
      "Epoch 55: val_loss did not improve from 0.73059\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6506 - val_loss: 0.7327\n",
      "Epoch 56/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6622\n",
      "Epoch 56: val_loss did not improve from 0.73059\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6569 - val_loss: 0.7314\n",
      "Epoch 57/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6353\n",
      "Epoch 57: val_loss did not improve from 0.73059\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6428 - val_loss: 0.7332\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6424\n",
      "Epoch 58: val_loss improved from 0.73059 to 0.73024, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6372 - val_loss: 0.7302\n",
      "Epoch 59/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6278\n",
      "Epoch 59: val_loss improved from 0.73024 to 0.72888, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6299 - val_loss: 0.7289\n",
      "Epoch 60/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.6040\n",
      "Epoch 60: val_loss improved from 0.72888 to 0.72877, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6103 - val_loss: 0.7288\n",
      "Epoch 61/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.6222\n",
      "Epoch 61: val_loss improved from 0.72877 to 0.72323, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6142 - val_loss: 0.7232\n",
      "Epoch 62/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6199\n",
      "Epoch 62: val_loss did not improve from 0.72323\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6176 - val_loss: 0.7258\n",
      "Epoch 63/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6097\n",
      "Epoch 63: val_loss improved from 0.72323 to 0.72028, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold0.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6096 - val_loss: 0.7203\n",
      "Epoch 64/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5768\n",
      "Epoch 64: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5823 - val_loss: 0.7260\n",
      "Epoch 65/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.6167\n",
      "Epoch 65: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6210 - val_loss: 0.7268\n",
      "Epoch 66/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6072\n",
      "Epoch 66: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6118 - val_loss: 0.7280\n",
      "Epoch 67/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5903\n",
      "Epoch 67: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5983 - val_loss: 0.7240\n",
      "Epoch 68/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5767\n",
      "Epoch 68: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5833 - val_loss: 0.7309\n",
      "Epoch 69/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6105\n",
      "Epoch 69: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6104 - val_loss: 0.7331\n",
      "Epoch 70/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.6009\n",
      "Epoch 70: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6013 - val_loss: 0.7304\n",
      "Epoch 71/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5800\n",
      "Epoch 71: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5830 - val_loss: 0.7423\n",
      "Epoch 72/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6051\n",
      "Epoch 72: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6103 - val_loss: 0.7425\n",
      "Epoch 73/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5967\n",
      "Epoch 73: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5968 - val_loss: 0.7369\n",
      "Epoch 74/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5795\n",
      "Epoch 74: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5728 - val_loss: 0.7375\n",
      "Epoch 75/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.6066\n",
      "Epoch 75: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5976 - val_loss: 0.7529\n",
      "Epoch 76/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5821\n",
      "Epoch 76: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5788 - val_loss: 0.7463\n",
      "Epoch 77/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5740\n",
      "Epoch 77: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5744 - val_loss: 0.7344\n",
      "Epoch 78/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5899\n",
      "Epoch 78: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5867 - val_loss: 0.7366\n",
      "Epoch 79/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5552\n",
      "Epoch 79: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5638 - val_loss: 0.7368\n",
      "Epoch 80/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5734\n",
      "Epoch 80: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5768 - val_loss: 0.7317\n",
      "Epoch 81/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5568\n",
      "Epoch 81: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5594 - val_loss: 0.7374\n",
      "Epoch 82/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5422\n",
      "Epoch 82: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5456 - val_loss: 0.7563\n",
      "Epoch 83/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5470\n",
      "Epoch 83: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5455 - val_loss: 0.7453\n",
      "Epoch 84/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5652\n",
      "Epoch 84: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5624 - val_loss: 0.7425\n",
      "Epoch 85/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5371\n",
      "Epoch 85: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5369 - val_loss: 0.7436\n",
      "Epoch 86/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5561\n",
      "Epoch 86: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5502 - val_loss: 0.7421\n",
      "Epoch 87/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5328\n",
      "Epoch 87: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5322 - val_loss: 0.7466\n",
      "Epoch 88/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5710\n",
      "Epoch 88: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5649 - val_loss: 0.7438\n",
      "Epoch 89/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5638\n",
      "Epoch 89: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5554 - val_loss: 0.7375\n",
      "Epoch 90/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5340\n",
      "Epoch 90: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5334 - val_loss: 0.7360\n",
      "Epoch 91/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5391\n",
      "Epoch 91: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5427 - val_loss: 0.7349\n",
      "Epoch 92/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5472\n",
      "Epoch 92: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5495 - val_loss: 0.7366\n",
      "Epoch 93/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5583\n",
      "Epoch 93: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5531 - val_loss: 0.7415\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5243\n",
      "Epoch 94: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5327 - val_loss: 0.7372\n",
      "Epoch 95/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5345\n",
      "Epoch 95: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5396 - val_loss: 0.7351\n",
      "Epoch 96/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5363\n",
      "Epoch 96: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5348 - val_loss: 0.7373\n",
      "Epoch 97/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5248\n",
      "Epoch 97: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5253 - val_loss: 0.7475\n",
      "Epoch 98/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5020\n",
      "Epoch 98: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5043 - val_loss: 0.7622\n",
      "Epoch 99/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5041\n",
      "Epoch 99: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5028 - val_loss: 0.7531\n",
      "Epoch 100/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5286\n",
      "Epoch 100: val_loss did not improve from 0.72028\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5217 - val_loss: 0.7484\n",
      "\n",
      "Train/Test model HS_990 on Fold #1.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tf_2_8_py_3_10\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/28 [=========================>....] - ETA: 0s - loss: 1.3159\n",
      "Epoch 1: val_loss improved from inf to 1.27175, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 2s 27ms/step - loss: 1.3138 - val_loss: 1.2718\n",
      "Epoch 2/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.2644\n",
      "Epoch 2: val_loss improved from 1.27175 to 1.22389, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 1.2619 - val_loss: 1.2239\n",
      "Epoch 3/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.2183\n",
      "Epoch 3: val_loss improved from 1.22389 to 1.18509, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 1.2144 - val_loss: 1.1851\n",
      "Epoch 4/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 1.1733\n",
      "Epoch 4: val_loss improved from 1.18509 to 1.14962, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.1692 - val_loss: 1.1496\n",
      "Epoch 5/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 1.1334\n",
      "Epoch 5: val_loss improved from 1.14962 to 1.11922, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.1348 - val_loss: 1.1192\n",
      "Epoch 6/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.1107\n",
      "Epoch 6: val_loss improved from 1.11922 to 1.08815, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.1081 - val_loss: 1.0882\n",
      "Epoch 7/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 1.0770\n",
      "Epoch 7: val_loss improved from 1.08815 to 1.06077, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.0744 - val_loss: 1.0608\n",
      "Epoch 8/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 1.0530\n",
      "Epoch 8: val_loss improved from 1.06077 to 1.03935, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.0498 - val_loss: 1.0393\n",
      "Epoch 9/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 1.0317\n",
      "Epoch 9: val_loss improved from 1.03935 to 1.01607, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.0302 - val_loss: 1.0161\n",
      "Epoch 10/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 1.0031\n",
      "Epoch 10: val_loss improved from 1.01607 to 0.99860, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.0050 - val_loss: 0.9986\n",
      "Epoch 11/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9871\n",
      "Epoch 11: val_loss improved from 0.99860 to 0.98110, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.9875 - val_loss: 0.9811\n",
      "Epoch 12/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9631\n",
      "Epoch 12: val_loss improved from 0.98110 to 0.96311, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.9638 - val_loss: 0.9631\n",
      "Epoch 13/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9578\n",
      "Epoch 13: val_loss improved from 0.96311 to 0.94835, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.9590 - val_loss: 0.9484\n",
      "Epoch 14/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9467\n",
      "Epoch 14: val_loss improved from 0.94835 to 0.93518, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9449 - val_loss: 0.9352\n",
      "Epoch 15/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9315\n",
      "Epoch 15: val_loss improved from 0.93518 to 0.92417, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9283 - val_loss: 0.9242\n",
      "Epoch 16/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9157\n",
      "Epoch 16: val_loss improved from 0.92417 to 0.91521, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9173 - val_loss: 0.9152\n",
      "Epoch 17/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8966\n",
      "Epoch 17: val_loss improved from 0.91521 to 0.90310, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8965 - val_loss: 0.9031\n",
      "Epoch 18/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8844\n",
      "Epoch 18: val_loss improved from 0.90310 to 0.89427, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.8890 - val_loss: 0.8943\n",
      "Epoch 19/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8803\n",
      "Epoch 19: val_loss improved from 0.89427 to 0.88146, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.8813 - val_loss: 0.8815\n",
      "Epoch 20/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8640\n",
      "Epoch 20: val_loss improved from 0.88146 to 0.87415, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8630 - val_loss: 0.8741\n",
      "Epoch 21/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8488\n",
      "Epoch 21: val_loss improved from 0.87415 to 0.87097, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.8500 - val_loss: 0.8710\n",
      "Epoch 22/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8488\n",
      "Epoch 22: val_loss improved from 0.87097 to 0.86352, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8455 - val_loss: 0.8635\n",
      "Epoch 23/100\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.8361\n",
      "Epoch 23: val_loss improved from 0.86352 to 0.85392, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.8381 - val_loss: 0.8539\n",
      "Epoch 24/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.8300\n",
      "Epoch 24: val_loss improved from 0.85392 to 0.84598, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8307 - val_loss: 0.8460\n",
      "Epoch 25/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8170\n",
      "Epoch 25: val_loss did not improve from 0.84598\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.8127 - val_loss: 0.8502\n",
      "Epoch 26/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.8099\n",
      "Epoch 26: val_loss improved from 0.84598 to 0.83799, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8148 - val_loss: 0.8380\n",
      "Epoch 27/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7974\n",
      "Epoch 27: val_loss improved from 0.83799 to 0.83121, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8018 - val_loss: 0.8312\n",
      "Epoch 28/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.7966\n",
      "Epoch 28: val_loss improved from 0.83121 to 0.82563, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.7993 - val_loss: 0.8256\n",
      "Epoch 29/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7877\n",
      "Epoch 29: val_loss improved from 0.82563 to 0.82174, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7918 - val_loss: 0.8217\n",
      "Epoch 30/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7792\n",
      "Epoch 30: val_loss did not improve from 0.82174\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7803 - val_loss: 0.8227\n",
      "Epoch 31/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.7695\n",
      "Epoch 31: val_loss improved from 0.82174 to 0.81697, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7737 - val_loss: 0.8170\n",
      "Epoch 32/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.7742\n",
      "Epoch 32: val_loss improved from 0.81697 to 0.81385, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.7742 - val_loss: 0.8138\n",
      "Epoch 33/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.7645\n",
      "Epoch 33: val_loss did not improve from 0.81385\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7594 - val_loss: 0.8178\n",
      "Epoch 34/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7420\n",
      "Epoch 34: val_loss did not improve from 0.81385\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7494 - val_loss: 0.8225\n",
      "Epoch 35/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7546\n",
      "Epoch 35: val_loss improved from 0.81385 to 0.81349, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.7517 - val_loss: 0.8135\n",
      "Epoch 36/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7411\n",
      "Epoch 36: val_loss improved from 0.81349 to 0.80542, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.7437 - val_loss: 0.8054\n",
      "Epoch 37/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.7396\n",
      "Epoch 37: val_loss improved from 0.80542 to 0.79840, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7479 - val_loss: 0.7984\n",
      "Epoch 38/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7503\n",
      "Epoch 38: val_loss improved from 0.79840 to 0.79746, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7472 - val_loss: 0.7975\n",
      "Epoch 39/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7479\n",
      "Epoch 39: val_loss improved from 0.79746 to 0.78764, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7465 - val_loss: 0.7876\n",
      "Epoch 40/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.7164\n",
      "Epoch 40: val_loss did not improve from 0.78764\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7212 - val_loss: 0.7963\n",
      "Epoch 41/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7186\n",
      "Epoch 41: val_loss improved from 0.78764 to 0.78711, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7172 - val_loss: 0.7871\n",
      "Epoch 42/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7113\n",
      "Epoch 42: val_loss did not improve from 0.78711\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7053 - val_loss: 0.7878\n",
      "Epoch 43/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6830\n",
      "Epoch 43: val_loss did not improve from 0.78711\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6867 - val_loss: 0.7880\n",
      "Epoch 44/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6961\n",
      "Epoch 44: val_loss improved from 0.78711 to 0.78567, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.7060 - val_loss: 0.7857\n",
      "Epoch 45/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7009\n",
      "Epoch 45: val_loss improved from 0.78567 to 0.78069, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7035 - val_loss: 0.7807\n",
      "Epoch 46/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7131\n",
      "Epoch 46: val_loss did not improve from 0.78069\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7068 - val_loss: 0.7821\n",
      "Epoch 47/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6934\n",
      "Epoch 47: val_loss did not improve from 0.78069\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6922 - val_loss: 0.7860\n",
      "Epoch 48/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.7010\n",
      "Epoch 48: val_loss did not improve from 0.78069\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6975 - val_loss: 0.7816\n",
      "Epoch 49/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.6927\n",
      "Epoch 49: val_loss did not improve from 0.78069\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6897 - val_loss: 0.7830\n",
      "Epoch 50/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.6678\n",
      "Epoch 50: val_loss did not improve from 0.78069\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6695 - val_loss: 0.7854\n",
      "Epoch 51/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.6660\n",
      "Epoch 51: val_loss did not improve from 0.78069\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6712 - val_loss: 0.7908\n",
      "Epoch 52/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6652\n",
      "Epoch 52: val_loss did not improve from 0.78069\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6642 - val_loss: 0.7964\n",
      "Epoch 53/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6692\n",
      "Epoch 53: val_loss did not improve from 0.78069\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6722 - val_loss: 0.7812\n",
      "Epoch 54/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.6692\n",
      "Epoch 54: val_loss improved from 0.78069 to 0.77375, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6713 - val_loss: 0.7738\n",
      "Epoch 55/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6641\n",
      "Epoch 55: val_loss did not improve from 0.77375\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6604 - val_loss: 0.7849\n",
      "Epoch 56/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.6640\n",
      "Epoch 56: val_loss did not improve from 0.77375\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6651 - val_loss: 0.7748\n",
      "Epoch 57/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6529\n",
      "Epoch 57: val_loss improved from 0.77375 to 0.77081, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6473 - val_loss: 0.7708\n",
      "Epoch 58/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.6435\n",
      "Epoch 58: val_loss did not improve from 0.77081\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6491 - val_loss: 0.8004\n",
      "Epoch 59/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6459\n",
      "Epoch 59: val_loss did not improve from 0.77081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6406 - val_loss: 0.7717\n",
      "Epoch 60/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6397\n",
      "Epoch 60: val_loss did not improve from 0.77081\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6354 - val_loss: 0.7795\n",
      "Epoch 61/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.6423\n",
      "Epoch 61: val_loss improved from 0.77081 to 0.76732, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6446 - val_loss: 0.7673\n",
      "Epoch 62/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6364\n",
      "Epoch 62: val_loss improved from 0.76732 to 0.76290, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold1.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.6361 - val_loss: 0.7629\n",
      "Epoch 63/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6446\n",
      "Epoch 63: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6370 - val_loss: 0.7680\n",
      "Epoch 64/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6245\n",
      "Epoch 64: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6316 - val_loss: 0.7780\n",
      "Epoch 65/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6178\n",
      "Epoch 65: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6260 - val_loss: 0.8041\n",
      "Epoch 66/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.6297\n",
      "Epoch 66: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6321 - val_loss: 0.7812\n",
      "Epoch 67/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6110\n",
      "Epoch 67: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6109 - val_loss: 0.7770\n",
      "Epoch 68/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6034\n",
      "Epoch 68: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5969 - val_loss: 0.7852\n",
      "Epoch 69/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5993\n",
      "Epoch 69: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5967 - val_loss: 0.7845\n",
      "Epoch 70/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.6030\n",
      "Epoch 70: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6004 - val_loss: 0.7772\n",
      "Epoch 71/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6080\n",
      "Epoch 71: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6087 - val_loss: 0.7912\n",
      "Epoch 72/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6239\n",
      "Epoch 72: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6096 - val_loss: 0.7661\n",
      "Epoch 73/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5980\n",
      "Epoch 73: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6045 - val_loss: 0.7635\n",
      "Epoch 74/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5882\n",
      "Epoch 74: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5910 - val_loss: 0.7720\n",
      "Epoch 75/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5976\n",
      "Epoch 75: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6074 - val_loss: 0.7826\n",
      "Epoch 76/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5860\n",
      "Epoch 76: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5918 - val_loss: 0.7794\n",
      "Epoch 77/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5772\n",
      "Epoch 77: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5810 - val_loss: 0.7797\n",
      "Epoch 78/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5877\n",
      "Epoch 78: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5923 - val_loss: 0.7711\n",
      "Epoch 79/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5698\n",
      "Epoch 79: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5740 - val_loss: 0.7931\n",
      "Epoch 80/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5712\n",
      "Epoch 80: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.5694 - val_loss: 0.7911\n",
      "Epoch 81/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5810\n",
      "Epoch 81: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5812 - val_loss: 0.7831\n",
      "Epoch 82/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5785\n",
      "Epoch 82: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5844 - val_loss: 0.7907\n",
      "Epoch 83/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5794\n",
      "Epoch 83: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5761 - val_loss: 0.7961\n",
      "Epoch 84/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.5820\n",
      "Epoch 84: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5734 - val_loss: 0.8148\n",
      "Epoch 85/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5638\n",
      "Epoch 85: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5717 - val_loss: 0.8112\n",
      "Epoch 86/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5627\n",
      "Epoch 86: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5700 - val_loss: 0.7849\n",
      "Epoch 87/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5420\n",
      "Epoch 87: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.5375 - val_loss: 0.8058\n",
      "Epoch 88/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5414\n",
      "Epoch 88: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5435 - val_loss: 0.8392\n",
      "Epoch 89/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5482\n",
      "Epoch 89: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5465 - val_loss: 0.8078\n",
      "Epoch 90/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5448\n",
      "Epoch 90: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5490 - val_loss: 0.8158\n",
      "Epoch 91/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5251\n",
      "Epoch 91: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5296 - val_loss: 0.8117\n",
      "Epoch 92/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5331\n",
      "Epoch 92: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5410 - val_loss: 0.8144\n",
      "Epoch 93/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5189\n",
      "Epoch 93: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5352 - val_loss: 0.8186\n",
      "Epoch 94/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5335\n",
      "Epoch 94: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5297 - val_loss: 0.8022\n",
      "Epoch 95/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5479\n",
      "Epoch 95: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5502 - val_loss: 0.7960\n",
      "Epoch 96/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5460\n",
      "Epoch 96: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5405 - val_loss: 0.8012\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5525\n",
      "Epoch 97: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5563 - val_loss: 0.8187\n",
      "Epoch 98/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5250\n",
      "Epoch 98: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5336 - val_loss: 0.8118\n",
      "Epoch 99/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5270\n",
      "Epoch 99: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5289 - val_loss: 0.8094\n",
      "Epoch 100/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5428\n",
      "Epoch 100: val_loss did not improve from 0.76290\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5415 - val_loss: 0.8142\n",
      "\n",
      "Train/Test model HS_990 on Fold #2.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tf_2_8_py_3_10\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/28 [=======================>......] - ETA: 0s - loss: 1.3125\n",
      "Epoch 1: val_loss improved from inf to 1.27642, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 2s 32ms/step - loss: 1.3083 - val_loss: 1.2764\n",
      "Epoch 2/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 1.2584\n",
      "Epoch 2: val_loss improved from 1.27642 to 1.23329, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.2553 - val_loss: 1.2333\n",
      "Epoch 3/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 1.2129\n",
      "Epoch 3: val_loss improved from 1.23329 to 1.19260, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.2101 - val_loss: 1.1926\n",
      "Epoch 4/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.1739\n",
      "Epoch 4: val_loss improved from 1.19260 to 1.15845, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 1.1723 - val_loss: 1.1584\n",
      "Epoch 5/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.1343\n",
      "Epoch 5: val_loss improved from 1.15845 to 1.13191, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.1347 - val_loss: 1.1319\n",
      "Epoch 6/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.1163\n",
      "Epoch 6: val_loss improved from 1.13191 to 1.10297, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.1144 - val_loss: 1.1030\n",
      "Epoch 7/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.0815\n",
      "Epoch 7: val_loss improved from 1.10297 to 1.07710, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 1.0801 - val_loss: 1.0771\n",
      "Epoch 8/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.0591\n",
      "Epoch 8: val_loss improved from 1.07710 to 1.05620, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 1.0589 - val_loss: 1.0562\n",
      "Epoch 9/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 1.0349\n",
      "Epoch 9: val_loss improved from 1.05620 to 1.03639, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 1.0349 - val_loss: 1.0364\n",
      "Epoch 10/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.0121\n",
      "Epoch 10: val_loss improved from 1.03639 to 1.01519, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.0105 - val_loss: 1.0152\n",
      "Epoch 11/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9943\n",
      "Epoch 11: val_loss improved from 1.01519 to 0.99884, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9923 - val_loss: 0.9988\n",
      "Epoch 12/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9756\n",
      "Epoch 12: val_loss improved from 0.99884 to 0.98389, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9755 - val_loss: 0.9839\n",
      "Epoch 13/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9566\n",
      "Epoch 13: val_loss improved from 0.98389 to 0.97153, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9531 - val_loss: 0.9715\n",
      "Epoch 14/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9428\n",
      "Epoch 14: val_loss improved from 0.97153 to 0.96050, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.9449 - val_loss: 0.9605\n",
      "Epoch 15/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9306\n",
      "Epoch 15: val_loss improved from 0.96050 to 0.94719, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9268 - val_loss: 0.9472\n",
      "Epoch 16/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9104\n",
      "Epoch 16: val_loss improved from 0.94719 to 0.93959, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.9133 - val_loss: 0.9396\n",
      "Epoch 17/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9020\n",
      "Epoch 17: val_loss improved from 0.93959 to 0.92755, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.9048 - val_loss: 0.9276\n",
      "Epoch 18/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8948\n",
      "Epoch 18: val_loss improved from 0.92755 to 0.91982, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8950 - val_loss: 0.9198\n",
      "Epoch 19/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8771\n",
      "Epoch 19: val_loss improved from 0.91982 to 0.91145, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8764 - val_loss: 0.9115\n",
      "Epoch 20/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.8717\n",
      "Epoch 20: val_loss improved from 0.91145 to 0.90242, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8708 - val_loss: 0.9024\n",
      "Epoch 21/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8558\n",
      "Epoch 21: val_loss improved from 0.90242 to 0.89419, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8570 - val_loss: 0.8942\n",
      "Epoch 22/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8497\n",
      "Epoch 22: val_loss improved from 0.89419 to 0.88841, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.8471 - val_loss: 0.8884\n",
      "Epoch 23/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.8385\n",
      "Epoch 23: val_loss improved from 0.88841 to 0.88135, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.8355 - val_loss: 0.8814\n",
      "Epoch 24/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.8160\n",
      "Epoch 24: val_loss improved from 0.88135 to 0.87824, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8174 - val_loss: 0.8782\n",
      "Epoch 25/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8272\n",
      "Epoch 25: val_loss improved from 0.87824 to 0.86807, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8284 - val_loss: 0.8681\n",
      "Epoch 26/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8052\n",
      "Epoch 26: val_loss improved from 0.86807 to 0.86624, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.8061 - val_loss: 0.8662\n",
      "Epoch 27/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8163\n",
      "Epoch 27: val_loss improved from 0.86624 to 0.85818, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8109 - val_loss: 0.8582\n",
      "Epoch 28/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.8032\n",
      "Epoch 28: val_loss improved from 0.85818 to 0.85299, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7996 - val_loss: 0.8530\n",
      "Epoch 29/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8014\n",
      "Epoch 29: val_loss improved from 0.85299 to 0.84862, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.7997 - val_loss: 0.8486\n",
      "Epoch 30/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.7924\n",
      "Epoch 30: val_loss improved from 0.84862 to 0.84274, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.7886 - val_loss: 0.8427\n",
      "Epoch 31/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.7689\n",
      "Epoch 31: val_loss improved from 0.84274 to 0.83606, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.7712 - val_loss: 0.8361\n",
      "Epoch 32/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.7877\n",
      "Epoch 32: val_loss improved from 0.83606 to 0.83203, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7845 - val_loss: 0.8320\n",
      "Epoch 33/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7634\n",
      "Epoch 33: val_loss improved from 0.83203 to 0.83097, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.7673 - val_loss: 0.8310\n",
      "Epoch 34/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.7516\n",
      "Epoch 34: val_loss improved from 0.83097 to 0.82302, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7543 - val_loss: 0.8230\n",
      "Epoch 35/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7633\n",
      "Epoch 35: val_loss improved from 0.82302 to 0.82050, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.7685 - val_loss: 0.8205\n",
      "Epoch 36/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.7606\n",
      "Epoch 36: val_loss improved from 0.82050 to 0.81465, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7560 - val_loss: 0.8147\n",
      "Epoch 37/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7322\n",
      "Epoch 37: val_loss did not improve from 0.81465\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7364 - val_loss: 0.8149\n",
      "Epoch 38/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7398\n",
      "Epoch 38: val_loss improved from 0.81465 to 0.81322, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.7378 - val_loss: 0.8132\n",
      "Epoch 39/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7384\n",
      "Epoch 39: val_loss did not improve from 0.81322\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7388 - val_loss: 0.8171\n",
      "Epoch 40/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7348\n",
      "Epoch 40: val_loss improved from 0.81322 to 0.80812, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7282 - val_loss: 0.8081\n",
      "Epoch 41/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7323\n",
      "Epoch 41: val_loss improved from 0.80812 to 0.80434, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7296 - val_loss: 0.8043\n",
      "Epoch 42/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6967\n",
      "Epoch 42: val_loss improved from 0.80434 to 0.80330, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6981 - val_loss: 0.8033\n",
      "Epoch 43/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7129\n",
      "Epoch 43: val_loss did not improve from 0.80330\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7100 - val_loss: 0.8070\n",
      "Epoch 44/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6972\n",
      "Epoch 44: val_loss did not improve from 0.80330\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6994 - val_loss: 0.8049\n",
      "Epoch 45/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7195\n",
      "Epoch 45: val_loss did not improve from 0.80330\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7211 - val_loss: 0.8069\n",
      "Epoch 46/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6949\n",
      "Epoch 46: val_loss improved from 0.80330 to 0.79823, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.7048 - val_loss: 0.7982\n",
      "Epoch 47/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7231\n",
      "Epoch 47: val_loss improved from 0.79823 to 0.79085, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7181 - val_loss: 0.7909\n",
      "Epoch 48/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6909\n",
      "Epoch 48: val_loss did not improve from 0.79085\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6924 - val_loss: 0.7980\n",
      "Epoch 49/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.7076\n",
      "Epoch 49: val_loss did not improve from 0.79085\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7017 - val_loss: 0.7949\n",
      "Epoch 50/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6907\n",
      "Epoch 50: val_loss did not improve from 0.79085\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6877 - val_loss: 0.7960\n",
      "Epoch 51/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6797\n",
      "Epoch 51: val_loss did not improve from 0.79085\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6798 - val_loss: 0.7917\n",
      "Epoch 52/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.6910\n",
      "Epoch 52: val_loss improved from 0.79085 to 0.78659, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.6829 - val_loss: 0.7866\n",
      "Epoch 53/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.6726\n",
      "Epoch 53: val_loss improved from 0.78659 to 0.78353, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6719 - val_loss: 0.7835\n",
      "Epoch 54/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6613\n",
      "Epoch 54: val_loss did not improve from 0.78353\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6681 - val_loss: 0.7856\n",
      "Epoch 55/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6685\n",
      "Epoch 55: val_loss did not improve from 0.78353\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6723 - val_loss: 0.7836\n",
      "Epoch 56/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.6477\n",
      "Epoch 56: val_loss improved from 0.78353 to 0.78299, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6492 - val_loss: 0.7830\n",
      "Epoch 57/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6436\n",
      "Epoch 57: val_loss did not improve from 0.78299\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6407 - val_loss: 0.7831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.6511\n",
      "Epoch 58: val_loss did not improve from 0.78299\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6494 - val_loss: 0.7835\n",
      "Epoch 59/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.6459\n",
      "Epoch 59: val_loss improved from 0.78299 to 0.77417, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.6477 - val_loss: 0.7742\n",
      "Epoch 60/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6536\n",
      "Epoch 60: val_loss did not improve from 0.77417\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6564 - val_loss: 0.7943\n",
      "Epoch 61/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6532\n",
      "Epoch 61: val_loss improved from 0.77417 to 0.77172, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold2.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6477 - val_loss: 0.7717\n",
      "Epoch 62/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.6319\n",
      "Epoch 62: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6301 - val_loss: 0.7768\n",
      "Epoch 63/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.6496\n",
      "Epoch 63: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6387 - val_loss: 0.7719\n",
      "Epoch 64/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6222\n",
      "Epoch 64: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6225 - val_loss: 0.7849\n",
      "Epoch 65/100\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.6332\n",
      "Epoch 65: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.6325 - val_loss: 0.7802\n",
      "Epoch 66/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.6168\n",
      "Epoch 66: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.6212 - val_loss: 0.7885\n",
      "Epoch 67/100\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.6114\n",
      "Epoch 67: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.6099 - val_loss: 0.7820\n",
      "Epoch 68/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.6297\n",
      "Epoch 68: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6284 - val_loss: 0.7779\n",
      "Epoch 69/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6148\n",
      "Epoch 69: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6164 - val_loss: 0.7784\n",
      "Epoch 70/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6146\n",
      "Epoch 70: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6187 - val_loss: 0.7779\n",
      "Epoch 71/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6142\n",
      "Epoch 71: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6098 - val_loss: 0.7803\n",
      "Epoch 72/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6034\n",
      "Epoch 72: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6058 - val_loss: 0.7828\n",
      "Epoch 73/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.6029\n",
      "Epoch 73: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.6043 - val_loss: 0.7776\n",
      "Epoch 74/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6120\n",
      "Epoch 74: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6093 - val_loss: 0.7792\n",
      "Epoch 75/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5782\n",
      "Epoch 75: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.5826 - val_loss: 0.7802\n",
      "Epoch 76/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.5949\n",
      "Epoch 76: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.5899 - val_loss: 0.7829\n",
      "Epoch 77/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5840\n",
      "Epoch 77: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5881 - val_loss: 0.7875\n",
      "Epoch 78/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5711\n",
      "Epoch 78: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5704 - val_loss: 0.7846\n",
      "Epoch 79/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.5887\n",
      "Epoch 79: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5807 - val_loss: 0.7884\n",
      "Epoch 80/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5981\n",
      "Epoch 80: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5936 - val_loss: 0.7913\n",
      "Epoch 81/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5933\n",
      "Epoch 81: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5918 - val_loss: 0.7726\n",
      "Epoch 82/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5782\n",
      "Epoch 82: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.5821 - val_loss: 0.7757\n",
      "Epoch 83/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5583\n",
      "Epoch 83: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5617 - val_loss: 0.7808\n",
      "Epoch 84/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5836\n",
      "Epoch 84: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5796 - val_loss: 0.7827\n",
      "Epoch 85/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.5625\n",
      "Epoch 85: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.5671 - val_loss: 0.7881\n",
      "Epoch 86/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5629\n",
      "Epoch 86: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5646 - val_loss: 0.7871\n",
      "Epoch 87/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5684\n",
      "Epoch 87: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.5687 - val_loss: 0.7828\n",
      "Epoch 88/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5586\n",
      "Epoch 88: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5590 - val_loss: 0.7804\n",
      "Epoch 89/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5600\n",
      "Epoch 89: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5503 - val_loss: 0.7782\n",
      "Epoch 90/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5482\n",
      "Epoch 90: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5601 - val_loss: 0.7897\n",
      "Epoch 91/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.5467\n",
      "Epoch 91: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5470 - val_loss: 0.7877\n",
      "Epoch 92/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5553\n",
      "Epoch 92: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5542 - val_loss: 0.7866\n",
      "Epoch 93/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5666\n",
      "Epoch 93: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5619 - val_loss: 0.7835\n",
      "Epoch 94/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5302\n",
      "Epoch 94: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5324 - val_loss: 0.7899\n",
      "Epoch 95/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5305\n",
      "Epoch 95: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5316 - val_loss: 0.8040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.5340\n",
      "Epoch 96: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5317 - val_loss: 0.7963\n",
      "Epoch 97/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5572\n",
      "Epoch 97: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5482 - val_loss: 0.7911\n",
      "Epoch 98/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5592\n",
      "Epoch 98: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.5553 - val_loss: 0.7865\n",
      "Epoch 99/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.5163\n",
      "Epoch 99: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5224 - val_loss: 0.7885\n",
      "Epoch 100/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5005\n",
      "Epoch 100: val_loss did not improve from 0.77172\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5112 - val_loss: 0.7952\n",
      "\n",
      "Train/Test model HS_990 on Fold #3.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tf_2_8_py_3_10\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/28 [=====================>........] - ETA: 0s - loss: 1.3235\n",
      "Epoch 1: val_loss improved from inf to 1.27675, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 2s 29ms/step - loss: 1.3189 - val_loss: 1.2767\n",
      "Epoch 2/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 1.2692\n",
      "Epoch 2: val_loss improved from 1.27675 to 1.22969, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.2643 - val_loss: 1.2297\n",
      "Epoch 3/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 1.2259\n",
      "Epoch 3: val_loss improved from 1.22969 to 1.18772, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 1.2233 - val_loss: 1.1877\n",
      "Epoch 4/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.1832\n",
      "Epoch 4: val_loss improved from 1.18772 to 1.15163, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 1.1804 - val_loss: 1.1516\n",
      "Epoch 5/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.1437\n",
      "Epoch 5: val_loss improved from 1.15163 to 1.11899, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.1444 - val_loss: 1.1190\n",
      "Epoch 6/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 1.1116\n",
      "Epoch 6: val_loss improved from 1.11899 to 1.08964, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 1.1119 - val_loss: 1.0896\n",
      "Epoch 7/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.0885\n",
      "Epoch 7: val_loss improved from 1.08964 to 1.06233, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 1.0864 - val_loss: 1.0623\n",
      "Epoch 8/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.0613\n",
      "Epoch 8: val_loss improved from 1.06233 to 1.03947, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 1.0586 - val_loss: 1.0395\n",
      "Epoch 9/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.0377\n",
      "Epoch 9: val_loss improved from 1.03947 to 1.01805, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.0369 - val_loss: 1.0181\n",
      "Epoch 10/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.0187\n",
      "Epoch 10: val_loss improved from 1.01805 to 0.99782, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.0167 - val_loss: 0.9978\n",
      "Epoch 11/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.0014\n",
      "Epoch 11: val_loss improved from 0.99782 to 0.98107, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9988 - val_loss: 0.9811\n",
      "Epoch 12/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.9814\n",
      "Epoch 12: val_loss improved from 0.98107 to 0.96364, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.9800 - val_loss: 0.9636\n",
      "Epoch 13/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9627\n",
      "Epoch 13: val_loss improved from 0.96364 to 0.94595, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.9626 - val_loss: 0.9460\n",
      "Epoch 14/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9535\n",
      "Epoch 14: val_loss improved from 0.94595 to 0.92928, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.9516 - val_loss: 0.9293\n",
      "Epoch 15/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9338\n",
      "Epoch 15: val_loss improved from 0.92928 to 0.92549, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.9345 - val_loss: 0.9255\n",
      "Epoch 16/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.9368\n",
      "Epoch 16: val_loss improved from 0.92549 to 0.90630, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.9379 - val_loss: 0.9063\n",
      "Epoch 17/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.9202\n",
      "Epoch 17: val_loss improved from 0.90630 to 0.89784, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9205 - val_loss: 0.8978\n",
      "Epoch 18/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.8996\n",
      "Epoch 18: val_loss improved from 0.89784 to 0.88393, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9009 - val_loss: 0.8839\n",
      "Epoch 19/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.8902\n",
      "Epoch 19: val_loss improved from 0.88393 to 0.87881, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.8864 - val_loss: 0.8788\n",
      "Epoch 20/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.8834\n",
      "Epoch 20: val_loss improved from 0.87881 to 0.86566, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8853 - val_loss: 0.8657\n",
      "Epoch 21/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.8797\n",
      "Epoch 21: val_loss improved from 0.86566 to 0.85835, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8764 - val_loss: 0.8583\n",
      "Epoch 22/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.8662\n",
      "Epoch 22: val_loss improved from 0.85835 to 0.85150, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8637 - val_loss: 0.8515\n",
      "Epoch 23/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.8547\n",
      "Epoch 23: val_loss improved from 0.85150 to 0.84419, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8559 - val_loss: 0.8442\n",
      "Epoch 24/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.8468\n",
      "Epoch 24: val_loss improved from 0.84419 to 0.83106, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8447 - val_loss: 0.8311\n",
      "Epoch 25/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8473\n",
      "Epoch 25: val_loss improved from 0.83106 to 0.82089, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8428 - val_loss: 0.8209\n",
      "Epoch 26/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.8383\n",
      "Epoch 26: val_loss did not improve from 0.82089\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.8352 - val_loss: 0.8244\n",
      "Epoch 27/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.8176\n",
      "Epoch 27: val_loss improved from 0.82089 to 0.81020, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 11ms/step - loss: 0.8161 - val_loss: 0.8102\n",
      "Epoch 28/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.8170\n",
      "Epoch 28: val_loss improved from 0.81020 to 0.80785, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.8199 - val_loss: 0.8079\n",
      "Epoch 29/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.8093\n",
      "Epoch 29: val_loss improved from 0.80785 to 0.79864, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.8093 - val_loss: 0.7986\n",
      "Epoch 30/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.7948\n",
      "Epoch 30: val_loss improved from 0.79864 to 0.78960, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7959 - val_loss: 0.7896\n",
      "Epoch 31/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.7877\n",
      "Epoch 31: val_loss improved from 0.78960 to 0.78584, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7841 - val_loss: 0.7858\n",
      "Epoch 32/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7998\n",
      "Epoch 32: val_loss improved from 0.78584 to 0.78405, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7987 - val_loss: 0.7840\n",
      "Epoch 33/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.7778\n",
      "Epoch 33: val_loss did not improve from 0.78405\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7827 - val_loss: 0.7930\n",
      "Epoch 34/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.7885\n",
      "Epoch 34: val_loss did not improve from 0.78405\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7907 - val_loss: 0.7888\n",
      "Epoch 35/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.7678\n",
      "Epoch 35: val_loss improved from 0.78405 to 0.77752, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7630 - val_loss: 0.7775\n",
      "Epoch 36/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.7692\n",
      "Epoch 36: val_loss improved from 0.77752 to 0.76868, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7591 - val_loss: 0.7687\n",
      "Epoch 37/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.7458\n",
      "Epoch 37: val_loss improved from 0.76868 to 0.76633, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7455 - val_loss: 0.7663\n",
      "Epoch 38/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.7620\n",
      "Epoch 38: val_loss improved from 0.76633 to 0.76464, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7606 - val_loss: 0.7646\n",
      "Epoch 39/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7732\n",
      "Epoch 39: val_loss improved from 0.76464 to 0.75605, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7666 - val_loss: 0.7560\n",
      "Epoch 40/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7545\n",
      "Epoch 40: val_loss did not improve from 0.75605\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7518 - val_loss: 0.7696\n",
      "Epoch 41/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7331\n",
      "Epoch 41: val_loss improved from 0.75605 to 0.75480, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7311 - val_loss: 0.7548\n",
      "Epoch 42/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.7328\n",
      "Epoch 42: val_loss did not improve from 0.75480\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.7299 - val_loss: 0.7636\n",
      "Epoch 43/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7183\n",
      "Epoch 43: val_loss improved from 0.75480 to 0.75356, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.7203 - val_loss: 0.7536\n",
      "Epoch 44/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7201\n",
      "Epoch 44: val_loss improved from 0.75356 to 0.74704, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7202 - val_loss: 0.7470\n",
      "Epoch 45/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7090\n",
      "Epoch 45: val_loss did not improve from 0.74704\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7060 - val_loss: 0.7520\n",
      "Epoch 46/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.7091\n",
      "Epoch 46: val_loss did not improve from 0.74704\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7071 - val_loss: 0.7473\n",
      "Epoch 47/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7104\n",
      "Epoch 47: val_loss improved from 0.74704 to 0.74574, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.7178 - val_loss: 0.7457\n",
      "Epoch 48/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6994\n",
      "Epoch 48: val_loss improved from 0.74574 to 0.74113, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6979 - val_loss: 0.7411\n",
      "Epoch 49/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7098\n",
      "Epoch 49: val_loss did not improve from 0.74113\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7080 - val_loss: 0.7442\n",
      "Epoch 50/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7030\n",
      "Epoch 50: val_loss did not improve from 0.74113\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7001 - val_loss: 0.7441\n",
      "Epoch 51/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6882\n",
      "Epoch 51: val_loss improved from 0.74113 to 0.73917, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6838 - val_loss: 0.7392\n",
      "Epoch 52/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6913\n",
      "Epoch 52: val_loss improved from 0.73917 to 0.73046, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6900 - val_loss: 0.7305\n",
      "Epoch 53/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.6789\n",
      "Epoch 53: val_loss did not improve from 0.73046\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6802 - val_loss: 0.7386\n",
      "Epoch 54/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.6950\n",
      "Epoch 54: val_loss did not improve from 0.73046\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6917 - val_loss: 0.7450\n",
      "Epoch 55/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6778\n",
      "Epoch 55: val_loss did not improve from 0.73046\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6716 - val_loss: 0.7420\n",
      "Epoch 56/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6679\n",
      "Epoch 56: val_loss did not improve from 0.73046\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6693 - val_loss: 0.7385\n",
      "Epoch 57/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6727\n",
      "Epoch 57: val_loss did not improve from 0.73046\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6708 - val_loss: 0.7337\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6534\n",
      "Epoch 58: val_loss did not improve from 0.73046\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6509 - val_loss: 0.7458\n",
      "Epoch 59/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.6630\n",
      "Epoch 59: val_loss did not improve from 0.73046\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6633 - val_loss: 0.7339\n",
      "Epoch 60/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6403\n",
      "Epoch 60: val_loss did not improve from 0.73046\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6382 - val_loss: 0.7344\n",
      "Epoch 61/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.6671\n",
      "Epoch 61: val_loss did not improve from 0.73046\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6586 - val_loss: 0.7306\n",
      "Epoch 62/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6839\n",
      "Epoch 62: val_loss did not improve from 0.73046\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6814 - val_loss: 0.7308\n",
      "Epoch 63/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.6472\n",
      "Epoch 63: val_loss improved from 0.73046 to 0.72581, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold3.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6542 - val_loss: 0.7258\n",
      "Epoch 64/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.6360\n",
      "Epoch 64: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6448 - val_loss: 0.7412\n",
      "Epoch 65/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.6548\n",
      "Epoch 65: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6521 - val_loss: 0.7350\n",
      "Epoch 66/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.6462\n",
      "Epoch 66: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.6442 - val_loss: 0.7340\n",
      "Epoch 67/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6334\n",
      "Epoch 67: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6244 - val_loss: 0.7379\n",
      "Epoch 68/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.6180\n",
      "Epoch 68: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6174 - val_loss: 0.7407\n",
      "Epoch 69/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.6343\n",
      "Epoch 69: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6293 - val_loss: 0.7374\n",
      "Epoch 70/100\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.6362\n",
      "Epoch 70: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.6341 - val_loss: 0.7332\n",
      "Epoch 71/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6339\n",
      "Epoch 71: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6280 - val_loss: 0.7330\n",
      "Epoch 72/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.6297\n",
      "Epoch 72: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6329 - val_loss: 0.7330\n",
      "Epoch 73/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6021\n",
      "Epoch 73: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6021 - val_loss: 0.7269\n",
      "Epoch 74/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6184\n",
      "Epoch 74: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6176 - val_loss: 0.7372\n",
      "Epoch 75/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5965\n",
      "Epoch 75: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5925 - val_loss: 0.7447\n",
      "Epoch 76/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6278\n",
      "Epoch 76: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6182 - val_loss: 0.7366\n",
      "Epoch 77/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6037\n",
      "Epoch 77: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6039 - val_loss: 0.7316\n",
      "Epoch 78/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.6059\n",
      "Epoch 78: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6065 - val_loss: 0.7327\n",
      "Epoch 79/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5975\n",
      "Epoch 79: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5971 - val_loss: 0.7344\n",
      "Epoch 80/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5789\n",
      "Epoch 80: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5746 - val_loss: 0.7343\n",
      "Epoch 81/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5920\n",
      "Epoch 81: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5946 - val_loss: 0.7490\n",
      "Epoch 82/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5700\n",
      "Epoch 82: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5654 - val_loss: 0.7527\n",
      "Epoch 83/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5774\n",
      "Epoch 83: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5821 - val_loss: 0.7507\n",
      "Epoch 84/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5764\n",
      "Epoch 84: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5744 - val_loss: 0.7531\n",
      "Epoch 85/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5791\n",
      "Epoch 85: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5831 - val_loss: 0.7420\n",
      "Epoch 86/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5616\n",
      "Epoch 86: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5518 - val_loss: 0.7513\n",
      "Epoch 87/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5595\n",
      "Epoch 87: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5591 - val_loss: 0.7573\n",
      "Epoch 88/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5591\n",
      "Epoch 88: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5526 - val_loss: 0.7699\n",
      "Epoch 89/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5756\n",
      "Epoch 89: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5748 - val_loss: 0.7583\n",
      "Epoch 90/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5697\n",
      "Epoch 90: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5740 - val_loss: 0.7599\n",
      "Epoch 91/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5878\n",
      "Epoch 91: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.5879 - val_loss: 0.7479\n",
      "Epoch 92/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5477\n",
      "Epoch 92: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5561 - val_loss: 0.7508\n",
      "Epoch 93/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5608\n",
      "Epoch 93: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5581 - val_loss: 0.7569\n",
      "Epoch 94/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5470\n",
      "Epoch 94: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5506 - val_loss: 0.7607\n",
      "Epoch 95/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5466\n",
      "Epoch 95: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5514 - val_loss: 0.7529\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5656\n",
      "Epoch 96: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5591 - val_loss: 0.7437\n",
      "Epoch 97/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5475\n",
      "Epoch 97: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5561 - val_loss: 0.7425\n",
      "Epoch 98/100\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.5482\n",
      "Epoch 98: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.5503 - val_loss: 0.7504\n",
      "Epoch 99/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5390\n",
      "Epoch 99: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5542 - val_loss: 0.7691\n",
      "Epoch 100/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5462\n",
      "Epoch 100: val_loss did not improve from 0.72581\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5466 - val_loss: 0.7590\n",
      "\n",
      "Train/Test model HS_990 on Fold #4.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tf_2_8_py_3_10\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/28 [========================>.....] - ETA: 0s - loss: 1.3201\n",
      "Epoch 1: val_loss improved from inf to 1.27829, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 2s 34ms/step - loss: 1.3179 - val_loss: 1.2783\n",
      "Epoch 2/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 1.2562\n",
      "Epoch 2: val_loss improved from 1.27829 to 1.22893, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.2542 - val_loss: 1.2289\n",
      "Epoch 3/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.2118\n",
      "Epoch 3: val_loss improved from 1.22893 to 1.18713, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 1.2084 - val_loss: 1.1871\n",
      "Epoch 4/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 1.1706\n",
      "Epoch 4: val_loss improved from 1.18713 to 1.14696, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.1672 - val_loss: 1.1470\n",
      "Epoch 5/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.1330\n",
      "Epoch 5: val_loss improved from 1.14696 to 1.11472, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.1329 - val_loss: 1.1147\n",
      "Epoch 6/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.1053\n",
      "Epoch 6: val_loss improved from 1.11472 to 1.08897, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.1017 - val_loss: 1.0890\n",
      "Epoch 7/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.0773\n",
      "Epoch 7: val_loss improved from 1.08897 to 1.05985, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.0765 - val_loss: 1.0599\n",
      "Epoch 8/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 1.0553\n",
      "Epoch 8: val_loss improved from 1.05985 to 1.03477, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.0550 - val_loss: 1.0348\n",
      "Epoch 9/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 1.0260\n",
      "Epoch 9: val_loss improved from 1.03477 to 1.01549, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.0270 - val_loss: 1.0155\n",
      "Epoch 10/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.0082\n",
      "Epoch 10: val_loss improved from 1.01549 to 0.99143, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.0069 - val_loss: 0.9914\n",
      "Epoch 11/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.9934\n",
      "Epoch 11: val_loss improved from 0.99143 to 0.97499, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9928 - val_loss: 0.9750\n",
      "Epoch 12/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.9696\n",
      "Epoch 12: val_loss improved from 0.97499 to 0.95764, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9697 - val_loss: 0.9576\n",
      "Epoch 13/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.9603\n",
      "Epoch 13: val_loss improved from 0.95764 to 0.94201, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9592 - val_loss: 0.9420\n",
      "Epoch 14/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9509\n",
      "Epoch 14: val_loss improved from 0.94201 to 0.92995, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9531 - val_loss: 0.9299\n",
      "Epoch 15/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9261\n",
      "Epoch 15: val_loss improved from 0.92995 to 0.91526, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9263 - val_loss: 0.9153\n",
      "Epoch 16/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9119\n",
      "Epoch 16: val_loss improved from 0.91526 to 0.90285, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9087 - val_loss: 0.9029\n",
      "Epoch 17/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8969\n",
      "Epoch 17: val_loss improved from 0.90285 to 0.88908, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8993 - val_loss: 0.8891\n",
      "Epoch 18/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8886\n",
      "Epoch 18: val_loss improved from 0.88908 to 0.88079, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8921 - val_loss: 0.8808\n",
      "Epoch 19/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.8772\n",
      "Epoch 19: val_loss improved from 0.88079 to 0.87389, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8744 - val_loss: 0.8739\n",
      "Epoch 20/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8699\n",
      "Epoch 20: val_loss improved from 0.87389 to 0.86176, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8717 - val_loss: 0.8618\n",
      "Epoch 21/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8634\n",
      "Epoch 21: val_loss improved from 0.86176 to 0.84932, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8656 - val_loss: 0.8493\n",
      "Epoch 22/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8545\n",
      "Epoch 22: val_loss improved from 0.84932 to 0.84321, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8554 - val_loss: 0.8432\n",
      "Epoch 23/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8446\n",
      "Epoch 23: val_loss improved from 0.84321 to 0.83373, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8434 - val_loss: 0.8337\n",
      "Epoch 24/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8305\n",
      "Epoch 24: val_loss improved from 0.83373 to 0.82583, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8319 - val_loss: 0.8258\n",
      "Epoch 25/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8228\n",
      "Epoch 25: val_loss did not improve from 0.82583\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.8252 - val_loss: 0.8264\n",
      "Epoch 26/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8172\n",
      "Epoch 26: val_loss improved from 0.82583 to 0.82504, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.8176 - val_loss: 0.8250\n",
      "Epoch 27/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8034\n",
      "Epoch 27: val_loss improved from 0.82504 to 0.80803, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8074 - val_loss: 0.8080\n",
      "Epoch 28/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7981\n",
      "Epoch 28: val_loss did not improve from 0.80803\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7974 - val_loss: 0.8117\n",
      "Epoch 29/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.7905\n",
      "Epoch 29: val_loss improved from 0.80803 to 0.79616, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7878 - val_loss: 0.7962\n",
      "Epoch 30/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7944\n",
      "Epoch 30: val_loss improved from 0.79616 to 0.79418, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7930 - val_loss: 0.7942\n",
      "Epoch 31/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7692\n",
      "Epoch 31: val_loss did not improve from 0.79418\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7721 - val_loss: 0.7974\n",
      "Epoch 32/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.7671\n",
      "Epoch 32: val_loss improved from 0.79418 to 0.78838, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7718 - val_loss: 0.7884\n",
      "Epoch 33/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.7711\n",
      "Epoch 33: val_loss improved from 0.78838 to 0.78418, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7702 - val_loss: 0.7842\n",
      "Epoch 34/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7483\n",
      "Epoch 34: val_loss improved from 0.78418 to 0.78119, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.7559 - val_loss: 0.7812\n",
      "Epoch 35/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7592\n",
      "Epoch 35: val_loss did not improve from 0.78119\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7623 - val_loss: 0.7899\n",
      "Epoch 36/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.7359\n",
      "Epoch 36: val_loss improved from 0.78119 to 0.77617, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.7426 - val_loss: 0.7762\n",
      "Epoch 37/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.7486\n",
      "Epoch 37: val_loss improved from 0.77617 to 0.76790, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.7429 - val_loss: 0.7679\n",
      "Epoch 38/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7207\n",
      "Epoch 38: val_loss improved from 0.76790 to 0.76663, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7227 - val_loss: 0.7666\n",
      "Epoch 39/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.7271\n",
      "Epoch 39: val_loss did not improve from 0.76663\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7232 - val_loss: 0.7688\n",
      "Epoch 40/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7320\n",
      "Epoch 40: val_loss improved from 0.76663 to 0.75988, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7265 - val_loss: 0.7599\n",
      "Epoch 41/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.7163\n",
      "Epoch 41: val_loss did not improve from 0.75988\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7233 - val_loss: 0.7646\n",
      "Epoch 42/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.7052\n",
      "Epoch 42: val_loss did not improve from 0.75988\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7022 - val_loss: 0.7616\n",
      "Epoch 43/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6987\n",
      "Epoch 43: val_loss improved from 0.75988 to 0.75778, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7038 - val_loss: 0.7578\n",
      "Epoch 44/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6891\n",
      "Epoch 44: val_loss improved from 0.75778 to 0.75678, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.6888 - val_loss: 0.7568\n",
      "Epoch 45/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.7194\n",
      "Epoch 45: val_loss did not improve from 0.75678\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7157 - val_loss: 0.7612\n",
      "Epoch 46/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7064\n",
      "Epoch 46: val_loss improved from 0.75678 to 0.74813, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.7086 - val_loss: 0.7481\n",
      "Epoch 47/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6692\n",
      "Epoch 47: val_loss did not improve from 0.74813\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6706 - val_loss: 0.7509\n",
      "Epoch 48/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6683\n",
      "Epoch 48: val_loss did not improve from 0.74813\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6667 - val_loss: 0.7560\n",
      "Epoch 49/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6893\n",
      "Epoch 49: val_loss did not improve from 0.74813\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6854 - val_loss: 0.7494\n",
      "Epoch 50/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6791\n",
      "Epoch 50: val_loss improved from 0.74813 to 0.74627, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6811 - val_loss: 0.7463\n",
      "Epoch 51/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6561\n",
      "Epoch 51: val_loss did not improve from 0.74627\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6586 - val_loss: 0.7522\n",
      "Epoch 52/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.6723\n",
      "Epoch 52: val_loss did not improve from 0.74627\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6747 - val_loss: 0.7490\n",
      "Epoch 53/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6742\n",
      "Epoch 53: val_loss did not improve from 0.74627\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6675 - val_loss: 0.7678\n",
      "Epoch 54/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.6521\n",
      "Epoch 54: val_loss did not improve from 0.74627\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6565 - val_loss: 0.7484\n",
      "Epoch 55/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6713\n",
      "Epoch 55: val_loss improved from 0.74627 to 0.74402, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6735 - val_loss: 0.7440\n",
      "Epoch 56/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6656\n",
      "Epoch 56: val_loss did not improve from 0.74402\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6668 - val_loss: 0.7491\n",
      "Epoch 57/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6363\n",
      "Epoch 57: val_loss did not improve from 0.74402\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6448 - val_loss: 0.7443\n",
      "Epoch 58/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6425\n",
      "Epoch 58: val_loss did not improve from 0.74402\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6375 - val_loss: 0.7489\n",
      "Epoch 59/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.6608\n",
      "Epoch 59: val_loss did not improve from 0.74402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6643 - val_loss: 0.7729\n",
      "Epoch 60/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6484\n",
      "Epoch 60: val_loss did not improve from 0.74402\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6462 - val_loss: 0.7516\n",
      "Epoch 61/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6189\n",
      "Epoch 61: val_loss did not improve from 0.74402\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6274 - val_loss: 0.7466\n",
      "Epoch 62/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6474\n",
      "Epoch 62: val_loss improved from 0.74402 to 0.74284, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6396 - val_loss: 0.7428\n",
      "Epoch 63/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.6280\n",
      "Epoch 63: val_loss did not improve from 0.74284\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6302 - val_loss: 0.7432\n",
      "Epoch 64/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6353\n",
      "Epoch 64: val_loss improved from 0.74284 to 0.73406, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6371 - val_loss: 0.7341\n",
      "Epoch 65/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6047\n",
      "Epoch 65: val_loss did not improve from 0.73406\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6113 - val_loss: 0.7428\n",
      "Epoch 66/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6225\n",
      "Epoch 66: val_loss improved from 0.73406 to 0.73370, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6227 - val_loss: 0.7337\n",
      "Epoch 67/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6043\n",
      "Epoch 67: val_loss did not improve from 0.73370\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6123 - val_loss: 0.7354\n",
      "Epoch 68/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6206\n",
      "Epoch 68: val_loss did not improve from 0.73370\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6217 - val_loss: 0.7371\n",
      "Epoch 69/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6282\n",
      "Epoch 69: val_loss did not improve from 0.73370\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6285 - val_loss: 0.7426\n",
      "Epoch 70/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5822\n",
      "Epoch 70: val_loss did not improve from 0.73370\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5912 - val_loss: 0.7440\n",
      "Epoch 71/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6116\n",
      "Epoch 71: val_loss did not improve from 0.73370\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6168 - val_loss: 0.7374\n",
      "Epoch 72/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6119\n",
      "Epoch 72: val_loss improved from 0.73370 to 0.73264, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6067 - val_loss: 0.7326\n",
      "Epoch 73/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5945\n",
      "Epoch 73: val_loss did not improve from 0.73264\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5988 - val_loss: 0.7353\n",
      "Epoch 74/100\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.6040\n",
      "Epoch 74: val_loss improved from 0.73264 to 0.72940, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold4.hdf5\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.6046 - val_loss: 0.7294\n",
      "Epoch 75/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5863\n",
      "Epoch 75: val_loss did not improve from 0.72940\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5827 - val_loss: 0.7315\n",
      "Epoch 76/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5903\n",
      "Epoch 76: val_loss did not improve from 0.72940\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5917 - val_loss: 0.7362\n",
      "Epoch 77/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5840\n",
      "Epoch 77: val_loss did not improve from 0.72940\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5828 - val_loss: 0.7377\n",
      "Epoch 78/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5825\n",
      "Epoch 78: val_loss did not improve from 0.72940\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5851 - val_loss: 0.7402\n",
      "Epoch 79/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5711\n",
      "Epoch 79: val_loss did not improve from 0.72940\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5708 - val_loss: 0.7375\n",
      "Epoch 80/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.5880\n",
      "Epoch 80: val_loss did not improve from 0.72940\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.5807 - val_loss: 0.7366\n",
      "Epoch 81/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5772\n",
      "Epoch 81: val_loss did not improve from 0.72940\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5838 - val_loss: 0.7308\n",
      "Epoch 82/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5712\n",
      "Epoch 82: val_loss did not improve from 0.72940\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5740 - val_loss: 0.7350\n",
      "Epoch 83/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5514\n",
      "Epoch 83: val_loss did not improve from 0.72940\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5484 - val_loss: 0.7474\n",
      "Epoch 84/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5726\n",
      "Epoch 84: val_loss did not improve from 0.72940\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5711 - val_loss: 0.7446\n",
      "Epoch 85/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5824\n",
      "Epoch 85: val_loss did not improve from 0.72940\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5787 - val_loss: 0.7531\n",
      "Epoch 86/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5600\n",
      "Epoch 86: val_loss did not improve from 0.72940\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5560 - val_loss: 0.7519\n",
      "Epoch 87/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5593\n",
      "Epoch 87: val_loss did not improve from 0.72940\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5622 - val_loss: 0.7563\n",
      "Epoch 88/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.5612\n",
      "Epoch 88: val_loss did not improve from 0.72940\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.5703 - val_loss: 0.7545\n",
      "Epoch 89/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5733\n",
      "Epoch 89: val_loss did not improve from 0.72940\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5723 - val_loss: 0.7551\n",
      "Epoch 90/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5633\n",
      "Epoch 90: val_loss did not improve from 0.72940\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5586 - val_loss: 0.7566\n",
      "Epoch 91/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5255\n",
      "Epoch 91: val_loss did not improve from 0.72940\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5230 - val_loss: 0.7497\n",
      "Epoch 92/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5256\n",
      "Epoch 92: val_loss did not improve from 0.72940\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5268 - val_loss: 0.7631\n",
      "Epoch 93/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5661\n",
      "Epoch 93: val_loss did not improve from 0.72940\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5586 - val_loss: 0.7585\n",
      "Epoch 94/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.5430\n",
      "Epoch 94: val_loss did not improve from 0.72940\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.5453 - val_loss: 0.7691\n",
      "Epoch 95/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.5431\n",
      "Epoch 95: val_loss did not improve from 0.72940\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5327 - val_loss: 0.7624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.5207\n",
      "Epoch 96: val_loss did not improve from 0.72940\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5267 - val_loss: 0.7768\n",
      "Epoch 97/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.5380\n",
      "Epoch 97: val_loss did not improve from 0.72940\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.5429 - val_loss: 0.7719\n",
      "Epoch 98/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.5256\n",
      "Epoch 98: val_loss did not improve from 0.72940\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5267 - val_loss: 0.7836\n",
      "Epoch 99/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5254\n",
      "Epoch 99: val_loss did not improve from 0.72940\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5267 - val_loss: 0.7830\n",
      "Epoch 100/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5166\n",
      "Epoch 100: val_loss did not improve from 0.72940\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5186 - val_loss: 0.7735\n",
      "\n",
      "Train/Test model HS_990 on Fold #5.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tf_2_8_py_3_10\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/28 [=========================>....] - ETA: 0s - loss: 1.3152\n",
      "Epoch 1: val_loss improved from inf to 1.27780, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 2s 25ms/step - loss: 1.3137 - val_loss: 1.2778\n",
      "Epoch 2/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.2562\n",
      "Epoch 2: val_loss improved from 1.27780 to 1.23421, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.2547 - val_loss: 1.2342\n",
      "Epoch 3/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 1.2146\n",
      "Epoch 3: val_loss improved from 1.23421 to 1.19452, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.2131 - val_loss: 1.1945\n",
      "Epoch 4/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 1.1765\n",
      "Epoch 4: val_loss improved from 1.19452 to 1.15820, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.1742 - val_loss: 1.1582\n",
      "Epoch 5/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.1407\n",
      "Epoch 5: val_loss improved from 1.15820 to 1.12644, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 1.1386 - val_loss: 1.1264\n",
      "Epoch 6/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.1095\n",
      "Epoch 6: val_loss improved from 1.12644 to 1.10012, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.1097 - val_loss: 1.1001\n",
      "Epoch 7/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.0921\n",
      "Epoch 7: val_loss improved from 1.10012 to 1.07527, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.0893 - val_loss: 1.0753\n",
      "Epoch 8/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.0537\n",
      "Epoch 8: val_loss improved from 1.07527 to 1.05154, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 1.0557 - val_loss: 1.0515\n",
      "Epoch 9/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.0356\n",
      "Epoch 9: val_loss improved from 1.05154 to 1.03082, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 1.0324 - val_loss: 1.0308\n",
      "Epoch 10/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.0209\n",
      "Epoch 10: val_loss improved from 1.03082 to 1.01234, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.0186 - val_loss: 1.0123\n",
      "Epoch 11/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9954\n",
      "Epoch 11: val_loss improved from 1.01234 to 0.99549, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.9948 - val_loss: 0.9955\n",
      "Epoch 12/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9814\n",
      "Epoch 12: val_loss improved from 0.99549 to 0.98020, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9800 - val_loss: 0.9802\n",
      "Epoch 13/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.9609\n",
      "Epoch 13: val_loss improved from 0.98020 to 0.96449, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9588 - val_loss: 0.9645\n",
      "Epoch 14/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9423\n",
      "Epoch 14: val_loss improved from 0.96449 to 0.95145, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9466 - val_loss: 0.9515\n",
      "Epoch 15/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.9328\n",
      "Epoch 15: val_loss improved from 0.95145 to 0.93844, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9343 - val_loss: 0.9384\n",
      "Epoch 16/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9202\n",
      "Epoch 16: val_loss improved from 0.93844 to 0.92883, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9202 - val_loss: 0.9288\n",
      "Epoch 17/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9087\n",
      "Epoch 17: val_loss improved from 0.92883 to 0.91765, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.9058 - val_loss: 0.9177\n",
      "Epoch 18/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9021\n",
      "Epoch 18: val_loss improved from 0.91765 to 0.90987, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8994 - val_loss: 0.9099\n",
      "Epoch 19/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8940\n",
      "Epoch 19: val_loss improved from 0.90987 to 0.89772, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8911 - val_loss: 0.8977\n",
      "Epoch 20/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8752\n",
      "Epoch 20: val_loss improved from 0.89772 to 0.88807, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8726 - val_loss: 0.8881\n",
      "Epoch 21/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.8667\n",
      "Epoch 21: val_loss improved from 0.88807 to 0.87988, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8641 - val_loss: 0.8799\n",
      "Epoch 22/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8590\n",
      "Epoch 22: val_loss improved from 0.87988 to 0.87559, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8584 - val_loss: 0.8756\n",
      "Epoch 23/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8589\n",
      "Epoch 23: val_loss improved from 0.87559 to 0.86706, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.8539 - val_loss: 0.8671\n",
      "Epoch 24/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.8351\n",
      "Epoch 24: val_loss improved from 0.86706 to 0.86000, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8359 - val_loss: 0.8600\n",
      "Epoch 25/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8226\n",
      "Epoch 25: val_loss improved from 0.86000 to 0.85333, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8222 - val_loss: 0.8533\n",
      "Epoch 26/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8169\n",
      "Epoch 26: val_loss improved from 0.85333 to 0.84678, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8247 - val_loss: 0.8468\n",
      "Epoch 27/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.8115\n",
      "Epoch 27: val_loss improved from 0.84678 to 0.83658, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8088 - val_loss: 0.8366\n",
      "Epoch 28/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.8152\n",
      "Epoch 28: val_loss improved from 0.83658 to 0.83120, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8174 - val_loss: 0.8312\n",
      "Epoch 29/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.8096\n",
      "Epoch 29: val_loss improved from 0.83120 to 0.82332, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.8114 - val_loss: 0.8233\n",
      "Epoch 30/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.7950\n",
      "Epoch 30: val_loss did not improve from 0.82332\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7961 - val_loss: 0.8245\n",
      "Epoch 31/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7927\n",
      "Epoch 31: val_loss improved from 0.82332 to 0.81865, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7916 - val_loss: 0.8187\n",
      "Epoch 32/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.7903\n",
      "Epoch 32: val_loss improved from 0.81865 to 0.81290, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.7906 - val_loss: 0.8129\n",
      "Epoch 33/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7775\n",
      "Epoch 33: val_loss improved from 0.81290 to 0.81286, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7786 - val_loss: 0.8129\n",
      "Epoch 34/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.7661\n",
      "Epoch 34: val_loss improved from 0.81286 to 0.80831, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7685 - val_loss: 0.8083\n",
      "Epoch 35/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7622\n",
      "Epoch 35: val_loss improved from 0.80831 to 0.80500, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.7603 - val_loss: 0.8050\n",
      "Epoch 36/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7447\n",
      "Epoch 36: val_loss improved from 0.80500 to 0.80165, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7428 - val_loss: 0.8017\n",
      "Epoch 37/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7640\n",
      "Epoch 37: val_loss improved from 0.80165 to 0.79324, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7603 - val_loss: 0.7932\n",
      "Epoch 38/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7391\n",
      "Epoch 38: val_loss improved from 0.79324 to 0.79034, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.7453 - val_loss: 0.7903\n",
      "Epoch 39/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.7372\n",
      "Epoch 39: val_loss did not improve from 0.79034\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7383 - val_loss: 0.7927\n",
      "Epoch 40/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7289\n",
      "Epoch 40: val_loss improved from 0.79034 to 0.78775, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7302 - val_loss: 0.7877\n",
      "Epoch 41/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7298\n",
      "Epoch 41: val_loss improved from 0.78775 to 0.78689, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7306 - val_loss: 0.7869\n",
      "Epoch 42/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7437\n",
      "Epoch 42: val_loss improved from 0.78689 to 0.78283, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7441 - val_loss: 0.7828\n",
      "Epoch 43/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.7332\n",
      "Epoch 43: val_loss improved from 0.78283 to 0.77693, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7326 - val_loss: 0.7769\n",
      "Epoch 44/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7237\n",
      "Epoch 44: val_loss did not improve from 0.77693\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7263 - val_loss: 0.7779\n",
      "Epoch 45/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7121\n",
      "Epoch 45: val_loss improved from 0.77693 to 0.77427, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7112 - val_loss: 0.7743\n",
      "Epoch 46/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.7072\n",
      "Epoch 46: val_loss improved from 0.77427 to 0.77082, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.7147 - val_loss: 0.7708\n",
      "Epoch 47/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.7006\n",
      "Epoch 47: val_loss improved from 0.77082 to 0.76928, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7045 - val_loss: 0.7693\n",
      "Epoch 48/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.7177\n",
      "Epoch 48: val_loss improved from 0.76928 to 0.76581, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.7165 - val_loss: 0.7658\n",
      "Epoch 49/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6976\n",
      "Epoch 49: val_loss improved from 0.76581 to 0.76486, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6902 - val_loss: 0.7649\n",
      "Epoch 50/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6836\n",
      "Epoch 50: val_loss improved from 0.76486 to 0.76241, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6857 - val_loss: 0.7624\n",
      "Epoch 51/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6793\n",
      "Epoch 51: val_loss improved from 0.76241 to 0.76103, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.6788 - val_loss: 0.7610\n",
      "Epoch 52/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6912\n",
      "Epoch 52: val_loss improved from 0.76103 to 0.75829, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.6929 - val_loss: 0.7583\n",
      "Epoch 53/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.6801\n",
      "Epoch 53: val_loss did not improve from 0.75829\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.6896 - val_loss: 0.7615\n",
      "Epoch 54/100\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.6789\n",
      "Epoch 54: val_loss did not improve from 0.75829\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.6771 - val_loss: 0.7589\n",
      "Epoch 55/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6639\n",
      "Epoch 55: val_loss improved from 0.75829 to 0.74893, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6616 - val_loss: 0.7489\n",
      "Epoch 56/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6609\n",
      "Epoch 56: val_loss did not improve from 0.74893\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6667 - val_loss: 0.7577\n",
      "Epoch 57/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6627\n",
      "Epoch 57: val_loss did not improve from 0.74893\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6572 - val_loss: 0.7535\n",
      "Epoch 58/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6536\n",
      "Epoch 58: val_loss did not improve from 0.74893\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6572 - val_loss: 0.7616\n",
      "Epoch 59/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.6374\n",
      "Epoch 59: val_loss did not improve from 0.74893\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6448 - val_loss: 0.7541\n",
      "Epoch 60/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6455\n",
      "Epoch 60: val_loss did not improve from 0.74893\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6454 - val_loss: 0.7595\n",
      "Epoch 61/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.6460\n",
      "Epoch 61: val_loss did not improve from 0.74893\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6407 - val_loss: 0.7576\n",
      "Epoch 62/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6279\n",
      "Epoch 62: val_loss did not improve from 0.74893\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6281 - val_loss: 0.7548\n",
      "Epoch 63/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6438\n",
      "Epoch 63: val_loss did not improve from 0.74893\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6527 - val_loss: 0.7490\n",
      "Epoch 64/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.6388\n",
      "Epoch 64: val_loss did not improve from 0.74893\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6395 - val_loss: 0.7532\n",
      "Epoch 65/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6302\n",
      "Epoch 65: val_loss did not improve from 0.74893\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6270 - val_loss: 0.7601\n",
      "Epoch 66/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6197\n",
      "Epoch 66: val_loss did not improve from 0.74893\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6290 - val_loss: 0.7517\n",
      "Epoch 67/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6460\n",
      "Epoch 67: val_loss improved from 0.74893 to 0.74763, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold5.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.6402 - val_loss: 0.7476\n",
      "Epoch 68/100\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.6284\n",
      "Epoch 68: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6278 - val_loss: 0.7517\n",
      "Epoch 69/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6110\n",
      "Epoch 69: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6197 - val_loss: 0.7555\n",
      "Epoch 70/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6038\n",
      "Epoch 70: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6090 - val_loss: 0.7544\n",
      "Epoch 71/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6194\n",
      "Epoch 71: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6145 - val_loss: 0.7536\n",
      "Epoch 72/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5988\n",
      "Epoch 72: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6029 - val_loss: 0.7579\n",
      "Epoch 73/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.6015\n",
      "Epoch 73: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6023 - val_loss: 0.7513\n",
      "Epoch 74/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5949\n",
      "Epoch 74: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6068 - val_loss: 0.7590\n",
      "Epoch 75/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5973\n",
      "Epoch 75: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5966 - val_loss: 0.7572\n",
      "Epoch 76/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6280\n",
      "Epoch 76: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6219 - val_loss: 0.7528\n",
      "Epoch 77/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5888\n",
      "Epoch 77: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5854 - val_loss: 0.7487\n",
      "Epoch 78/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5972\n",
      "Epoch 78: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5934 - val_loss: 0.7517\n",
      "Epoch 79/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5765\n",
      "Epoch 79: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5793 - val_loss: 0.7554\n",
      "Epoch 80/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6023\n",
      "Epoch 80: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6054 - val_loss: 0.7616\n",
      "Epoch 81/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6033\n",
      "Epoch 81: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6002 - val_loss: 0.7728\n",
      "Epoch 82/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5694\n",
      "Epoch 82: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5725 - val_loss: 0.7592\n",
      "Epoch 83/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6051\n",
      "Epoch 83: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6003 - val_loss: 0.7591\n",
      "Epoch 84/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5626\n",
      "Epoch 84: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5613 - val_loss: 0.7653\n",
      "Epoch 85/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5650\n",
      "Epoch 85: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5704 - val_loss: 0.7729\n",
      "Epoch 86/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6021\n",
      "Epoch 86: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5997 - val_loss: 0.7750\n",
      "Epoch 87/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5680\n",
      "Epoch 87: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5678 - val_loss: 0.7799\n",
      "Epoch 88/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5556\n",
      "Epoch 88: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5605 - val_loss: 0.7824\n",
      "Epoch 89/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5713\n",
      "Epoch 89: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5694 - val_loss: 0.7720\n",
      "Epoch 90/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5717\n",
      "Epoch 90: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5831 - val_loss: 0.7648\n",
      "Epoch 91/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5598\n",
      "Epoch 91: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5655 - val_loss: 0.7639\n",
      "Epoch 92/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5492\n",
      "Epoch 92: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5433 - val_loss: 0.7697\n",
      "Epoch 93/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5340\n",
      "Epoch 93: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5344 - val_loss: 0.7715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5521\n",
      "Epoch 94: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5505 - val_loss: 0.7709\n",
      "Epoch 95/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5374\n",
      "Epoch 95: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5307 - val_loss: 0.7778\n",
      "Epoch 96/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5301\n",
      "Epoch 96: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5372 - val_loss: 0.7830\n",
      "Epoch 97/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5498\n",
      "Epoch 97: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5545 - val_loss: 0.7758\n",
      "Epoch 98/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5143\n",
      "Epoch 98: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5151 - val_loss: 0.7834\n",
      "Epoch 99/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.5635\n",
      "Epoch 99: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5631 - val_loss: 0.7741\n",
      "Epoch 100/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5388\n",
      "Epoch 100: val_loss did not improve from 0.74763\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5437 - val_loss: 0.7747\n",
      "\n",
      "Train/Test model HS_990 on Fold #6.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tf_2_8_py_3_10\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/28 [=======================>......] - ETA: 0s - loss: 1.3206\n",
      "Epoch 1: val_loss improved from inf to 1.26808, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold6.hdf5\n",
      "28/28 [==============================] - 2s 26ms/step - loss: 1.3117 - val_loss: 1.2681\n",
      "Epoch 2/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 1.2524\n",
      "Epoch 2: val_loss improved from 1.26808 to 1.21614, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold6.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.2490 - val_loss: 1.2161\n",
      "Epoch 3/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.2042\n",
      "Epoch 3: val_loss improved from 1.21614 to 1.17224, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold6.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.2018 - val_loss: 1.1722\n",
      "Epoch 4/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.1678\n",
      "Epoch 4: val_loss improved from 1.17224 to 1.13472, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold6.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.1664 - val_loss: 1.1347\n",
      "Epoch 5/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.1254\n",
      "Epoch 5: val_loss improved from 1.13472 to 1.10048, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold6.hdf5\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 1.1246 - val_loss: 1.1005\n",
      "Epoch 6/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.0968\n",
      "Epoch 6: val_loss improved from 1.10048 to 1.06987, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold6.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.0931 - val_loss: 1.0699\n",
      "Epoch 7/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 1.0643\n",
      "Epoch 7: val_loss improved from 1.06987 to 1.04259, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold6.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.0614 - val_loss: 1.0426\n",
      "Epoch 8/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.0382\n",
      "Epoch 8: val_loss improved from 1.04259 to 1.01844, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold6.hdf5\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 1.0351 - val_loss: 1.0184\n",
      "Epoch 9/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 1.0184\n",
      "Epoch 9: val_loss improved from 1.01844 to 0.99555, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold6.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 1.0154 - val_loss: 0.9955\n",
      "Epoch 10/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.9983\n",
      "Epoch 10: val_loss improved from 0.99555 to 0.97608, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold6.hdf5\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.9965 - val_loss: 0.9761\n",
      "Epoch 11/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.9806\n",
      "Epoch 11: val_loss improved from 0.97608 to 0.95531, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold6.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9775 - val_loss: 0.9553\n",
      "Epoch 12/100\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.9518\n",
      "Epoch 12: val_loss improved from 0.95531 to 0.93850, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold6.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9533 - val_loss: 0.9385\n",
      "Epoch 13/100\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.9378\n",
      "Epoch 13: val_loss improved from 0.93850 to 0.92143, saving model to Results\\PSI_Site_DLNN_CORENup\\HS_990\\10fold\\models\\HS_990_bestModel-fold6.hdf5\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.9389 - val_loss: 0.9214\n",
      "Epoch 14/100\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.9286"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[38;5;66;03m## Define the model callbacks for early stopping and saving the model. Then train model\u001b[39;00m\n\u001b[0;32m     83\u001b[0m             modelCallbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     84\u001b[0m                 tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(model_file_path,\n\u001b[0;32m     85\u001b[0m                                                    monitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, save_best_only \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m#                                                  mode = 'auto', baseline = None, restore_best_weights = True)\u001b[39;00m\n\u001b[0;32m     89\u001b[0m             ]\n\u001b[1;32m---> 90\u001b[0m             \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX_train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrandomized_index_arr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrandomized_index_arr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodelCallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mfold\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m             model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(model_file_path)\n\u001b[0;32m     95\u001b[0m             \u001b[38;5;66;03m##################################################################################\u001b[39;00m\n\u001b[0;32m     96\u001b[0m             \u001b[38;5;66;03m##### Prediction and metrics for TRAIN dataset\u001b[39;00m\n\u001b[0;32m     97\u001b[0m             \u001b[38;5;66;03m##################################################################################\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_2_8_py_3_10\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_2_8_py_3_10\\lib\\site-packages\\keras\\engine\\training.py:1389\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1387\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[0;32m   1388\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1389\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1391\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_2_8_py_3_10\\lib\\site-packages\\keras\\callbacks.py:438\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 438\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_2_8_py_3_10\\lib\\site-packages\\keras\\callbacks.py:297\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    295\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 297\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    300\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_2_8_py_3_10\\lib\\site-packages\\keras\\callbacks.py:318\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    316\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 318\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    321\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_2_8_py_3_10\\lib\\site-packages\\keras\\callbacks.py:356\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    355\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 356\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    359\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_2_8_py_3_10\\lib\\site-packages\\keras\\callbacks.py:1034\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1034\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_2_8_py_3_10\\lib\\site-packages\\keras\\callbacks.py:1106\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1102\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1105\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1106\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1107\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_2_8_py_3_10\\lib\\site-packages\\keras\\utils\\tf_utils.py:563\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[0;32m    561\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m--> 563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_2_8_py_3_10\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:914\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    910\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    911\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 914\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    915\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_2_8_py_3_10\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:914\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    910\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    911\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 914\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    915\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_2_8_py_3_10\\lib\\site-packages\\keras\\utils\\tf_utils.py:557\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    555\u001b[0m   \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    556\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 557\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m   \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_2_8_py_3_10\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1223\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1200\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1201\u001b[0m \n\u001b[0;32m   1202\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1221\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1222\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1223\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_2_8_py_3_10\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1189\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1188\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1190\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(input_data_folder):\n",
    "    for file in files:\n",
    "        \n",
    "        input_data_file = os.path.join(root, file)\n",
    "        \n",
    "        current_dataset_variety = input_data_file.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "        \n",
    "        openFile = open(input_data_file)\n",
    "        fastaSequences = SeqIO.parse(openFile, \"fasta\")\n",
    "        \n",
    "        ##################################################################################\n",
    "        ##### extract data from the current fasta file\n",
    "        ##################################################################################\n",
    "\n",
    "        positive_List = []\n",
    "        negative_List = []\n",
    "        positive_onehotencoded_List = []\n",
    "        negative_onehotencoded_List = []\n",
    "\n",
    "        for fasta in fastaSequences: \n",
    "            name, sequence = fasta.id, str(fasta.seq)\n",
    "            if \"P\" in name:\n",
    "                positive_List.append(sequence)\n",
    "                aus_seq = one_hot_encode_rna(sequence)\n",
    "                if(len(aus_seq) != 0):\n",
    "                    positive_onehotencoded_List.append(aus_seq)\n",
    "            elif \"N\" in name:\n",
    "                negative_List.append(sequence)\n",
    "                aus_seq = one_hot_encode_rna(sequence)\n",
    "                if(len(aus_seq) != 0):\n",
    "                    negative_onehotencoded_List.append(aus_seq)\n",
    "\n",
    "        openFile.close()\n",
    "\n",
    "        print(\"\\n======================================================================\")\n",
    "        print(\"\\nFile: \"+os.path.join(root, file))\n",
    "        print(\"Positive: \"+str(len(positive_onehotencoded_List)))\n",
    "        print(\"Negative: \"+str(len(negative_onehotencoded_List)))\n",
    "        \n",
    "        ##################################################################################\n",
    "        ##### Generate Folds from dataset, and store to file\n",
    "        ##################################################################################\n",
    "\n",
    "        ## create the features and labels datasets for the training\n",
    "        input_size = (len(positive_onehotencoded_List[1]), 4)\n",
    "        labels = np.concatenate((np.ones((len(positive_onehotencoded_List), 1), dtype=np.float32), np.zeros((len(negative_onehotencoded_List), 1), dtype=np.float32)), axis=0)\n",
    "        features = np.concatenate((positive_onehotencoded_List,negative_onehotencoded_List), 0)\n",
    "\n",
    "        ## Generate the k-fold dataset\n",
    "        folds = build_kfold(features, labels, k=n_fold, shuffle=shuffle, seed=seed)\n",
    "\n",
    "        ## Write the k-fold dataset to file\n",
    "        foldPath = os.path.join(outPath, expName, current_dataset_variety, \"{}fold\".format(n_fold))\n",
    "        if(not os.path.isdir(foldPath)):\n",
    "            os.makedirs(foldPath)\n",
    "        pickle.dump(folds, open(os.path.join(foldPath, foldName), \"wb\"))\n",
    "\n",
    "        ## Create and set directory to save model\n",
    "        modelPath = os.path.join(outPath, expName, current_dataset_variety, \"{}fold\".format(n_fold), \"models\")\n",
    "        if(not os.path.isdir(modelPath)):\n",
    "            os.makedirs(modelPath)\n",
    "            \n",
    "        ##################################################################################\n",
    "        ##### TRAIN and PREDICT for every Fold, using models\n",
    "        ##################################################################################\n",
    "\n",
    "        # fold counter\n",
    "        i = 0\n",
    "\n",
    "        for fold in folds:\n",
    "            \n",
    "            # adding random shuffling of the dataset for training purpose\n",
    "            randomized_index_arr = np.arange(fold[\"X_train\"].shape[0])\n",
    "            randomized_index_arr = np.random.permutation(randomized_index_arr)\n",
    "\n",
    "            print(\"\\nTrain/Test model \"+current_dataset_variety+\" on Fold #\"+str(i)+\".\")\n",
    "\n",
    "            ## Generate model using function\n",
    "            model = DLNN_CORENup(input_shape = input_size)\n",
    "    \n",
    "            model_file_path = os.path.join(modelPath, \"{}_bestModel-fold{}.hdf5\".format(current_dataset_variety, i))\n",
    "            ## Define the model callbacks for early stopping and saving the model. Then train model\n",
    "            modelCallbacks = [\n",
    "                tf.keras.callbacks.ModelCheckpoint(model_file_path,\n",
    "                                                   monitor = 'val_loss', verbose = 1, save_best_only = True, \n",
    "                                                   save_weights_only = False, mode = 'auto', save_freq = 'epoch'),\n",
    "#                 tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 10, verbose = 0, \n",
    "#                                                  mode = 'auto', baseline = None, restore_best_weights = True)\n",
    "            ]\n",
    "            model.fit(x = fold[\"X_train\"][randomized_index_arr], y = fold[\"y_train\"][randomized_index_arr], batch_size = batch_size, epochs = epochs, verbose = 1, \n",
    "                      callbacks = modelCallbacks, validation_data = (fold[\"X_test\"], fold[\"y_test\"]))\n",
    "            \n",
    "            model = tf.keras.models.load_model(model_file_path)\n",
    "            \n",
    "            ##################################################################################\n",
    "            ##### Prediction and metrics for TRAIN dataset\n",
    "            ##################################################################################\n",
    "\n",
    "            y_pred = model.predict(fold[\"X_train\"])\n",
    "            label_pred = pred2label(y_pred)\n",
    "            # Compute precision, recall, sensitivity, specifity, mcc\n",
    "            acc = accuracy_score(fold[\"y_train\"], label_pred)\n",
    "            prec = precision_score(fold[\"y_train\"],label_pred)\n",
    "\n",
    "            conf = confusion_matrix(fold[\"y_train\"], label_pred)\n",
    "            if(conf[0][0]+conf[1][0]):\n",
    "                sens = float(conf[0][0])/float(conf[0][0]+conf[1][0])\n",
    "            else:\n",
    "                sens = 0.0\n",
    "            if(conf[1][1]+conf[0][1]):\n",
    "                spec = float(conf[1][1])/float(conf[1][1]+conf[0][1])\n",
    "            else:\n",
    "                spec = 0.0\n",
    "            if((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0])):\n",
    "                mcc = (float(conf[0][0])*float(conf[1][1]) - float(conf[1][0])*float(conf[0][1]))/math.sqrt((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0]))\n",
    "            else:\n",
    "                mcc= 0.0\n",
    "            fpr, tpr, thresholds = roc_curve(fold[\"y_train\"], y_pred)\n",
    "            auc = roc_auc_score(fold[\"y_train\"], y_pred)\n",
    "\n",
    "            evaluations[\"Model\"].append(current_dataset_variety)\n",
    "            evaluations[\"Dataset\"].append(current_dataset_variety)\n",
    "            evaluations[\"Fold\"].append(i)\n",
    "            evaluations[\"Train_Test\"].append(\"Train\")\n",
    "            evaluations[\"Accuracy\"].append(acc)\n",
    "            evaluations[\"Precision\"].append(prec)\n",
    "            evaluations[\"TPR\"].append(tpr)\n",
    "            evaluations[\"FPR\"].append(fpr)\n",
    "            evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "            evaluations[\"AUC\"].append(auc)\n",
    "            evaluations[\"Sensitivity\"].append(sens)\n",
    "            evaluations[\"Specificity\"].append(spec)\n",
    "            evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "            ##################################################################################\n",
    "            ##### Prediction and metrics for TEST dataset\n",
    "            ##################################################################################\n",
    "\n",
    "            y_pred = model.predict(fold[\"X_test\"])\n",
    "            label_pred = pred2label(y_pred)\n",
    "            # Compute precision, recall, sensitivity, specifity, mcc\n",
    "            acc = accuracy_score(fold[\"y_test\"], label_pred)\n",
    "            prec = precision_score(fold[\"y_test\"],label_pred)\n",
    "\n",
    "            conf = confusion_matrix(fold[\"y_test\"], label_pred)\n",
    "            if(conf[0][0]+conf[1][0]):\n",
    "                sens = float(conf[0][0])/float(conf[0][0]+conf[1][0])\n",
    "            else:\n",
    "                sens = 0.0\n",
    "            if(conf[1][1]+conf[0][1]):\n",
    "                spec = float(conf[1][1])/float(conf[1][1]+conf[0][1])\n",
    "            else:\n",
    "                spec = 0.0\n",
    "            if((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0])):\n",
    "                mcc = (float(conf[0][0])*float(conf[1][1]) - float(conf[1][0])*float(conf[0][1]))/math.sqrt((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0]))\n",
    "            else:\n",
    "                mcc= 0.0\n",
    "            fpr, tpr, thresholds = roc_curve(fold[\"y_test\"], y_pred)\n",
    "            auc = roc_auc_score(fold[\"y_test\"], y_pred)\n",
    "\n",
    "            evaluations[\"Model\"].append(current_dataset_variety)\n",
    "            evaluations[\"Dataset\"].append(current_dataset_variety)\n",
    "            evaluations[\"Fold\"].append(i)\n",
    "            evaluations[\"Train_Test\"].append(\"Test\")\n",
    "            evaluations[\"Accuracy\"].append(acc)\n",
    "            evaluations[\"Precision\"].append(prec)\n",
    "            evaluations[\"TPR\"].append(tpr)\n",
    "            evaluations[\"FPR\"].append(fpr)\n",
    "            evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "            evaluations[\"AUC\"].append(auc)\n",
    "            evaluations[\"Sensitivity\"].append(sens)\n",
    "            evaluations[\"Specificity\"].append(spec)\n",
    "            evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "            i = i+1\n",
    "            del model\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "        ##################################################################################\n",
    "        ##### Dump evaluations to a file\n",
    "        ##################################################################################\n",
    "\n",
    "        evalPath = os.path.join(outPath, expName, \"_Evaluation_All_Datasets\")\n",
    "        if(not os.path.isdir(evalPath)):\n",
    "            os.makedirs(evalPath)\n",
    "\n",
    "        pickle.dump(evaluations,\n",
    "                    open(os.path.join(evalPath, \"{}fold_evaluations.pickle\".format(n_fold)), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Add import statement here, to make this next part of code standalone executable\n",
    "##################################################################################\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter, FormatStrFormatter\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Load file and convert to dataframe for easy manipulation\n",
    "##################################################################################\n",
    "\n",
    "# evalPath = os.path.join(outPath, expName, \"_Evaluation_All_Datasets\")\n",
    "# if(not os.path.isdir(evalPath)):\n",
    "#     os.makedirs(evalPath)\n",
    "\n",
    "# evaluations = pickle.load(open(os.path.join(evalPath, \"{}fold_evaluations.pickle\".format(n_fold)), \"rb\"))\n",
    "\n",
    "evaluations_df = pd.DataFrame.from_dict(evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Group dataset (mean of metrics) by [Dataset, Model, Train_Test] combinations\n",
    "##################################################################################\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Dataset\", \n",
    "                                                 \"Model\", \n",
    "                                                 \"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC'])\n",
    "\n",
    "# DLNN_3 = evaluations_df_grouped[np.in1d(evaluations_df_grouped.index.get_level_values(1), ['DLNN_3'])]\n",
    "# DLNN_5 = evaluations_df_grouped[np.in1d(evaluations_df_grouped.index.get_level_values(1), ['DLNN_5'])]\n",
    "\n",
    "# DLNN_3_Train = DLNN_3[np.in1d(DLNN_3.index.get_level_values(2), ['Train'])]\n",
    "# DLNN_3_Test = DLNN_3[np.in1d(DLNN_3.index.get_level_values(2), ['Test'])]\n",
    "\n",
    "# DLNN_5_Train = DLNN_5[np.in1d(DLNN_5.index.get_level_values(2), ['Train'])]\n",
    "# DLNN_5_Test = DLNN_5[np.in1d(DLNN_5.index.get_level_values(2), ['Test'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluations_df_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max values in evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_df_max = evaluations_df[[\"Dataset\",\n",
    "                                     \"Model\",\n",
    "                                     \"Train_Test\",\n",
    "                                     \"Accuracy\",\n",
    "                                     \"Precision\",\n",
    "                                     \"Sensitivity\",\n",
    "                                     \"Specificity\",\n",
    "                                     \"AUC\",\n",
    "                                     \"MCC\"]].groupby([\"Dataset\", \n",
    "                                                      \"Model\", \n",
    "                                                      \"Train_Test\"]).max().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC']).reset_index()\n",
    "\n",
    "evaluations_df_test_max = evaluations_df_max[evaluations_df_max[\"Train_Test\"] == 'Test']\n",
    "evaluations_df_test_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Dataset\tModel\tTrain_Test\tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# 0\tHS_990\tHS_990\tTest\t0.676768\t0.704545\t0.704082\t0.654545\t0.704545\t0.356886\n",
    "# 2\tMM_944\tMM_944\tTest\t0.744681\t0.744681\t0.779538\t0.769231\t0.744681\t0.489362\n",
    "# 4\tSS_628\tSS_628\tTest\t0.730159\t0.727273\t0.776210\t0.760000\t0.727273\t0.461469"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\tModel\tTrain_Test\tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# 0\tHS_990\tHS_990\tTest\t0.707071\t0.705882\t0.722041\t0.708333\t0.705882\t0.414047\n",
    "# 2\tMM_944\tMM_944\tTest\t0.715789\t0.678571\t0.764599\t0.769231\t0.678571\t0.440598\n",
    "# 4\tSS_628\tSS_628\tTest\t0.777778\t0.781250\t0.826613\t0.774194\t0.781250\t0.555444       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate only top 5 model folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_df5 = evaluations_df.sort_values(['Accuracy'], ascending=False).groupby([\"Dataset\", \n",
    "                                                                                    \"Model\", \n",
    "                                                                                    \"Train_Test\"]).head(5).reset_index()\n",
    "\n",
    "evaluations_df5_grouped = evaluations_df5.groupby([\"Dataset\", \n",
    "                                                 \"Model\", \n",
    "                                                 \"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_df5_grouped[evaluations_df5_grouped[\"Train_Test\"] == 'Test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_df5_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Dataset\tModel\tTrain_Test\tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# 0\tHS_990\tHS_990\tTest\t0.636364\t0.647777\t0.663347\t0.629172\t0.647777\t0.274748\n",
    "# 2\tMM_944\tMM_944\tTest\t0.692788\t0.673316\t0.746133\t0.722696\t0.673316\t0.390784\n",
    "# 4\tSS_628\tSS_628\tTest\t0.680389\t0.670531\t0.770259\t0.707128\t0.670531\t0.369560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \tDataset\tModel\tTrain_Test\tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# 0\tHS_990\tHS_990\tTest\t0.624242\t0.646211\t0.671184\t0.611681\t0.646211\t0.252720\n",
    "# 2\tMM_944\tMM_944\tTest\t0.692744\t0.661864\t0.743313\t0.739310\t0.661864\t0.393179\n",
    "# 4\tSS_628\tSS_628\tTest\t0.720635\t0.731435\t0.776210\t0.715402\t0.731435\t0.443644"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_data_folder = \"Data\\\\Psi_Site_Chen_Independent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### For each input file, train model and generate different outputs in a structured folder\n",
    "##################################################################################\n",
    "\n",
    "## create the evaluation data structure for all iterations\n",
    "evaluations = {\n",
    "    \"Model\" : [],\n",
    "    \"Dataset\" : [],\n",
    "    \"Fold\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "sum_evaluations = {\n",
    "    \"Model\" : [],\n",
    "    \"Dataset\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "vote_evaluations = {\n",
    "    \"Model\" : [],\n",
    "    \"Dataset\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "for root, dirs, files in os.walk(independent_data_folder):\n",
    "    for file in files:\n",
    "        \n",
    "        input_data_file = os.path.join(root, file)\n",
    "        \n",
    "        if 'HS' in file:\n",
    "            bench_data = 'HS_990'\n",
    "        elif 'SS' in file:\n",
    "            bench_data = 'SS_628'\n",
    "            \n",
    "        current_dataset_variety = input_data_file.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "        \n",
    "        openFile = open(input_data_file)\n",
    "        fastaSequences = SeqIO.parse(openFile, \"fasta\")\n",
    "        \n",
    "        ##################################################################################\n",
    "        ##### extract data from the current fasta file\n",
    "        ##################################################################################\n",
    "\n",
    "        positive_List = []\n",
    "        negative_List = []\n",
    "        positive_onehotencoded_List = []\n",
    "        negative_onehotencoded_List = []\n",
    "\n",
    "        for fasta in fastaSequences: \n",
    "            name, sequence = fasta.id, str(fasta.seq)\n",
    "            if \"P\" in name:\n",
    "                positive_List.append(sequence)\n",
    "                aus_seq = one_hot_encode_rna(sequence)\n",
    "                if(len(aus_seq) != 0):\n",
    "                    positive_onehotencoded_List.append(aus_seq)\n",
    "            elif \"N\" in name:\n",
    "                negative_List.append(sequence)\n",
    "                aus_seq = one_hot_encode_rna(sequence)\n",
    "                if(len(aus_seq) != 0):\n",
    "                    negative_onehotencoded_List.append(aus_seq)\n",
    "\n",
    "        openFile.close()\n",
    "\n",
    "        print(\"\\n======================================================================\")\n",
    "        print(\"\\nFile: \"+os.path.join(root, file))\n",
    "        print(\"Positive: \"+str(len(positive_onehotencoded_List)))\n",
    "        print(\"Negative: \"+str(len(negative_onehotencoded_List)))\n",
    "        \n",
    "        ##################################################################################\n",
    "        ##### Generate Folds from dataset, and store to file\n",
    "        ##################################################################################\n",
    "\n",
    "        ## create the features and labels datasets for the training\n",
    "        labels = np.concatenate((np.ones((len(positive_onehotencoded_List), 1), dtype=np.float32), np.zeros((len(negative_onehotencoded_List), 1), dtype=np.float32)), axis=0)\n",
    "        features = np.concatenate((positive_onehotencoded_List,negative_onehotencoded_List), 0)\n",
    "        \n",
    "        benchModelPath = os.path.join(outPath, expName, bench_data, \"{}fold\".format(n_fold), \"models\")\n",
    "            \n",
    "        ##################################################################################\n",
    "        ##### TRAIN and PREDICT for every Fold, using models\n",
    "        ##################################################################################\n",
    "        \n",
    "        y_pred_list = []\n",
    "\n",
    "        for fold in range(n_fold):\n",
    "\n",
    "            print(\"\\nIndependent test on \"+current_dataset_variety+\" using Fold #\"+str(fold)+\" model from \"+bench_data+\".\")\n",
    "            \n",
    "            current_model_path = os.path.join(\n",
    "                benchModelPath, \n",
    "                \"{}_bestModel-fold{}.hdf5\".format(bench_data, fold)\n",
    "            )\n",
    "            \n",
    "            model = tf.keras.models.load_model(current_model_path)\n",
    "\n",
    "            ##################################################################################\n",
    "            ##### Prediction and metrics for TEST dataset\n",
    "            ##################################################################################\n",
    "            \n",
    "            y_pred = model.predict(features)\n",
    "            y_pred_list.append(y_pred)\n",
    "            label_pred = pred2label(y_pred)\n",
    "            # Compute precision, recall, sensitivity, specifity, mcc\n",
    "            acc = accuracy_score(labels, label_pred)\n",
    "            prec = precision_score(labels,label_pred)\n",
    "\n",
    "            conf = confusion_matrix(labels, label_pred)\n",
    "            if(conf[0][0]+conf[1][0]):\n",
    "                sens = float(conf[0][0])/float(conf[0][0]+conf[1][0])\n",
    "            else:\n",
    "                sens = 0.0\n",
    "            if(conf[1][1]+conf[0][1]):\n",
    "                spec = float(conf[1][1])/float(conf[1][1]+conf[0][1])\n",
    "            else:\n",
    "                spec = 0.0\n",
    "            if((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0])):\n",
    "                mcc = (float(conf[0][0])*float(conf[1][1]) - float(conf[1][0])*float(conf[0][1]))/math.sqrt((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0]))\n",
    "            else:\n",
    "                mcc= 0.0\n",
    "            fpr, tpr, thresholds = roc_curve(labels, y_pred)\n",
    "            auc = roc_auc_score(labels, y_pred)\n",
    "\n",
    "            evaluations[\"Model\"].append(current_dataset_variety)\n",
    "            evaluations[\"Dataset\"].append(current_dataset_variety)\n",
    "            evaluations[\"Fold\"].append(i)\n",
    "            evaluations[\"Accuracy\"].append(acc)\n",
    "            evaluations[\"Precision\"].append(prec)\n",
    "            evaluations[\"TPR\"].append(tpr)\n",
    "            evaluations[\"FPR\"].append(fpr)\n",
    "            evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "            evaluations[\"AUC\"].append(auc)\n",
    "            evaluations[\"Sensitivity\"].append(sens)\n",
    "            evaluations[\"Specificity\"].append(spec)\n",
    "            evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "            del model\n",
    "            tf.keras.backend.clear_session()\n",
    "            \n",
    "        ##################################################################################\n",
    "        ##### Prediction and metrics using sum of all folds\n",
    "        ##################################################################################\n",
    "        \n",
    "        y_pred_list_arr = np.swapaxes(np.array(y_pred_list), 0,1)\n",
    "        y_pred_vote = np.mean(y_pred_list_arr, axis = 1)\n",
    "        label_vote = pred2label(y_pred_vote)\n",
    "        \n",
    "        # Compute precision, recall, sensitivity, specifity, mcc\n",
    "        acc = accuracy_score(labels, label_vote)\n",
    "        prec = precision_score(labels, label_vote)\n",
    "\n",
    "        conf = confusion_matrix(labels, label_vote)\n",
    "        if(conf[0][0]+conf[1][0]):\n",
    "            sens = float(conf[0][0])/float(conf[0][0]+conf[1][0])\n",
    "        else:\n",
    "            sens = 0.0\n",
    "        if(conf[1][1]+conf[0][1]):\n",
    "            spec = float(conf[1][1])/float(conf[1][1]+conf[0][1])\n",
    "        else:\n",
    "            spec = 0.0\n",
    "        if((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0])):\n",
    "            mcc = (float(conf[0][0])*float(conf[1][1]) - float(conf[1][0])*float(conf[0][1]))/math.sqrt((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0]))\n",
    "        else:\n",
    "            mcc= 0.0\n",
    "        fpr, tpr, thresholds = roc_curve(labels, y_pred_vote)\n",
    "        auc = roc_auc_score(labels, y_pred_vote)\n",
    "        \n",
    "        sum_evaluations[\"Model\"].append(current_dataset_variety)\n",
    "        sum_evaluations[\"Dataset\"].append(current_dataset_variety)\n",
    "        sum_evaluations[\"Accuracy\"].append(acc)\n",
    "        sum_evaluations[\"Precision\"].append(prec)\n",
    "        sum_evaluations[\"TPR\"].append(tpr)\n",
    "        sum_evaluations[\"FPR\"].append(fpr)\n",
    "        sum_evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "        sum_evaluations[\"AUC\"].append(auc)\n",
    "        sum_evaluations[\"Sensitivity\"].append(sens)\n",
    "        sum_evaluations[\"Specificity\"].append(spec)\n",
    "        sum_evaluations[\"MCC\"].append(mcc)\n",
    "        \n",
    "        ##################################################################################\n",
    "        ##### Prediction and metrics using vote of all folds\n",
    "        ##################################################################################\n",
    "        \n",
    "        y_pred_list_arr = np.swapaxes(np.array(y_pred_list), 0,1)\n",
    "        y_pred_vote = np.sum(np.round(y_pred_list_arr), axis = 1)\n",
    "        label_vote = (y_pred_vote > 5).astype(int)\n",
    "        \n",
    "        # Compute precision, recall, sensitivity, specifity, mcc\n",
    "        acc = accuracy_score(labels, label_vote)\n",
    "        prec = precision_score(labels, label_vote)\n",
    "\n",
    "        conf = confusion_matrix(labels, label_vote)\n",
    "        if(conf[0][0]+conf[1][0]):\n",
    "            sens = float(conf[0][0])/float(conf[0][0]+conf[1][0])\n",
    "        else:\n",
    "            sens = 0.0\n",
    "        if(conf[1][1]+conf[0][1]):\n",
    "            spec = float(conf[1][1])/float(conf[1][1]+conf[0][1])\n",
    "        else:\n",
    "            spec = 0.0\n",
    "        if((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0])):\n",
    "            mcc = (float(conf[0][0])*float(conf[1][1]) - float(conf[1][0])*float(conf[0][1]))/math.sqrt((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0]))\n",
    "        else:\n",
    "            mcc= 0.0\n",
    "        fpr, tpr, thresholds = roc_curve(labels, y_pred_vote)\n",
    "        auc = roc_auc_score(labels, y_pred_vote)\n",
    "\n",
    "        vote_evaluations[\"Model\"].append(current_dataset_variety)\n",
    "        vote_evaluations[\"Dataset\"].append(current_dataset_variety)\n",
    "        vote_evaluations[\"Accuracy\"].append(acc)\n",
    "        vote_evaluations[\"Precision\"].append(prec)\n",
    "        vote_evaluations[\"TPR\"].append(tpr)\n",
    "        vote_evaluations[\"FPR\"].append(fpr)\n",
    "        vote_evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "        vote_evaluations[\"AUC\"].append(auc)\n",
    "        vote_evaluations[\"Sensitivity\"].append(sens)\n",
    "        vote_evaluations[\"Specificity\"].append(spec)\n",
    "        vote_evaluations[\"MCC\"].append(mcc)\n",
    "        \n",
    "        ##################################################################################\n",
    "        ##### Dump evaluations to a file\n",
    "        ##################################################################################\n",
    "\n",
    "        evalPath = os.path.join(outPath, expName, \"_Evaluation_Independent_Datasets\")\n",
    "        if(not os.path.isdir(evalPath)):\n",
    "            os.makedirs(evalPath)\n",
    "\n",
    "        pickle.dump(evaluations,\n",
    "                    open(os.path.join(evalPath, \"{}fold_evaluations.pickle\".format(n_fold)), \"wb\"))\n",
    "        \n",
    "        pickle.dump(sum_evaluations,\n",
    "                    open(os.path.join(evalPath, \"{}fold_sum_evaluations.pickle\".format(n_fold)), \"wb\"))\n",
    "        \n",
    "        pickle.dump(vote_evaluations,\n",
    "                    open(os.path.join(evalPath, \"{}fold_vote_evaluations.pickle\".format(n_fold)), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using each fold, average result of all 10 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_df = pd.DataFrame.from_dict(evaluations)\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Dataset\", \n",
    "                                                 \"Model\"]).mean().filter(['Accuracy', \n",
    "                                                                           'Precision', \n",
    "                                                                           'AUC', \n",
    "                                                                           'Sensitivity', \n",
    "                                                                           'Specificity', \n",
    "                                                                           'MCC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\tModel\tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# HS_200\tHS_200\t0.6780\t0.676877\t0.72104\t0.684022\t0.676877\t0.358413\n",
    "# SS_200\tSS_200\t0.6765\t0.646861\t0.75923\t0.733891\t0.646861\t0.366287"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                 Accuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# Dataset\tModel\t\t\t\t\t\t\n",
    "# HS_200\tHS_200\t0.6750\t0.664200\t0.72708\t0.693586\t0.664200\t0.353822\n",
    "# SS_200\tSS_200\t0.6975\t0.674367\t0.76201\t0.731014\t0.674367\t0.400129"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using each fold, average result of top 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_df5 = evaluations_df.sort_values(['Accuracy'],ascending=False).groupby([\"Dataset\", \n",
    "                                                                                    \"Model\"]).head(5).reset_index()\n",
    "\n",
    "evaluations_df5_grouped = evaluations_df5.groupby([\"Dataset\", \n",
    "                                                   \"Model\"]).mean().filter(['Accuracy', \n",
    "                                                                            'Precision', \n",
    "                                                                            'AUC', \n",
    "                                                                            'Sensitivity', \n",
    "                                                                            'Specificity', \n",
    "                                                                            'MCC']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_df5_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\tModel\tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# 0\tHS_200\tHS_200\t0.704\t0.697657\t0.73026\t0.712350\t0.697657\t0.409001\n",
    "# 1\tSS_200\tSS_200\t0.701\t0.679132\t0.76770\t0.731744\t0.679132\t0.406404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \tDataset\tModel\tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# 0\tHS_200\tHS_200\t0.690\t0.690418\t0.72364\t0.690651\t0.690418\t0.380534\n",
    "# 1\tSS_200\tSS_200\t0.714\t0.695143\t0.76684\t0.738915\t0.695143\t0.431015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using all 10 folds, vote using sum of scores of all 10 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_evaluations_df = pd.DataFrame.from_dict(sum_evaluations)\n",
    "\n",
    "sum_evaluations_df.filter([\"Dataset\", \"Model\", 'Accuracy', 'Precision', 'AUC', 'Sensitivity', 'Specificity', 'MCC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \tDataset\tModel\tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# 0\tHS_200\tHS_200\t0.705\t0.702970\t0.7352\t0.707071\t0.702970\t0.410021\n",
    "# 1\tSS_200\tSS_200\t0.710\t0.666667\t0.7834\t0.783784\t0.666667\t0.434959"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \tDataset\tModel\tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# 0\tHS_200\tHS_200\t0.695\t0.685714\t0.7445\t0.705263\t0.685714\t0.390488\n",
    "# 1\tSS_200\tSS_200\t0.705\t0.672269\t0.7873\t0.753086\t0.672269\t0.417607"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using all 10 folds, vote using absolute vote of all 10 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_evaluations_df = pd.DataFrame.from_dict(vote_evaluations)\n",
    "\n",
    "vote_evaluations_df.filter([\"Dataset\", \"Model\", 'Accuracy', 'Precision', 'AUC', 'Sensitivity', 'Specificity', 'MCC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Dataset\tModel\tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# 0\tHS_200\tHS_200\t0.715\t0.712871\t0.72785\t0.717172\t0.712871\t0.430022\n",
    "# 1\tSS_200\tSS_200\t0.695\t0.666667\t0.76455\t0.734940\t0.666667\t0.395761"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
