{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Define all parameters for model tuning\n",
    "##################################################################################\n",
    "\n",
    "n_fold = 10\n",
    "expName = \"PSI_Site_DLNN_custom\"\n",
    "outPath = \"Results\"\n",
    "foldName = \"folds.pickle\"\n",
    "\n",
    "# modelNames = [\"DLNN_3\", \"DLNN_5\"]\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "shuffle = False\n",
    "seed = None\n",
    "\n",
    "\n",
    "input_data_folder = \"Data\\\\Psi_Site_Chen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from Bio import SeqIO\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# print(tf.test.is_gpu_available(cuda_only=True))\n",
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### define all CUSTOM functions\n",
    "##################################################################################\n",
    "\n",
    "def one_hot_encode_dna(sequence):\n",
    "    \n",
    "    seq_encoded = np.zeros((len(sequence),4))\n",
    "    dict_nuc = {\n",
    "        \"A\": 0,\n",
    "        \"C\": 1,\n",
    "        \"G\": 2,\n",
    "        \"T\":3\n",
    "    }\n",
    "    i = 0\n",
    "    \n",
    "    for single_character in sequence:\n",
    "        if(single_character.upper() in dict_nuc.keys()):\n",
    "            seq_encoded[i][dict_nuc[single_character.upper()]] = 1\n",
    "            i = i+1\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    return seq_encoded\n",
    "\n",
    "def one_hot_encode_rna(sequence):\n",
    "    \n",
    "    seq_encoded = np.zeros((len(sequence),4))\n",
    "    dict_nuc = {\n",
    "        \"A\": 0,\n",
    "        \"C\": 1,\n",
    "        \"G\": 2,\n",
    "        \"U\":3\n",
    "    }\n",
    "    i = 0\n",
    "    \n",
    "    for single_character in sequence:\n",
    "        if(single_character.upper() in dict_nuc.keys()):\n",
    "            seq_encoded[i][dict_nuc[single_character.upper()]] = 1\n",
    "            i = i+1\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    return seq_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### define evaluator functions\n",
    "##################################################################################\n",
    "\n",
    "## Build the K-fold from dataset\n",
    "def build_kfold(features, labels, k=10, shuffle=False, seed=None):\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=shuffle, random_state=seed)\n",
    "    kfoldList = []\n",
    "    for train_index, test_index in skf.split(features, labels):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        kfoldList.append({\n",
    "            \"X_train\": X_train,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_train\":y_train,\n",
    "            \"y_test\":y_test\n",
    "        })\n",
    "    return kfoldList\n",
    "\n",
    "def pred2label(y_pred):\n",
    "    y_pred = np.round(np.clip(y_pred, 0, 1))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################################\n",
    "# ##### Function to customize the DLNN architecture with parameters\n",
    "# ##################################################################################\n",
    "\n",
    "# def DLNN_custom_1(input_shape = (21,4),\n",
    "#                 conv_filters_per_layer = 10, max_kernel_length = 5, conv_strides = 1, ## 1st Convolutional layer parameters\n",
    "#                 max_pool_width = 2, max_pool_stride = 2, ## 1st Maxpool layer parameters\n",
    "#                 lstm_decode_units = 100, ## LSTM layer parameters\n",
    "#                 dense_decode_units = 100, ## Dense layer parameters\n",
    "#                 prob = 0.5, learn_rate = 0.0005, loss = 'binary_crossentropy', \n",
    "#                 metrics = None):\n",
    "    \n",
    "#     input1 = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "#     ## LSTM Path\n",
    "\n",
    "#     x1 = tf.keras.layers.LSTM(lstm_decode_units, return_sequences = True)(input1)\n",
    "# #     x1 = tf.keras.layers.Dropout(prob)(x1)\n",
    "    \n",
    "#     x1 = tf.keras.layers.Flatten()(x1)\n",
    "\n",
    "#     ## Conv Path\n",
    "\n",
    "#     x2 = tf.keras.layers.Conv1D(conv_filters_per_layer, max_kernel_length, strides = conv_strides)(input1)\n",
    "#     x2 = tf.keras.layers.Activation('relu')(x2)\n",
    "# #     x2 = tf.keras.layers.MaxPooling1D(pool_size = max_pool_width, strides = max_pool_stride)(x2)\n",
    "# #     x2 = tf.keras.layers.Dropout(prob)(x2)\n",
    "    \n",
    "#     x2 = tf.keras.layers.Flatten()(x2)\n",
    "\n",
    "#     ## Fully connected Layers\n",
    "\n",
    "#     y = tf.keras.layers.Concatenate(1)([x1,x2])\n",
    "    \n",
    "#     y1 = tf.keras.layers.Dense(dense_decode_units)(y)\n",
    "# #     y1 = tf.keras.layers.Dropout(prob)(y1)\n",
    "    \n",
    "#     y1 = tf.keras.layers.Dense(dense_decode_units/2)(y1)\n",
    "# #     y1 = tf.keras.layers.Dropout(prob)(y1)\n",
    "    \n",
    "#     y1 = tf.keras.layers.Dense(1, activation = 'sigmoid')(y1)\n",
    "\n",
    "#     ## Generate Model from input and output\n",
    "#     model = tf.keras.models.Model(inputs=[input1], outputs=y1)\n",
    "    \n",
    "#     ## Compile model\n",
    "#     if(metrics != None):\n",
    "#         model.compile(optimizer = tf.keras.optimizers.Adam(lr=learn_rate), loss = loss, metrics = metrics)\n",
    "#     else:\n",
    "#         model.compile(optimizer = tf.keras.optimizers.Adam(lr=learn_rate), loss = loss)\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################################\n",
    "# ##### Function to customize the DLNN architecture with parameters\n",
    "# ##################################################################################\n",
    "\n",
    "# def DLNN_custom_2(input_shape = (21,4),\n",
    "#                 conv_filters_per_layer = 30, max_kernel_length = 11, conv_strides = 1, ## 1st Convolutional layer parameters\n",
    "#                 max_pool_width = 2, max_pool_stride = 2, ## 1st Maxpool layer parameters\n",
    "#                 lstm_decode_units = 100, ## LSTM layer parameters\n",
    "#                 dense_decode_units = 200, ## Dense layer parameters\n",
    "#                 prob = 0.5, learn_rate = 0.0005, loss = 'binary_crossentropy', \n",
    "#                 metrics = None):\n",
    "    \n",
    "#     assert max_kernel_length >= 2\n",
    "    \n",
    "#     input1 = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "#     ## LSTM Path\n",
    "\n",
    "#     x1 = tf.keras.layers.LSTM(lstm_decode_units, return_sequences = True)(input1)\n",
    "# #     x1 = tf.keras.layers.Dropout(prob)(x1)\n",
    "#     x1 = tf.keras.layers.Flatten()(x1)\n",
    "\n",
    "#     ## Multipath Conv\n",
    "    \n",
    "#     x2 = []\n",
    "#     for kernel_length in range(2,max_kernel_length):\n",
    "        \n",
    "\n",
    "#         xx = tf.keras.layers.Conv1D(conv_filters_per_layer, kernel_length, strides = conv_strides)(input1)\n",
    "# #         xx = tf.keras.layers.Activation('relu')(xx)\n",
    "#         xx = tf.keras.layers.Flatten()(xx)\n",
    "        \n",
    "#         x2.append(xx)\n",
    "# #     x2 = tf.keras.layers.MaxPooling1D(pool_size = max_pool_width, strides = max_pool_stride)(x2)\n",
    "# #     x2 = tf.keras.layers.Dropout(prob)(x2)\n",
    "\n",
    "#     ## Fully connected Layers\n",
    "\n",
    "#     y = tf.keras.layers.Concatenate(1)([x1]+x2)\n",
    "    \n",
    "#     y1 = tf.keras.layers.Dense(dense_decode_units)(y)\n",
    "#     y1 = tf.keras.layers.Dense(dense_decode_units/2)(y1)\n",
    "#     y1 = tf.keras.layers.Dense(1, activation = 'sigmoid')(y1)\n",
    "# #     y1 = tf.keras.layers.Dropout(prob)(y1)\n",
    "\n",
    "# #     y1 = tf.keras.layers.Dense(dense_decode_units)(y1)\n",
    "    \n",
    "# #     y1 = tf.keras.layers.Dense(dense_decode_units/2)(y1)\n",
    "# #     y1 = tf.keras.layers.Dropout(prob)(y1)\n",
    "    \n",
    "# #     y1 = tf.keras.layers.Dense(1, activation = 'sigmoid')(y1)\n",
    "# #     y1 = tf.keras.layers.Dense(1, activation = 'sigmoid')(y1)\n",
    "\n",
    "#     ## Generate Model from input and output\n",
    "#     model = tf.keras.models.Model(inputs=[input1], outputs=y1)\n",
    "    \n",
    "#     ## Compile model\n",
    "#     if(metrics != None):\n",
    "#         model.compile(optimizer = tf.keras.optimizers.Adam(lr=learn_rate), loss = loss, metrics = metrics)\n",
    "#     else:\n",
    "#         model.compile(optimizer = tf.keras.optimizers.Adam(lr=learn_rate), loss = loss)\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################################\n",
    "# ##### Function to customize the DLNN architecture with parameters\n",
    "# ##################################################################################\n",
    "\n",
    "# def DLNN_custom_3(input_shape = (21,4),\n",
    "#                 conv_filters_per_layer = 10, max_kernel_length = 11, conv_strides = 1, ## 1st Convolutional layer parameters\n",
    "#                 max_pool_width = 2, max_pool_stride = 2, ## 1st Maxpool layer parameters\n",
    "#                 lstm_decode_units = 50, ## LSTM layer parameters\n",
    "#                 dense_decode_units = 100, ## Dense layer parameters\n",
    "#                 prob = 0.25, learn_rate = 0.001, loss = 'binary_crossentropy', \n",
    "#                 metrics = None):\n",
    "    \n",
    "#     assert max_kernel_length >= 2\n",
    "    \n",
    "#     input1 = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "#     ##### LSTM Path\n",
    "#     #########################\n",
    "\n",
    "# #     x1 = tf.keras.layers.LSTM(lstm_decode_units, return_sequences = True)(input1)\n",
    "# #     x1 = tf.keras.layers.LSTM(1, return_sequences = True)(x1)\n",
    "# #     x1 = tf.keras.layers.Dropout(prob)(x1)\n",
    "    \n",
    "#     x1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_decode_units, return_sequences=True))(input1)\n",
    "    \n",
    "# #     model.add(Bidirectional(tf.keras.layers.LSTM(21)))\n",
    "    \n",
    "#     x1 = tf.keras.layers.Flatten()(x1)\n",
    "#     x1 = tf.keras.layers.Dropout(prob)(x1)\n",
    "\n",
    "#     ##### Multipath Conv\n",
    "#     #########################\n",
    "    \n",
    "#     x2 = []\n",
    "#     for kernel_length in range(2,max_kernel_length):\n",
    "        \n",
    "\n",
    "#         xx = tf.keras.layers.Conv1D(conv_filters_per_layer, kernel_length, strides = conv_strides)(input1)\n",
    "# #         xx = tf.keras.layers.Activation('relu')(xx)\n",
    "#         xx = tf.keras.layers.Flatten()(xx)\n",
    "#         xx = tf.keras.layers.Dropout(prob)(xx)\n",
    "        \n",
    "#         x2.append(xx)\n",
    "# #     x2 = tf.keras.layers.MaxPooling1D(pool_size = max_pool_width, strides = max_pool_stride)(x2)\n",
    "# #     x2 = tf.keras.layers.Dropout(prob)(x2)\n",
    "\n",
    "#     ##### Fully connected Layers\n",
    "#     #########################\n",
    "\n",
    "#     y = tf.keras.layers.Concatenate(1)([x1]+x2)\n",
    "    \n",
    "#     y1 = tf.keras.layers.Dense(dense_decode_units)(y)\n",
    "#     y1 = tf.keras.layers.Dropout(prob)(y1)\n",
    "#     y1 = tf.keras.layers.Dense(dense_decode_units/2)(y1)\n",
    "#     x1 = tf.keras.layers.Dropout(prob)(x1)\n",
    "#     y1 = tf.keras.layers.Dense(1, activation = 'sigmoid')(y1)\n",
    "# #     y1 = tf.keras.layers.Dropout(prob)(y1)\n",
    "\n",
    "# #     y1 = tf.keras.layers.Dense(dense_decode_units)(y1)\n",
    "    \n",
    "# #     y1 = tf.keras.layers.Dense(dense_decode_units/2)(y1)\n",
    "# #     y1 = tf.keras.layers.Dropout(prob)(y1)\n",
    "    \n",
    "# #     y1 = tf.keras.layers.Dense(1, activation = 'sigmoid')(y1)\n",
    "# #     y1 = tf.keras.layers.Dense(1, activation = 'sigmoid')(y1)\n",
    "\n",
    "#     ##### Generate Model from input and output\n",
    "#     #########################\n",
    "    \n",
    "#     model = tf.keras.models.Model(inputs=[input1], outputs=y1)\n",
    "    \n",
    "#     ## Compile model\n",
    "#     if(metrics != None):\n",
    "#         model.compile(optimizer = tf.keras.optimizers.Adam(lr=learn_rate), loss = loss, metrics = metrics)\n",
    "#     else:\n",
    "#         model.compile(optimizer = tf.keras.optimizers.Adam(lr=learn_rate), loss = loss)\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################################\n",
    "# ##### Function to customize the DLNN architecture with parameters\n",
    "# ##################################################################################\n",
    "\n",
    "# def DLNN_custom_4(input_shape = (21,4),\n",
    "#                   conv_filters_per_layer = 5, max_kernel_length = 11, conv_strides = 1, ## 1st Convolutional layer parameters\n",
    "#                   max_pool_width = 2, max_pool_stride = 2, ## 1st Maxpool layer parameters\n",
    "#                   rnn_decode_units = 25, ## LSTM layer parameters\n",
    "#                   dense_decode_units = 100, ## Dense layer parameters\n",
    "#                   prob = 0.5, learn_rate = 0.001, loss = 'binary_crossentropy', \n",
    "#                   metrics = None):\n",
    "    \n",
    "#     # beta = 0.001\n",
    "    \n",
    "#     assert max_kernel_length >= 2\n",
    "    \n",
    "#     input1 = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "#     #########################\n",
    "#     ##### LSTM Path\n",
    "#     #########################\n",
    "    \n",
    "#     x1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_decode_units, \n",
    "#                                                             return_sequences=True, \n",
    "#                                                             return_state=False))(input1)\n",
    "#     x1 = tf.keras.layers.Dropout(prob)(x1)\n",
    "#     x1 = tf.keras.layers.Flatten()(x1)\n",
    "    \n",
    "#     #########################\n",
    "#     ##### Multipath Conv\n",
    "#     #########################\n",
    "    \n",
    "#     x2 = []\n",
    "#     for kernel_length in range(2, max_kernel_length):\n",
    "        \n",
    "#         xx = tf.keras.layers.Conv1D(conv_filters_per_layer, kernel_length, strides = conv_strides)(input1)\n",
    "#         xx = tf.keras.layers.Activation('relu')(xx)\n",
    "#         xx = tf.keras.layers.MaxPool1D(pool_size = max_pool_width, strides = max_pool_stride)(xx)\n",
    "#         xx = tf.keras.layers.Dropout(prob)(xx)\n",
    "#         xx = tf.keras.layers.Flatten()(xx)\n",
    "        \n",
    "#         x2.append(xx)\n",
    "        \n",
    "#     #########################\n",
    "#     ##### Fully connected Layers\n",
    "#     #########################\n",
    "\n",
    "#     y = tf.keras.layers.Concatenate(1)([x1]+x2)\n",
    "    \n",
    "#     y1 = tf.keras.layers.Dense(dense_decode_units)(y)\n",
    "#     y1 = tf.keras.layers.Activation('relu')(y1)\n",
    "#     y1 = tf.keras.layers.Dropout(prob)(y1)\n",
    "    \n",
    "#     y1 = tf.keras.layers.Dense(1, activation = 'sigmoid')(y1)\n",
    "    \n",
    "#     #########################\n",
    "#     ##### Generate Model from input and output\n",
    "#     #########################\n",
    "    \n",
    "#     model = tf.keras.models.Model(inputs=[input1], outputs=y1)\n",
    "    \n",
    "#     ## Compile model\n",
    "#     if(metrics != None):\n",
    "#         model.compile(optimizer = tf.keras.optimizers.Adam(lr=learn_rate), loss = loss, metrics = metrics)\n",
    "#     else:\n",
    "#         model.compile(optimizer = tf.keras.optimizers.Adam(lr=learn_rate), loss = loss)\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Function to customize the DLNN architecture with parameters\n",
    "##################################################################################\n",
    "\n",
    "def DLNN_custom_5(input_shape = (21,4),\n",
    "                  conv_filters_per_layer = 10, max_kernel_length = 11, conv_strides = 1, ## 1st Convolutional layer parameters\n",
    "                  max_pool_width = 2, max_pool_stride = 2, ## 1st Maxpool layer parameters\n",
    "                  rnn_decode_units = 10, ## LSTM layer parameters\n",
    "                  dense_decode_units = 100, ## Dense layer parameters\n",
    "                  prob = 0.5, learn_rate = 0.001, loss = 'binary_crossentropy', \n",
    "                  metrics = None):\n",
    "    \n",
    "    beta = 0.01\n",
    "    \n",
    "    assert max_kernel_length >= 2\n",
    "    \n",
    "    input1 = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    #########################\n",
    "    ##### LSTM Path\n",
    "    #########################\n",
    "    \n",
    "    x1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_decode_units*5, \n",
    "                                                            return_sequences=True, \n",
    "                                                            return_state=False))(input1)\n",
    "    x1 = tf.keras.layers.Dropout(prob)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    \n",
    "    #########################\n",
    "    ##### Multipath Conv-LSTM\n",
    "    #########################\n",
    "    \n",
    "    x2 = []\n",
    "    for kernel_length in range(2, max_kernel_length):\n",
    "        \n",
    "#         xx = tf.keras.layers.Conv1D(conv_filters_per_layer, kernel_length, \n",
    "#                                     strides = conv_strides, kernel_regularizer = tf.keras.regularizers.l2(beta))(input1)\n",
    "        xx = tf.keras.layers.Conv1D(conv_filters_per_layer, kernel_length, \n",
    "                                    strides = conv_strides)(input1)\n",
    "        \n",
    "        xxc = tf.keras.layers.Activation('relu')(xx)\n",
    "        xxc = tf.keras.layers.MaxPool1D(pool_size = max_pool_width, strides = max_pool_stride)(xxc)\n",
    "        xxc = tf.keras.layers.Dropout(prob)(xxc)\n",
    "        xxc = tf.keras.layers.Flatten()(xxc)\n",
    "        x2.append(xxc)\n",
    "        \n",
    "        xxl = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(int((input_shape[0]-kernel_length)/2), \n",
    "                                                                 return_sequences=True, \n",
    "                                                                 return_state=False))(xx)\n",
    "        xxl = tf.keras.layers.Dropout(prob)(xxl)\n",
    "        xxl = tf.keras.layers.Flatten()(xxl)\n",
    "        x2.append(xxl)\n",
    "        \n",
    "    #########################\n",
    "    ##### Fully connected Layers\n",
    "    #########################\n",
    "\n",
    "    y = tf.keras.layers.Concatenate(1)([x1]+x2)\n",
    "    \n",
    "    y1 = tf.keras.layers.Dense(dense_decode_units)(y)\n",
    "    y1 = tf.keras.layers.Activation('relu')(y1)\n",
    "    y1 = tf.keras.layers.Dropout(prob)(y1)\n",
    "    \n",
    "    y1 = tf.keras.layers.Dense(1, activation = 'sigmoid')(y1)\n",
    "    \n",
    "    #########################\n",
    "    ##### Generate Model from input and output\n",
    "    #########################\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=[input1], outputs=y1)\n",
    "    \n",
    "    ## Compile model\n",
    "    if(metrics != None):\n",
    "        model.compile(optimizer = tf.keras.optimizers.Adam(lr=learn_rate), loss = loss, metrics = metrics)\n",
    "    else:\n",
    "        model.compile(optimizer = tf.keras.optimizers.Adam(lr=learn_rate), loss = loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 21, 4)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 10)       90          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 19, 10)       130         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 18, 10)       170         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 17, 10)       210         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 16, 10)       250         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 15, 10)       290         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 14, 10)       330         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 13, 10)       370         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 12, 10)       410         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 10)       0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 19, 10)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 18, 10)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 17, 10)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 16, 10)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 15, 10)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 14, 10)       0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 13, 10)       0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 12, 10)       0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 21, 100)      22000       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 10, 10)       0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 20, 18)       1440        conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 9, 10)        0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 19, 18)       1440        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 9, 10)        0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 18, 16)       1216        conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 8, 10)        0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 17, 16)       1216        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 8, 10)        0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 16, 14)       1008        conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 7, 10)        0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 15, 14)       1008        conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 7, 10)        0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 14, 12)       816         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 6, 10)        0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 13, 12)       816         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 6, 10)        0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 12, 10)       640         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 21, 100)      0           bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 10, 10)       0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 18)       0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 9, 10)        0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 19, 18)       0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 9, 10)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 18, 16)       0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 8, 10)        0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 17, 16)       0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 8, 10)        0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 16, 14)       0           bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 7, 10)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 15, 14)       0           bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 7, 10)        0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 14, 12)       0           bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 6, 10)        0           max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 13, 12)       0           bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 6, 10)        0           max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 12, 10)       0           bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 2100)         0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 100)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 360)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 90)           0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 342)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 90)           0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 288)          0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 80)           0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 272)          0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 80)           0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 224)          0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 70)           0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 210)          0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 70)           0           dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 168)          0           dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 60)           0           dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 156)          0           dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 60)           0           dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 120)          0           dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 4940)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "                                                                 flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "                                                                 flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "                                                                 flatten_14[0][0]                 \n",
      "                                                                 flatten_15[0][0]                 \n",
      "                                                                 flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "                                                                 flatten_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100)          494100      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 100)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 100)          0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            101         dropout_19[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 528,051\n",
      "Trainable params: 528,051\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DLNN_custom_5().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(DLNN_custom_5())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "File: Data\\Psi_Site_Chen\\HS_990.txt\n",
      "Positive: 495\n",
      "Negative: 495\n",
      "(990, 21, 4)\n",
      "(990, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "BLEH",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-c396d1e1efbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;31m## Generate the k-fold dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0mfolds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_kfold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_fold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;31m## Write the k-fold dataset to file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-419e51f41ef4>\u001b[0m in \u001b[0;36mbuild_kfold\u001b[1;34m(features, labels, k, shuffle, seed)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'BLEH'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mskf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: BLEH"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "##### For each input file, train model and generate different outputs in a structured folder\n",
    "##################################################################################\n",
    "\n",
    "## create the evaluation data structure for all iterations\n",
    "evaluations = {\n",
    "    \"Model\" : [],\n",
    "    \"Kernel_Length\" : [],\n",
    "    \"Dataset\" : [],\n",
    "    \"Fold\" : [],\n",
    "    \"Train_Test\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "for root, dirs, files in os.walk(input_data_folder):\n",
    "    for file in files:\n",
    "        \n",
    "        input_data_file = os.path.join(root, file)\n",
    "        \n",
    "        current_dataset_variety = input_data_file.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "        \n",
    "        openFile = open(input_data_file)\n",
    "        fastaSequences = SeqIO.parse(openFile, \"fasta\")\n",
    "        \n",
    "        ##################################################################################\n",
    "        ##### extract data from the current fasta file\n",
    "        ##################################################################################\n",
    "\n",
    "        positive_List = []\n",
    "        negative_List = []\n",
    "        positive_onehotencoded_List = []\n",
    "        negative_onehotencoded_List = []\n",
    "\n",
    "        for fasta in fastaSequences: \n",
    "            name, sequence = fasta.id, str(fasta.seq)\n",
    "            if \"P\" in name:\n",
    "                positive_List.append(sequence)\n",
    "                aus_seq = one_hot_encode_rna(sequence)\n",
    "                if(len(aus_seq) != 0):\n",
    "                    positive_onehotencoded_List.append(aus_seq)\n",
    "            elif \"N\" in name:\n",
    "                negative_List.append(sequence)\n",
    "                aus_seq = one_hot_encode_rna(sequence)\n",
    "                if(len(aus_seq) != 0):\n",
    "                    negative_onehotencoded_List.append(aus_seq)\n",
    "\n",
    "        openFile.close()\n",
    "\n",
    "        print(\"\\n======================================================================\")\n",
    "        print(\"\\nFile: \"+os.path.join(root, file))\n",
    "        print(\"Positive: \"+str(len(positive_onehotencoded_List)))\n",
    "        print(\"Negative: \"+str(len(negative_onehotencoded_List)))\n",
    "        \n",
    "        ##################################################################################\n",
    "        ##### Generate Folds from dataset, and store to file\n",
    "        ##################################################################################\n",
    "\n",
    "        ## create the features and labels datasets for the training\n",
    "        input_size = (len(positive_onehotencoded_List[1]), 4)\n",
    "        labels = np.concatenate((np.ones((len(positive_onehotencoded_List), 1), dtype=np.float32), np.zeros((len(negative_onehotencoded_List), 1), dtype=np.float32)), axis=0)\n",
    "        features = np.concatenate((positive_onehotencoded_List,negative_onehotencoded_List), 0)\n",
    "\n",
    "        ## Generate the k-fold dataset\n",
    "        folds = build_kfold(features, labels, k=n_fold, shuffle=shuffle, seed=seed)\n",
    "\n",
    "        ## Write the k-fold dataset to file\n",
    "        foldPath = os.path.join(outPath, expName, current_dataset_variety, \"{}fold\".format(n_fold))\n",
    "        if(not os.path.isdir(foldPath)):\n",
    "            os.makedirs(foldPath)\n",
    "        pickle.dump(folds, open(os.path.join(foldPath, foldName), \"wb\"))\n",
    "\n",
    "        ## Create and set directory to save model\n",
    "        modelPath = os.path.join(outPath, expName, current_dataset_variety, \"{}fold\".format(n_fold), \"models\")\n",
    "        if(not os.path.isdir(modelPath)):\n",
    "            os.makedirs(modelPath)\n",
    "            \n",
    "        ##################################################################################\n",
    "        ##### TRAIN and PREDICT for every Fold, using models\n",
    "        ##################################################################################\n",
    "\n",
    "        # fold counter\n",
    "        i = 0\n",
    "\n",
    "        for fold in folds:\n",
    "\n",
    "            print(\"\\nTrain/Test model \"+current_dataset_variety+\" on Fold #\"+str(i)+\".\")\n",
    "\n",
    "            kernel_length = 3\n",
    "            ## Generate model using function\n",
    "#             model = Conv_LSTM_DLNN(input_shape = input_size, conv_filters_per_layer = 50, kernel_length = kernel_length, \n",
    "#                                    lstm_decode_units = 50, max_pool_width = 2, max_pool_stride = 2, dense_decode_units = 50,\n",
    "#                                    learn_rate = 0.0001, prob = 0.5, loss='binary_crossentropy', metrics=None)\n",
    "\n",
    "            model = DLNN_custom_4(input_shape = input_size)\n",
    "\n",
    "#             ## Define the model callbacks for early stopping and saving the model. Then train model\n",
    "#             modelCallbacks = [\n",
    "#                 tf.keras.callbacks.ModelCheckpoint(os.path.join(modelPath, \"{}_bestModel-fold{}.hdf5\".format(current_dataset_variety, i)),\n",
    "#                                                    monitor = 'val_loss', verbose = 0, save_best_only = True, \n",
    "#                                                    save_weights_only = False, mode = 'auto', save_freq = 'epoch'),\n",
    "#                 tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 10, verbose = 0, \n",
    "#                                                  mode = 'auto', baseline = None, restore_best_weights = False)\n",
    "#             ]\n",
    "#             model.fit(x = fold[\"X_train\"], y = fold[\"y_train\"], batch_size = batch_size, epochs = epochs, verbose = 0, \n",
    "#                       callbacks = modelCallbacks, validation_split=0.2)\n",
    "\n",
    "            model.fit(x = fold[\"X_train\"], y = fold[\"y_train\"], batch_size = batch_size, epochs = epochs, verbose = 1, \n",
    "                      validation_split=0.2)\n",
    "\n",
    "            ##################################################################################\n",
    "            ##### Prediction and metrics for TRAIN dataset\n",
    "            ##################################################################################\n",
    "\n",
    "            y_pred = model.predict(fold[\"X_train\"])\n",
    "            label_pred = pred2label(y_pred)\n",
    "            # Compute precision, recall, sensitivity, specifity, mcc\n",
    "            acc = accuracy_score(fold[\"y_train\"], label_pred)\n",
    "            prec = precision_score(fold[\"y_train\"],label_pred)\n",
    "\n",
    "            conf = confusion_matrix(fold[\"y_train\"], label_pred)\n",
    "            if(conf[0][0]+conf[1][0]):\n",
    "                sens = float(conf[0][0])/float(conf[0][0]+conf[1][0])\n",
    "            else:\n",
    "                sens = 0.0\n",
    "            if(conf[1][1]+conf[0][1]):\n",
    "                spec = float(conf[1][1])/float(conf[1][1]+conf[0][1])\n",
    "            else:\n",
    "                spec = 0.0\n",
    "            if((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0])):\n",
    "                mcc = (float(conf[0][0])*float(conf[1][1]) - float(conf[1][0])*float(conf[0][1]))/math.sqrt((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0]))\n",
    "            else:\n",
    "                mcc= 0.0\n",
    "            fpr, tpr, thresholds = roc_curve(fold[\"y_train\"], y_pred)\n",
    "            auc = roc_auc_score(fold[\"y_train\"], y_pred)\n",
    "\n",
    "            evaluations[\"Model\"].append(current_dataset_variety)\n",
    "            evaluations[\"Kernel_Length\"].append(kernel_length)\n",
    "            evaluations[\"Dataset\"].append(current_dataset_variety)\n",
    "            evaluations[\"Fold\"].append(i)\n",
    "            evaluations[\"Train_Test\"].append(\"Train\")\n",
    "            evaluations[\"Accuracy\"].append(acc)\n",
    "            evaluations[\"Precision\"].append(prec)\n",
    "            evaluations[\"TPR\"].append(tpr)\n",
    "            evaluations[\"FPR\"].append(fpr)\n",
    "            evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "            evaluations[\"AUC\"].append(auc)\n",
    "            evaluations[\"Sensitivity\"].append(sens)\n",
    "            evaluations[\"Specificity\"].append(spec)\n",
    "            evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "            ##################################################################################\n",
    "            ##### Prediction and metrics for TEST dataset\n",
    "            ##################################################################################\n",
    "\n",
    "            y_pred = model.predict(fold[\"X_test\"])\n",
    "            label_pred = pred2label(y_pred)\n",
    "            # Compute precision, recall, sensitivity, specifity, mcc\n",
    "            acc = accuracy_score(fold[\"y_test\"], label_pred)\n",
    "            prec = precision_score(fold[\"y_test\"],label_pred)\n",
    "\n",
    "            conf = confusion_matrix(fold[\"y_test\"], label_pred)\n",
    "            if(conf[0][0]+conf[1][0]):\n",
    "                sens = float(conf[0][0])/float(conf[0][0]+conf[1][0])\n",
    "            else:\n",
    "                sens = 0.0\n",
    "            if(conf[1][1]+conf[0][1]):\n",
    "                spec = float(conf[1][1])/float(conf[1][1]+conf[0][1])\n",
    "            else:\n",
    "                spec = 0.0\n",
    "            if((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0])):\n",
    "                mcc = (float(conf[0][0])*float(conf[1][1]) - float(conf[1][0])*float(conf[0][1]))/math.sqrt((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0]))\n",
    "            else:\n",
    "                mcc= 0.0\n",
    "            fpr, tpr, thresholds = roc_curve(fold[\"y_test\"], y_pred)\n",
    "            auc = roc_auc_score(fold[\"y_test\"], y_pred)\n",
    "\n",
    "            evaluations[\"Model\"].append(current_dataset_variety)\n",
    "            evaluations[\"Kernel_Length\"].append(kernel_length)\n",
    "            evaluations[\"Dataset\"].append(current_dataset_variety)\n",
    "            evaluations[\"Fold\"].append(i)\n",
    "            evaluations[\"Train_Test\"].append(\"Test\")\n",
    "            evaluations[\"Accuracy\"].append(acc)\n",
    "            evaluations[\"Precision\"].append(prec)\n",
    "            evaluations[\"TPR\"].append(tpr)\n",
    "            evaluations[\"FPR\"].append(fpr)\n",
    "            evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "            evaluations[\"AUC\"].append(auc)\n",
    "            evaluations[\"Sensitivity\"].append(sens)\n",
    "            evaluations[\"Specificity\"].append(spec)\n",
    "            evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "            i = i+1\n",
    "            del model\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "        ##################################################################################\n",
    "        ##### Dump evaluations to a file\n",
    "        ##################################################################################\n",
    "\n",
    "        evalPath = os.path.join(outPath, expName, \"_Evaluation_All_Datasets\")\n",
    "        if(not os.path.isdir(evalPath)):\n",
    "            os.makedirs(evalPath)\n",
    "\n",
    "        pickle.dump(evaluations,\n",
    "                    open(os.path.join(evalPath, \"{}fold_evaluations.pickle\".format(n_fold)), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Add import statement here, to make this next part of code standalone executable\n",
    "##################################################################################\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter, FormatStrFormatter\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Load file and convert to dataframe for easy manipulation\n",
    "##################################################################################\n",
    "\n",
    "evalPath = os.path.join(outPath, expName, \"_Evaluation_All_Datasets\")\n",
    "if(not os.path.isdir(evalPath)):\n",
    "    os.makedirs(evalPath)\n",
    "\n",
    "evaluations = pickle.load(open(os.path.join(evalPath, \"{}fold_evaluations.pickle\".format(n_fold)), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluations[\"Model\"] = evaluations[\"Model\"][0:20]\n",
    "# evaluations_df = pd.DataFrame.from_dict(evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_df = pd.DataFrame.from_dict(evaluations)\n",
    "\n",
    "##################################################################################\n",
    "##### Group dataset (mean of metrics) by [Dataset, Model, Train_Test] combinations\n",
    "##################################################################################\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Dataset\", \n",
    "                                                 \"Model\", \n",
    "                                                 \"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC'])\n",
    "\n",
    "# DLNN_3 = evaluations_df_grouped[np.in1d(evaluations_df_grouped.index.get_level_values(1), ['DLNN_3'])]\n",
    "# DLNN_5 = evaluations_df_grouped[np.in1d(evaluations_df_grouped.index.get_level_values(1), ['DLNN_5'])]\n",
    "\n",
    "# DLNN_3_Train = DLNN_3[np.in1d(DLNN_3.index.get_level_values(2), ['Train'])]\n",
    "# DLNN_3_Test = DLNN_3[np.in1d(DLNN_3.index.get_level_values(2), ['Test'])]\n",
    "\n",
    "# DLNN_5_Train = DLNN_5[np.in1d(DLNN_5.index.get_level_values(2), ['Train'])]\n",
    "# DLNN_5_Test = DLNN_5[np.in1d(DLNN_5.index.get_level_values(2), ['Test'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################################\n",
    "# ##### Decide on metric to visualize\n",
    "# ##################################################################################\n",
    "\n",
    "# print(\"Metrics Available : \")\n",
    "# print(list(evaluations_df_grouped.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select a metric to plot below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric_to_plot = \"Accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################################\n",
    "# ##### Visualize with a multiple Bar chart\n",
    "# ##################################################################################\n",
    "\n",
    "# x = np.arange(len(DLNN_3_Train[metric_to_plot]))\n",
    "# width = 0.15\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(17,6))\n",
    "# rects1 = ax.bar(x - (4*(width/2)), round(DLNN_3_Train[metric_to_plot]*100, 3), width, label='DLNN_3, Train')\n",
    "# rects2 = ax.bar(x - (1.5*(width/2)), round(DLNN_5_Train[metric_to_plot]*100, 3), width, label='DLNN_5, Train')\n",
    "# rects3 = ax.bar(x + (1.5*(width/2)), round(DLNN_3_Test[metric_to_plot]*100, 3), width, label='DLNN_3, Test')\n",
    "# rects4 = ax.bar(x + (4*(width/2)), round(DLNN_5_Test[metric_to_plot]*100, 3), width, label='DLNN_5, Test')\n",
    "\n",
    "# ## Custom y-axis tick labels\n",
    "# ax.set_ylabel(metric_to_plot)\n",
    "# ax.set_ylim([(math.floor(min(evaluations_df_grouped[metric_to_plot])*10)-1)*10, \n",
    "#             (math.ceil(max(evaluations_df_grouped[metric_to_plot])*10)+1)*10])\n",
    "# # ax.set_ylim([80, 105])\n",
    "\n",
    "# ## Custom x-axis tick labels\n",
    "# ax.set_xticks(x)\n",
    "# # ax.set_xticklabels(DLNN_3_Train.index.get_level_values(0))\n",
    "# # ax.set_xticklabels([m+\" - \"+str(n) for m,n in \n",
    "# #                         zip(DLNN_3_Train.index.get_level_values(0),DLNN_3_Train.index.get_level_values(1))],\n",
    "# #                   rotation=30)\n",
    "# ax.set_xticklabels(DLNN_3_Train.index.get_level_values(0))\n",
    "\n",
    "# ax.set_title(metric_to_plot+' by Dataset, Model, Train/Test')\n",
    "# ax.legend(loc='upper left')\n",
    "\n",
    "# def autolabel(rects):\n",
    "#     for rect in rects:\n",
    "#         height = rect.get_height()\n",
    "#         ax.annotate('{}'.format(height),\n",
    "#                     xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "#                     xytext=(0, 3),  # 3 points vertical offset\n",
    "#                     textcoords=\"offset points\", \n",
    "#                     ha='center', va='bottom', rotation=90)\n",
    "\n",
    "# autolabel(rects1)\n",
    "# autolabel(rects2)\n",
    "# autolabel(rects3)\n",
    "# autolabel(rects4)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store all metrics' plots to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################################\n",
    "# ##### Iteratively generate comparison plot using every metric\n",
    "# ##################################################################################\n",
    "\n",
    "# for metric_to_plot in list(evaluations_df_grouped.columns):\n",
    "    \n",
    "#     x = np.arange(len(DLNN_3_Train[metric_to_plot]))\n",
    "#     width = 0.15\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(17,6))\n",
    "#     rects1 = ax.bar(x - (4*(width/2)), round(DLNN_3_Train[metric_to_plot]*100, 3), width, label='DLNN_3, Train')\n",
    "#     rects2 = ax.bar(x - (1.5*(width/2)), round(DLNN_5_Train[metric_to_plot]*100, 3), width, label='DLNN_5, Train')\n",
    "#     rects3 = ax.bar(x + (1.5*(width/2)), round(DLNN_3_Test[metric_to_plot]*100, 3), width, label='DLNN_3, Test')\n",
    "#     rects4 = ax.bar(x + (4*(width/2)), round(DLNN_5_Test[metric_to_plot]*100, 3), width, label='DLNN_5, Test')\n",
    "\n",
    "#     ## Custom y-axis tick labels\n",
    "#     ax.set_ylabel(metric_to_plot)\n",
    "#     ax.set_ylim([(math.floor(min(evaluations_df_grouped[metric_to_plot])*10)-1)*10, \n",
    "#                 (math.ceil(max(evaluations_df_grouped[metric_to_plot])*10)+1)*10])\n",
    "#     # ax.set_ylim([80, 105])\n",
    "\n",
    "#     ## Custom x-axis tick labels\n",
    "#     ax.set_xticks(x)\n",
    "#     # ax.set_xticklabels(DLNN_3_Train.index.get_level_values(0))\n",
    "#     # ax.set_xticklabels([m+\" - \"+str(n) for m,n in \n",
    "#     #                         zip(DLNN_3_Train.index.get_level_values(0),DLNN_3_Train.index.get_level_values(1))],\n",
    "#     #                   rotation=30)\n",
    "#     ax.set_xticklabels(DLNN_3_Train.index.get_level_values(0))\n",
    "\n",
    "#     ax.set_title(metric_to_plot+' by Dataset, Model, Train/Test')\n",
    "#     ax.legend(loc='upper left')\n",
    "\n",
    "#     def autolabel(rects):\n",
    "#         for rect in rects:\n",
    "#             height = rect.get_height()\n",
    "#             ax.annotate('{}'.format(height),\n",
    "#                         xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "#                         xytext=(0, 3),  # 3 points vertical offset\n",
    "#                         textcoords=\"offset points\", \n",
    "#                         ha='center', va='bottom', rotation=90)\n",
    "\n",
    "#     autolabel(rects1)\n",
    "#     autolabel(rects2)\n",
    "#     autolabel(rects3)\n",
    "#     autolabel(rects4)\n",
    "    \n",
    "#     plt.savefig(os.path.join(evalPath, \"{}_DLNN_Comparison\".format(metric_to_plot)))\n",
    "#     plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
