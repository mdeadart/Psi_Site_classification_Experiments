{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import os\n",
    "import pickle\n",
    "from Bio import SeqIO\n",
    "import six.moves.cPickle\n",
    "\n",
    "import r2v_functions_RNAmod as r2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Define all parameters for model tuning\n",
    "##################################################################################\n",
    "\n",
    "k = 6\n",
    "\n",
    "dataset_dir_path = \"Data\"\n",
    "setting_dir = \"Psi_Site_Chen\"\n",
    "\n",
    "out_dataset_path = os.path.join(dataset_dir_path[0:dataset_dir_path.find('\\\\')]+\"-Embeddings-Word2Vec\", \n",
    "                                dataset_dir_path[dataset_dir_path.find('\\\\')+1:len(dataset_dir_path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Data\\\\Psi_Site_Chen', 'HS_990.txt'),\n",
       " ('Data\\\\Psi_Site_Chen', 'MM_944.txt'),\n",
       " ('Data\\\\Psi_Site_Chen', 'SS_628.txt')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_setting_dir_path = os.path.join(dataset_dir_path, setting_dir)\n",
    "\n",
    "file_list = []\n",
    "\n",
    "for root, dirs, files in os.walk(dataset_setting_dir_path):\n",
    "    for file in files:\n",
    "        file_list.append((root, file))\n",
    "        \n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing started for file: Data\\Psi_Site_Chen\\HS_990.txt \n",
      "\n",
      "Processing read 0. Last batch: 0.000 minutes. Total time: 0.000 hours.\n",
      "Loading total kmers.\n",
      "Loading model.\n",
      "path_sample: Data\\Psi_Site_Chen\\HS_990.txt\n",
      "Total reads in sample HS_990: 990.\n",
      "Normalizing each read by total number of kmers in that read.\n",
      "Processing P1: 0/990.\n",
      "Saving reads to Data-Embeddings-Word2Vec\\Psi_Site_Chen\\HS_990_Site_Chen_6_10_10_5_5_0.0001_10_1e-05_remb_raw.csv.gz.\n",
      "Performing SVD: (10,990).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\_Uni\\5. Thesis\\Thesis_Work\\_WORK\\r2v_functions_RNAmod.py:134: RuntimeWarning: invalid value encountered in true_divide\n",
      "  reads[:,i] /= n_kmer\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0f9124521c2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcPickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_kmers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_totalkmers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m     \u001b[0mr2v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_reads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath_reads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_totalkmers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath_totalkmers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout_current_dir_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormread\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nProcessing completed for current file.\\n============================================================================\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\_Uni\\5. Thesis\\Thesis_Work\\_WORK\\r2v_functions_RNAmod.py\u001b[0m in \u001b[0;36membed_reads\u001b[1;34m(path_sample, path_totalkmers, path_model, path_out, k, normread, to_sample, a, n_components, delim, svm, verbose, v)\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Performing SVD: (%s,%s).'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtotal_reads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[0msvd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m         \u001b[0msvd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m         \u001b[0mpc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mreads\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mreads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mpc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\base_tf_24\\lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtransformer\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \"\"\"\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\base_tf_24\\lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    163\u001b[0m         \"\"\"\n\u001b[0;32m    164\u001b[0m         X = self._validate_data(X, accept_sparse=['csr', 'csc'],\n\u001b[1;32m--> 165\u001b[1;33m                                 ensure_min_features=2)\n\u001b[0m\u001b[0;32m    166\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\base_tf_24\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'no_validation'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\base_tf_24\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\base_tf_24\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    662\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 664\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\base_tf_24\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                     (type_err,\n\u001b[1;32m--> 106\u001b[1;33m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[0;32m    107\u001b[0m             )\n\u001b[0;32m    108\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "for current_file in file_list:\n",
    "    \n",
    "    print(\"\\nProcessing started for file:\", os.path.join(current_file[0], current_file[1]), \"\\n\")\n",
    "    \n",
    "    ##################################################################################\n",
    "    ##### getting file in/out paths and file names ready\n",
    "    ##################################################################################\n",
    "    \n",
    "    in_current_file_name = current_file[1]\n",
    "    \n",
    "    in_current_file_path = os.path.join(current_file[0],\n",
    "                                        current_file[1])\n",
    "    \n",
    "    out_current_kmers_file_name = current_file[1][0:current_file[1].find(\".\")]+\"_{}mers\".format(k)+current_file[1][current_file[1].find(\".\"):len(current_file[1])]\n",
    "        \n",
    "    out_current_dir_path = os.path.join(current_file[0][0:current_file[0].find(\"\\\\\")]+\"-Embeddings-Word2Vec\",\n",
    "                                        current_file[0][current_file[0].find('\\\\')+1:len(current_file[0])])\n",
    "    \n",
    "    ##################################################################################\n",
    "    ##### read FASTA file\n",
    "    ##################################################################################\n",
    "    \n",
    "    openFile = open(in_current_file_path)\n",
    "    fastaSequences = SeqIO.parse(openFile, \"fasta\")\n",
    "    \n",
    "    ##################################################################################\n",
    "    ##### extract data from the current fasta file\n",
    "    ##################################################################################\n",
    "    \n",
    "    id_List = []\n",
    "    seq_List = []\n",
    "    \n",
    "    for fasta in fastaSequences:\n",
    "        \n",
    "        name, sequence = fasta.id, str(fasta.seq)\n",
    "        \n",
    "        id_List.append(name)\n",
    "        seq_List.append(sequence)\n",
    "    \n",
    "    ##################################################################################\n",
    "    ##### Generate k-mers and write to file\n",
    "    ##################################################################################\n",
    "    \n",
    "    if(not os.path.isdir(out_current_dir_path)):\n",
    "        os.makedirs(out_current_dir_path)\n",
    "        \n",
    "    kmers_path = os.path.join(out_current_dir_path, out_current_kmers_file_name+\".gz\")\n",
    "    \n",
    "    out_kmers = gzip.open(kmers_path,'w')\n",
    "    \n",
    "    for sequence in seq_List:\n",
    "        curr_seq_kmers = []\n",
    "        for i in range(0,len(seq_List[0]) - k + 1):\n",
    "            curr_seq_kmers.append(sequence[i:i+k])\n",
    "\n",
    "        curr_seq_kmers_joined = \" \".join(map(str, curr_seq_kmers))+\"\\n\"\n",
    "        out_kmers.write(curr_seq_kmers_joined.encode())\n",
    "\n",
    "    out_kmers.close()\n",
    "    \n",
    "    ##################################################################################\n",
    "    ##### word2vec Model Training parameters\n",
    "    ##################################################################################\n",
    "\n",
    "    seed = random.randint(1,9999999)\n",
    "    d = 10\n",
    "    w = 5\n",
    "    neg_samps = 5\n",
    "    samp_freq = 0.0001\n",
    "    n_min = 10\n",
    "    epochs = 10\n",
    "    n_cores = 1\n",
    "    prefix = setting_dir\n",
    "    \n",
    "    model_fn = prefix + '_' + str(k) + '_' + str(d) + \\\n",
    "        '_' + str(epochs) + '_' + str(w) + '_' + \\\n",
    "        str(neg_samps).replace('0.','') + '_' + \\\n",
    "        str(samp_freq) + '_' + str(n_min) + '_model.pkl'\n",
    "\n",
    "    model_file_path = os.path.join(out_current_dir_path, model_fn)\n",
    "    \n",
    "    ##################################################################################\n",
    "    ##### Train word2vec Model\n",
    "    ##################################################################################\n",
    "    \n",
    "    kmers_init = LineSentence(kmers_path)\n",
    "\n",
    "    model = Word2Vec(kmers_init, sg=1, size=d, window=w, min_count=n_min, negative=neg_samps,\n",
    "                     sample=samp_freq, iter=epochs, workers=n_cores, seed=seed)\n",
    "\n",
    "    model.save(model_file_path)\n",
    "    \n",
    "    ##################################################################################\n",
    "    ##### Embedding Parameters \n",
    "    ##################################################################################\n",
    "\n",
    "    nr = bool(int(1))\n",
    "    a = 1e-05\n",
    "    v = 1000\n",
    "\n",
    "    path_reads = in_current_file_path\n",
    "    path_model = model_file_path\n",
    "    \n",
    "    fn_totalkmers = '%s_%s_totalkmers.pkl' % (prefix,str(k))\n",
    "\n",
    "    path_totalkmers = os.path.join(out_current_dir_path, fn_totalkmers)\n",
    "    \n",
    "    ##################################################################################\n",
    "    ##### Generating the read embeddings from the sequences using word2vec\n",
    "    ##################################################################################\n",
    "    \n",
    "    total_kmers = r2v.calc_total_kmers(path_reads, path_model, k, verbose=True, v=v)\n",
    "    \n",
    "    six.moves.cPickle.dump(total_kmers, open(path_totalkmers, 'wb'), protocol=4)\n",
    "    \n",
    "    r2v.embed_reads(path_sample = path_reads, path_totalkmers = path_totalkmers, path_model = path_model, path_out = out_current_dir_path, normread=nr, k=k, a=a, verbose=True, v=v)\n",
    "    \n",
    "    print(\"\\nProcessing completed for current file.\\n============================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
